### 🧩 What is ZooKeeper?

ZooKeeper is a **distributed coordination service**. It's like a toolbox that helps large-scale distributed applications **manage configuration, synchronization, group membership**, and **leader election**.

The abstract introduces ZooKeeper as:

- A **wait-free**, **centralized**, **replicated** coordination service.
- Offering **simple primitives** that clients can use to build more complex ones.
- Providing **FIFO order per client**, and **linearizability** for write requests.
- Scalable to **hundreds of thousands of ops/sec** in **read-heavy** workloads.

---

## 🧭 1. Introduction

### 📌 Coordination Needs in Distributed Systems

- Distributed apps need coordination: configuration sharing, knowing which services are alive (group membership), and leader election.
    
- Some services exist for specific needs (Amazon SQS for queues, Chubby for locks).
    
- ZooKeeper's approach: **provide a flexible API** rather than hard-coded primitives.
    

### 🔑 Key Design Decision

- Don't implement coordination primitives (like locks) on the server.
    
- **Expose a simple API** so clients can build their own.
    
- Focus on **wait-free operations** to avoid slow clients blocking others.
    

### ⚖️ Importance of Ordering Guarantees

- Wait-free alone isn’t enough.
    
- ZooKeeper provides:
    
    - **FIFO** (first-in, first-out) ordering for a single client.
        
    - **Linearizability** for writes (consistent global order).
        
- These guarantees are sufficient to implement strong coordination (even consensus).
    

### ⚙️ ZooKeeper Server Design

- Ensemble of servers (replicas).
    
- **Pipelined architecture**: handles many requests simultaneously.
    
- **Reads served locally** for speed.
    
- Writes are ordered using a leader and **Zab protocol** (ZooKeeper Atomic Broadcast).
    

---

## 🔧 2. The ZooKeeper Service

### 2.1 Service Overview

#### 📁 Znodes & Hierarchical Namespace

- ZooKeeper data is stored in **znodes** (like files/directories).
    
- Path-like structure: `/app1/client1`.

![[Pasted image 20250516064254.png]]
    
- Each znode holds data and metadata.
    
- Two types:
    
    - **Regular**: stay until explicitly deleted.
        
    - **Ephemeral**: auto-deleted when session ends.
        

#### 🧮 Sequential Flag

- Adds an incrementing number to node names, useful for ordering.
    

#### 👁️ Watches

- Clients can **watch** znodes for changes.
    
- Triggered only once, must be reset.
    
- Efficient way to stay updated without polling.
    

---

### 2.2 Client API

|Operation|Description|
|---|---|
|`create()`|Create znode|
|`delete()`|Delete znode|
|`exists()`|Check if znode exists|
|`getData()`|Get data from znode|
|`setData()`|Update znode data|
|`getChildren()`|List children znodes|
|`sync()`|Sync client’s view to latest state|

- Each operation has **sync and async** versions.
    
- Paths are passed directly; no need for open/close.
    
- Version numbers used for **conditional updates** (optimistic concurrency).
    

---

### 2.3 ZooKeeper Guarantees

#### ✅ Ordering

- **Linearizable Writes**: all updates are globally ordered.
    
- **FIFO Client Order**: operations from the same client processed in order.
    

#### 🔁 A-Linearizability

- Clients can have **multiple outstanding operations** (relaxed version of standard linearizability).
    
- Still safe and consistent.
    

#### 📊 Example Use Case

- Leader updates 5,000 znodes.
    
- By using **async writes + FIFO order**, the entire update can complete in **<1 second**, vs. 10 seconds in a blocking system.
    

#### 🔔 Watch Order

- A process watching a znode gets notified **before** it sees the new value.
    
- Ensures coordination even with async updates.
    

#### 🔄 `sync()` Use Case

- Ensures you see the latest data across different servers.
    
- Useful when combining ZooKeeper with other communication channels.
    

#### 🔒 Liveness & Durability

- Works if **majority of servers are alive**.
    
- Writes are durable if acknowledged.
    

---

### 2.4 Examples of Primitives (Built Using ZooKeeper)

#### 🔧 Configuration Management

- Store config in a znode.
    
- Clients watch it and reload if changed.
    

#### 🧭 Rendezvous

- Client creates a known znode.
    
- Master writes address info.
    
- Workers watch for the znode and use it to connect.
    

#### 👥 Group Membership

- Each member creates an **ephemeral child** under a group znode.
    
- Group = list of children.
    

#### 🔐 Simple Locks

- Try to create an ephemeral znode.
    
- If exists, wait for delete.
    
- Basic exclusive lock, but suffers from the **herd effect** (many clients wake up together).
    

#### 🐑 Herd-Free Locks

- Use **SEQUENTIAL ephemeral znodes**.
    
- Clients watch **only the previous znode** to minimize wake-ups.
    

#### 📖 Read/Write Locks

- Write lock: block if any prior znode exists.
    
- Read lock: only block if earlier write lock exists.
    

#### 🪩 Double Barrier

- Used to start and end a group task.
    
- Processes register by creating znodes, then wait.
    
- Task begins once enough join; ends when all leave.
    

---

## 🧪 3. ZooKeeper Applications

### 1. Yahoo! Fetching Service (Web Crawling)

- Master-worker coordination.
    
- ZooKeeper used for:
    
    - Configuration
        
    - Leader election
        
    - Detecting healthy servers
        
- Heavy **read-dominant** workload (read:write = 10:1 to 100:1).
    

### 2. Katta (Distributed Indexing)

- Uses shards and slaves.
    
- ZooKeeper handles:
    
    - Group membership
        
    - Leader election
        
    - Shard assignment
        

### 3. Yahoo! Message Broker (YMB)

- Pub/sub system for thousands of topics.
    
- ZooKeeper tracks:
    
    - Brokers
        
    - Topic ownership (primary/backup)
        
    - System control (e.g., shutdown, migration)
        

![[Pasted image 20250516064409.png]]

---

## 🏗️ 4. ZooKeeper Implementation

### 📦 System Components

- Each server has:
    
    - **Request Processor**
        
    - **Atomic Broadcast Layer (Zab)**
        
    - **Replicated In-Memory DB**
        

### 4.1 Request Processor

- Converts requests to **transactions**.
    
- Applies conditional logic before converting (e.g., version check).
    
- Creates **setDataTXN** or **errorTXN**.
    

### 4.2 Zab (ZooKeeper Atomic Broadcast)

- Leader sends proposals to followers.
    
- Needs quorum (majority) to commit.
    
- Guarantees total order.
    
- Delivers **exactly once in order**, even after recovery.
    

### 4.3 Replicated Database

- Fully in-memory for fast reads.
    
- Persistent **write-ahead log** on disk for durability.
    
- Uses **fuzzy snapshots** for crash recovery:
    
    - Snapshot isn’t atomic, but **transactions are idempotent**, so recovery is safe.
        

### 4.4 Client-Server Interactions

- Reads handled **locally**, very fast.
    
- Writes go through the leader and Zab.
    
- `sync()` ensures read sees latest committed write.
    
- Session migration: clients can reconnect to new servers safely using **zxid checks**.
    
- Heartbeats used to detect session timeout.
    

---

## 📊 5. Evaluation

### Setup:

- 50 servers
    
- Java clients
    
- 250 concurrent clients simulated
    

### 5.1 Throughput

#### 🔍 Observations:

- **Read throughput** scales well (hundreds of thousands ops/sec).
    
- **Write throughput** limited by Zab and disk I/O.
    
- More servers = higher read throughput, but **more overhead for writes**.
    

#### 👑 Important Finding:

- Forcing all clients to use the **leader only** (like Chubby) hurts performance for reads.
    

#### 🔄 Failures

- Fast recovery (leader election in <200ms).
    
- Quorum-based system stays alive despite some failures.
    

---

### 5.2 Latency of Requests

- Benchmarked with multiple worker clients.
    
- Even with 1KB create requests, ZooKeeper performs **3× faster than Chubby**.
    
- 1.2–1.4ms latency for `create()` operations.
    

---

### 5.3 Barrier Performance

- Test: 50–200 clients, 200–1600 barrier rounds.
    
- Results:
    
    - Linear scaling with number of barriers.
        
    - Performance consistent and reliable.
        
    - Up to **17,000 ops/sec** in lockstep scenarios.
        

---

## 📚 6. Related Work

- Compares ZooKeeper to:
    
    - **Chubby** (stronger consistency, weaker performance)
        
    - **Sinfonia** (mini-transactions)
        
    - **Dynamo** (eventual consistency)
        
    - **DepSpace** (Byzantine tolerant but slow)
        
    - **Paxos, ISIS, Ensemble** (underlying theory of fault tolerance)
        

---

## 🏁 7. Conclusions

- ZooKeeper provides **wait-free**, **high-performance**, **fault-tolerant** coordination.
    
- Offers **simple interface**, but supports rich coordination via client logic.
    
- Effective even with relaxed consistency (for reads).
    
- Used across Yahoo! and other organizations at scale.
    
- **Watches + async ops** = high throughput and responsiveness


## ✅ Pros and ❌ Cons of Wait-Free Coordination / Connections

In the context of ZooKeeper (and concurrent systems in general), **wait-free** means that **every client operation completes in a bounded number of steps**, **regardless of other clients' behavior**. This contrasts with **blocking** mechanisms like locks, where slow or faulty clients can delay others.

Here’s a breakdown of the **pros and cons**:

---

### ✅ **Pros of Wait-Free Connections**

### 1. **High Performance and Low Latency**

- Clients can issue operations asynchronously without being blocked.
    
- ZooKeeper can process hundreds of thousands of read ops per second due to its wait-free design.
    

### 2. **Fault Isolation**

- One slow or crashed client **does not impact** the progress of others.
    
- Prevents system-wide slowdowns caused by a single faulty node.
    

### 3. **Better Scalability**

- Works well in large-scale systems with thousands of clients.
    
- Reads scale linearly with more servers because they don't require coordination.
    

### 4. **Simple Server Design**

- No need for deadlock detection or timeout management.
    
- Less state to manage at the server (no lock ownership, queues, etc.).
    

### 5. **Increased Availability**

- Avoids delays due to blocked resources.
    
- Encourages **eventual consistency** mechanisms like watches for notification instead of synchronous coordination.
    

---

## ❌ **Cons of Wait-Free Connections**

### 1. **Weaker Consistency for Reads**

- Fast reads are not guaranteed to reflect the most recent write (can be stale).
    
- Clients may need to issue a `sync()` for strongly consistent reads, which adds latency.
    

### 2. **More Complex Client Logic**

- Clients must build coordination primitives (locks, barriers, etc.) themselves.
    
- Error handling, race conditions, and retries become the client's responsibility.
    

### 3. **Inefficient for Some Primitives**

- Implementing strongly consistent or blocking primitives (e.g., mutual exclusion) on top of a wait-free API can be complex or less efficient.
    

### 4. **Potential Redundant Work**

- Since operations aren't blocked, multiple clients might perform redundant or conflicting work simultaneously unless additional coordination is built.
    

### 5. **Notification Overhead**

- Watches are one-time triggers — clients must reset them after each notification.
    
- Designing event-driven applications around watches can be tricky.
    

---

## 🧠 Summary

|Criteria|Wait-Free Approach|
|---|---|
|**Speed**|✅ Very fast, low latency|
|**Fault tolerance**|✅ High; one client doesn't block others|
|**Consistency**|❌ Weaker by default (especially for reads)|
|**Complexity**|❌ Pushed to client-side logic|
|**Scalability**|✅ Excellent|
|**Coordination Tools**|❌ Must be implemented manually|

### 🛠️ `create(path, data, flags)`

|**Aspect**|**Details**|
|---|---|
|**Purpose**|To **create a new znode** in the ZooKeeper data tree.|
|**Inputs**|- `path`: full path of the znode to create  <br>- `data`: byte array to store  <br>- `flags`: type of znode (regular, ephemeral, sequential)|
|**Effect**|A new znode is created with the given data and characteristics.|
|**Fails if**|The znode at that path **already exists**.|
|**Use Case**|- Registering a new member  <br>- Creating a lock node  <br>- Initializing config|
|**Returns**|The actual path of the created znode (with sequence number if sequential).|

---

### ✏️ `setData(path, data, version)`

| **Aspect**   | **Details**                                                                                                                |
| ------------ | -------------------------------------------------------------------------------------------------------------------------- |
| **Purpose**  | To **update the data** of an **existing znode**.                                                                           |
| **Inputs**   | - `path`: path to the existing znode  <br>- `data`: new byte array  <br>- `version`: expected version (use `-1` to ignore) |
| **Effect**   | Modifies the znode's data and increments its version number.                                                               |
| **Fails if** | - The znode **doesn’t exist**  <br>- The version doesn’t match (unless `-1`)                                               |
| **Use Case** | - Updating leader info  <br>- Changing config  <br>- Marking status updates                                                |
| **Returns**  | Updated metadata (e.g., new version).                                                                                      |

---

### ❌ `delete(path, version)`

|**Aspect**|**Details**|
|---|---|
|**Purpose**|Delete a znode.|
|**Inputs**|- `path`: Znode to delete  <br>- `version`: Expected version (or `-1` to skip check)|
|**Effect**|Removes the znode from the data tree.|
|**Fails if**|Znode does not exist, version mismatch, or has children.|
|**Returns**|Nothing (void on success).|

---

### ❓ `exists(path, watch)`

|**Aspect**|**Details**|
|---|---|
|**Purpose**|Check if a znode exists.|
|**Inputs**|- `path`: Znode to check  <br>- `watch`: Set `true` to register a watch|
|**Effect**|Returns whether the znode exists.|
|**Fails if**|Never throws error (returns false if not found).|
|**Returns**|Metadata if exists, otherwise `null`.|

---

### 📥 `getData(path, watch)`

|**Aspect**|**Details**|
|---|---|
|**Purpose**|Retrieve data and metadata of a znode.|
|**Inputs**|- `path`: Znode to read  <br>- `watch`: Set `true` to register a watch|
|**Effect**|Returns the current data and metadata of the znode.|
|**Fails if**|Znode does not exist.|
|**Returns**|Data (byte array) + metadata (version, timestamps).|

---

### 📂 `getChildren(path, watch)`

|**Aspect**|**Details**|
|---|---|
|**Purpose**|List the children of a znode.|
|**Inputs**|- `path`: Parent znode  <br>- `watch`: Set `true` to register a watch|
|**Effect**|Returns the names of all child znodes.|
|**Fails if**|Znode does not exist.|
|**Returns**|List of child znode names (not full paths).|

---

### 🔄 `sync(path)`

|**Aspect**|**Details**|
|---|---|
|**Purpose**|Synchronize a client's view with the latest ZooKeeper state.|
|**Inputs**|- `path`: Path to sync (currently ignored)|
|**Effect**|Ensures all preceding writes are visible before subsequent reads.|
|**Fails if**|None (used to prevent stale reads, not data access).|
|**Returns**|Confirmation that sync has completed.|

---

### ⚖️ Summary: `create` vs `setData`

| Feature              | `create`                              | `setData`                                |
| -------------------- | ------------------------------------- | ---------------------------------------- |
| **Action**           | Creates a **new znode**               | Updates **existing znode's data**        |
| **Fails if**         | Node **already exists**               | Node **doesn’t exist**, or wrong version |
| **Main Use**         | Initialization / creation of metadata | Updating configuration / state           |
| **Affects version?** | Starts at version 0                   | Increments version on success            |
| **Watch Triggered?** | Yes, on node creation                 | Yes, on data change                      |



[[Distributed Guide]]
[[999. Questions]]