### ğŸ§© What is ZooKeeper?

ZooKeeper is a **distributed coordination service**. It's like a toolbox that helps large-scale distributed applications **manage configuration, synchronization, group membership**, and **leader election**.

The abstract introduces ZooKeeper as:

- A **wait-free**, **centralized**, **replicated** coordination service.
- Offering **simple primitives** that clients can use to build more complex ones.
- Providing **FIFO order per client**, and **linearizability** for write requests.
- Scalable to **hundreds of thousands of ops/sec** in **read-heavy** workloads.

---

## ğŸ§­ 1. Introduction

### ğŸ“Œ Coordination Needs in Distributed Systems

- Distributed apps need coordination: configuration sharing, knowing which services are alive (group membership), and leader election.
    
- Some services exist for specific needs (Amazon SQS for queues, Chubby for locks).
    
- ZooKeeper's approach: **provide a flexible API** rather than hard-coded primitives.
    

### ğŸ”‘ Key Design Decision

- Don't implement coordination primitives (like locks) on the server.
    
- **Expose a simple API** so clients can build their own.
    
- Focus on **wait-free operations** to avoid slow clients blocking others.
    

### âš–ï¸ Importance of Ordering Guarantees

- Wait-free alone isnâ€™t enough.
    
- ZooKeeper provides:
    
    - **FIFO** (first-in, first-out) ordering for a single client.
        
    - **Linearizability** for writes (consistent global order).
        
- These guarantees are sufficient to implement strong coordination (even consensus).
    

### âš™ï¸ ZooKeeper Server Design

- Ensemble of servers (replicas).
    
- **Pipelined architecture**: handles many requests simultaneously.
    
- **Reads served locally** for speed.
    
- Writes are ordered using a leader and **Zab protocol** (ZooKeeper Atomic Broadcast).
    

---

## ğŸ”§ 2. The ZooKeeper Service

### 2.1 Service Overview

#### ğŸ“ Znodes & Hierarchical Namespace

- ZooKeeper data is stored in **znodes** (like files/directories).
    
- Path-like structure: `/app1/client1`.

![[Pasted image 20250516064254.png]]
    
- Each znode holds data and metadata.
    
- Two types:
    
    - **Regular**: stay until explicitly deleted.
        
    - **Ephemeral**: auto-deleted when session ends.
        

#### ğŸ§® Sequential Flag

- Adds an incrementing number to node names, useful for ordering.
    

#### ğŸ‘ï¸ Watches

- Clients can **watch** znodes for changes.
    
- Triggered only once, must be reset.
    
- Efficient way to stay updated without polling.
    

---

### 2.2 Client API

|Operation|Description|
|---|---|
|`create()`|Create znode|
|`delete()`|Delete znode|
|`exists()`|Check if znode exists|
|`getData()`|Get data from znode|
|`setData()`|Update znode data|
|`getChildren()`|List children znodes|
|`sync()`|Sync clientâ€™s view to latest state|

- Each operation has **sync and async** versions.
    
- Paths are passed directly; no need for open/close.
    
- Version numbers used for **conditional updates** (optimistic concurrency).
    

---

### 2.3 ZooKeeper Guarantees

#### âœ… Ordering

- **Linearizable Writes**: all updates are globally ordered.
    
- **FIFO Client Order**: operations from the same client processed in order.
    

#### ğŸ” A-Linearizability

- Clients can have **multiple outstanding operations** (relaxed version of standard linearizability).
    
- Still safe and consistent.
    

#### ğŸ“Š Example Use Case

- Leader updates 5,000 znodes.
    
- By using **async writes + FIFO order**, the entire update can complete in **<1 second**, vs. 10 seconds in a blocking system.
    

#### ğŸ”” Watch Order

- A process watching a znode gets notified **before** it sees the new value.
    
- Ensures coordination even with async updates.
    

#### ğŸ”„ `sync()` Use Case

- Ensures you see the latest data across different servers.
    
- Useful when combining ZooKeeper with other communication channels.
    

#### ğŸ”’ Liveness & Durability

- Works if **majority of servers are alive**.
    
- Writes are durable if acknowledged.
    

---

### 2.4 Examples of Primitives (Built Using ZooKeeper)

#### ğŸ”§ Configuration Management

- Store config in a znode.
    
- Clients watch it and reload if changed.
    

#### ğŸ§­ Rendezvous

- Client creates a known znode.
    
- Master writes address info.
    
- Workers watch for the znode and use it to connect.
    

#### ğŸ‘¥ Group Membership

- Each member creates an **ephemeral child** under a group znode.
    
- Group = list of children.
    

#### ğŸ” Simple Locks

- Try to create an ephemeral znode.
    
- If exists, wait for delete.
    
- Basic exclusive lock, but suffers from the **herd effect** (many clients wake up together).
    

#### ğŸ‘ Herd-Free Locks

- Use **SEQUENTIAL ephemeral znodes**.
    
- Clients watch **only the previous znode** to minimize wake-ups.
    

#### ğŸ“– Read/Write Locks

- Write lock: block if any prior znode exists.
    
- Read lock: only block if earlier write lock exists.
    

#### ğŸª© Double Barrier

- Used to start and end a group task.
    
- Processes register by creating znodes, then wait.
    
- Task begins once enough join; ends when all leave.
    

---

## ğŸ§ª 3. ZooKeeper Applications

### 1. Yahoo! Fetching Service (Web Crawling)

- Master-worker coordination.
    
- ZooKeeper used for:
    
    - Configuration
        
    - Leader election
        
    - Detecting healthy servers
        
- Heavy **read-dominant** workload (read:write = 10:1 to 100:1).
    

### 2. Katta (Distributed Indexing)

- Uses shards and slaves.
    
- ZooKeeper handles:
    
    - Group membership
        
    - Leader election
        
    - Shard assignment
        

### 3. Yahoo! Message Broker (YMB)

- Pub/sub system for thousands of topics.
    
- ZooKeeper tracks:
    
    - Brokers
        
    - Topic ownership (primary/backup)
        
    - System control (e.g., shutdown, migration)
        

![[Pasted image 20250516064409.png]]

---

## ğŸ—ï¸ 4. ZooKeeper Implementation

### ğŸ“¦ System Components

- Each server has:
    
    - **Request Processor**
        
    - **Atomic Broadcast Layer (Zab)**
        
    - **Replicated In-Memory DB**
        

### 4.1 Request Processor

- Converts requests to **transactions**.
    
- Applies conditional logic before converting (e.g., version check).
    
- Creates **setDataTXN** or **errorTXN**.
    

### 4.2 Zab (ZooKeeper Atomic Broadcast)

- Leader sends proposals to followers.
    
- Needs quorum (majority) to commit.
    
- Guarantees total order.
    
- Delivers **exactly once in order**, even after recovery.
    

### 4.3 Replicated Database

- Fully in-memory for fast reads.
    
- Persistent **write-ahead log** on disk for durability.
    
- Uses **fuzzy snapshots** for crash recovery:
    
    - Snapshot isnâ€™t atomic, but **transactions are idempotent**, so recovery is safe.
        

### 4.4 Client-Server Interactions

- Reads handled **locally**, very fast.
    
- Writes go through the leader and Zab.
    
- `sync()` ensures read sees latest committed write.
    
- Session migration: clients can reconnect to new servers safely using **zxid checks**.
    
- Heartbeats used to detect session timeout.
    

---

## ğŸ“Š 5. Evaluation

### Setup:

- 50 servers
    
- Java clients
    
- 250 concurrent clients simulated
    

### 5.1 Throughput

#### ğŸ” Observations:

- **Read throughput** scales well (hundreds of thousands ops/sec).
    
- **Write throughput** limited by Zab and disk I/O.
    
- More servers = higher read throughput, but **more overhead for writes**.
    

#### ğŸ‘‘ Important Finding:

- Forcing all clients to use the **leader only** (like Chubby) hurts performance for reads.
    

#### ğŸ”„ Failures

- Fast recovery (leader election in <200ms).
    
- Quorum-based system stays alive despite some failures.
    

---

### 5.2 Latency of Requests

- Benchmarked with multiple worker clients.
    
- Even with 1KB create requests, ZooKeeper performs **3Ã— faster than Chubby**.
    
- 1.2â€“1.4ms latency for `create()` operations.
    

---

### 5.3 Barrier Performance

- Test: 50â€“200 clients, 200â€“1600 barrier rounds.
    
- Results:
    
    - Linear scaling with number of barriers.
        
    - Performance consistent and reliable.
        
    - Up to **17,000 ops/sec** in lockstep scenarios.
        

---

## ğŸ“š 6. Related Work

- Compares ZooKeeper to:
    
    - **Chubby** (stronger consistency, weaker performance)
        
    - **Sinfonia** (mini-transactions)
        
    - **Dynamo** (eventual consistency)
        
    - **DepSpace** (Byzantine tolerant but slow)
        
    - **Paxos, ISIS, Ensemble** (underlying theory of fault tolerance)
        

---

## ğŸ 7. Conclusions

- ZooKeeper provides **wait-free**, **high-performance**, **fault-tolerant** coordination.
    
- Offers **simple interface**, but supports rich coordination via client logic.
    
- Effective even with relaxed consistency (for reads).
    
- Used across Yahoo! and other organizations at scale.
    
- **Watches + async ops** = high throughput and responsiveness


## âœ… Pros and âŒ Cons of Wait-Free Coordination / Connections

In the context of ZooKeeper (and concurrent systems in general), **wait-free** means that **every client operation completes in a bounded number of steps**, **regardless of other clients' behavior**. This contrasts with **blocking** mechanisms like locks, where slow or faulty clients can delay others.

Hereâ€™s a breakdown of the **pros and cons**:

---

### âœ… **Pros of Wait-Free Connections**

### 1. **High Performance and Low Latency**

- Clients can issue operations asynchronously without being blocked.
    
- ZooKeeper can process hundreds of thousands of read ops per second due to its wait-free design.
    

### 2. **Fault Isolation**

- One slow or crashed client **does not impact** the progress of others.
    
- Prevents system-wide slowdowns caused by a single faulty node.
    

### 3. **Better Scalability**

- Works well in large-scale systems with thousands of clients.
    
- Reads scale linearly with more servers because they don't require coordination.
    

### 4. **Simple Server Design**

- No need for deadlock detection or timeout management.
    
- Less state to manage at the server (no lock ownership, queues, etc.).
    

### 5. **Increased Availability**

- Avoids delays due to blocked resources.
    
- Encourages **eventual consistency** mechanisms like watches for notification instead of synchronous coordination.
    

---

## âŒ **Cons of Wait-Free Connections**

### 1. **Weaker Consistency for Reads**

- Fast reads are not guaranteed to reflect the most recent write (can be stale).
    
- Clients may need to issue a `sync()` for strongly consistent reads, which adds latency.
    

### 2. **More Complex Client Logic**

- Clients must build coordination primitives (locks, barriers, etc.) themselves.
    
- Error handling, race conditions, and retries become the client's responsibility.
    

### 3. **Inefficient for Some Primitives**

- Implementing strongly consistent or blocking primitives (e.g., mutual exclusion) on top of a wait-free API can be complex or less efficient.
    

### 4. **Potential Redundant Work**

- Since operations aren't blocked, multiple clients might perform redundant or conflicting work simultaneously unless additional coordination is built.
    

### 5. **Notification Overhead**

- Watches are one-time triggers â€” clients must reset them after each notification.
    
- Designing event-driven applications around watches can be tricky.
    

---

## ğŸ§  Summary

|Criteria|Wait-Free Approach|
|---|---|
|**Speed**|âœ… Very fast, low latency|
|**Fault tolerance**|âœ… High; one client doesn't block others|
|**Consistency**|âŒ Weaker by default (especially for reads)|
|**Complexity**|âŒ Pushed to client-side logic|
|**Scalability**|âœ… Excellent|
|**Coordination Tools**|âŒ Must be implemented manually|

### ğŸ› ï¸ `create(path, data, flags)`

|**Aspect**|**Details**|
|---|---|
|**Purpose**|To **create a new znode** in the ZooKeeper data tree.|
|**Inputs**|- `path`: full path of the znode to create  <br>- `data`: byte array to store  <br>- `flags`: type of znode (regular, ephemeral, sequential)|
|**Effect**|A new znode is created with the given data and characteristics.|
|**Fails if**|The znode at that path **already exists**.|
|**Use Case**|- Registering a new member  <br>- Creating a lock node  <br>- Initializing config|
|**Returns**|The actual path of the created znode (with sequence number if sequential).|

---

### âœï¸ `setData(path, data, version)`

| **Aspect**   | **Details**                                                                                                                |
| ------------ | -------------------------------------------------------------------------------------------------------------------------- |
| **Purpose**  | To **update the data** of an **existing znode**.                                                                           |
| **Inputs**   | - `path`: path to the existing znode  <br>- `data`: new byte array  <br>- `version`: expected version (use `-1` to ignore) |
| **Effect**   | Modifies the znode's data and increments its version number.                                                               |
| **Fails if** | - The znode **doesnâ€™t exist**  <br>- The version doesnâ€™t match (unless `-1`)                                               |
| **Use Case** | - Updating leader info  <br>- Changing config  <br>- Marking status updates                                                |
| **Returns**  | Updated metadata (e.g., new version).                                                                                      |

---

### âŒ `delete(path, version)`

|**Aspect**|**Details**|
|---|---|
|**Purpose**|Delete a znode.|
|**Inputs**|- `path`: Znode to delete  <br>- `version`: Expected version (or `-1` to skip check)|
|**Effect**|Removes the znode from the data tree.|
|**Fails if**|Znode does not exist, version mismatch, or has children.|
|**Returns**|Nothing (void on success).|

---

### â“ `exists(path, watch)`

|**Aspect**|**Details**|
|---|---|
|**Purpose**|Check if a znode exists.|
|**Inputs**|- `path`: Znode to check  <br>- `watch`: Set `true` to register a watch|
|**Effect**|Returns whether the znode exists.|
|**Fails if**|Never throws error (returns false if not found).|
|**Returns**|Metadata if exists, otherwise `null`.|

---

### ğŸ“¥ `getData(path, watch)`

|**Aspect**|**Details**|
|---|---|
|**Purpose**|Retrieve data and metadata of a znode.|
|**Inputs**|- `path`: Znode to read  <br>- `watch`: Set `true` to register a watch|
|**Effect**|Returns the current data and metadata of the znode.|
|**Fails if**|Znode does not exist.|
|**Returns**|Data (byte array) + metadata (version, timestamps).|

---

### ğŸ“‚ `getChildren(path, watch)`

|**Aspect**|**Details**|
|---|---|
|**Purpose**|List the children of a znode.|
|**Inputs**|- `path`: Parent znode  <br>- `watch`: Set `true` to register a watch|
|**Effect**|Returns the names of all child znodes.|
|**Fails if**|Znode does not exist.|
|**Returns**|List of child znode names (not full paths).|

---

### ğŸ”„ `sync(path)`

|**Aspect**|**Details**|
|---|---|
|**Purpose**|Synchronize a client's view with the latest ZooKeeper state.|
|**Inputs**|- `path`: Path to sync (currently ignored)|
|**Effect**|Ensures all preceding writes are visible before subsequent reads.|
|**Fails if**|None (used to prevent stale reads, not data access).|
|**Returns**|Confirmation that sync has completed.|

---

### âš–ï¸ Summary: `create` vs `setData`

| Feature              | `create`                              | `setData`                                |
| -------------------- | ------------------------------------- | ---------------------------------------- |
| **Action**           | Creates a **new znode**               | Updates **existing znode's data**        |
| **Fails if**         | Node **already exists**               | Node **doesnâ€™t exist**, or wrong version |
| **Main Use**         | Initialization / creation of metadata | Updating configuration / state           |
| **Affects version?** | Starts at version 0                   | Increments version on success            |
| **Watch Triggered?** | Yes, on node creation                 | Yes, on data change                      |

## Limitations of ZooKeeper

## 1.Â **Centralized Coordination Kernel**

- **Limitation:**  
    Although ZooKeeper is replicated for fault tolerance, it acts as a logically centralized service. All writes must go through the leader, which can become a bottleneck for write-heavy workloads.
    
- **Consequence:**
    
    - Write throughput is limited by the leaderâ€™s capacity.
        
    - In large clusters or highly write-intensive applications, this can lead to performance degradation.
        

## 2.Â **Read/Write Consistency Trade-off**

- **Limitation:**  
    ZooKeeper provides linearizability for writes but allows reads to be served locally by any server, which can result in stale reads unless the client explicitly callsÂ `sync()`.
    
- **Consequence:**
    
    - Clients may observe old data unless they take extra steps.
        
    - Developers must be careful to avoid consistency bugs in applications that require strong read-after-write guarantees.
        

## 3.Â **Limited Data Model and Storage**

- **Limitation:**  
    Znodes are designed for metadata and small configuration data, not for storing large or binary files.
    
- **Consequence:**
    
    - ZooKeeper is unsuitable for general-purpose data storage.
        
    - Attempting to store large data can impact performance and reliability.
        

## 4.Â **No Native Support for Advanced Coordination Primitives**

- **Limitation:**  
    ZooKeeper exposes a simple API; higher-level primitives (like distributed locks, leader election, barriers) must be implemented by the client.
    
- **Consequence:**
    
    - Increases the complexity and potential for bugs in client code.
        
    - No standardized, out-of-the-box recipes for advanced patterns.
        

## 5.Â **Ephemeral Node Semantics and Session Management**

- **Limitation:**  
    Ephemeral znodes are tied to client sessions, which are managed via heartbeats. Network partitions or long GC pauses can cause session expiration and unwanted node deletion.
    
- **Consequence:**
    
    - Can lead to false positives in failure detection.
        
    - May cause cascading failures in systems relying on ephemeral nodes for membership or locks.
        

## 6.Â **Scalability Constraints**

- **Limitation:**  
    ZooKeeper ensembles are typically small (3â€“7 nodes) due to the cost of consensus and replication.
    
- **Consequence:**
    
    - Cannot scale horizontally to hundreds of nodes.
        
    - Limits the maximum throughput and availability zone coverage.
        

## 7.Â **Operational Complexity**

- **Limitation:**  
    Requires careful tuning of session timeouts, ensemble size, and network reliability.
    
- **Consequence:**
    
    - Misconfiguration can lead to split-brain, data loss, or poor performance.
        
    - Operational overhead for upgrades, backups, and monitoring.
        

## 8.Â **No Multi-Tenancy or Namespacing**

- **Limitation:**  
    ZooKeeper lacks built-in support for namespaces or multi-tenancy isolation.
    
- **Consequence:**
    
    - Multiple applications must carefully partition their znodes to avoid interference.
        
    - Security and access control are coarse-grained.
        

---



## How Modern Technologies Mitigate These Limitations

## 1.Â **Decentralized and Sharded Coordination**

- **Modern Solution:**  
    Systems likeÂ **etcd v3**,Â **Consul**, andÂ **Raft-based sharded clusters**Â allow for sharding of data and decentralized leadership, reducing bottlenecks.
    
- **Benefit:**
    
    - Higher write throughput by distributing load.
        
    - Better scalability across datacenters and regions.
        

## 2.Â **Stronger and Configurable Consistency Models**

- **Modern Solution:**  
    Newer systems allow clients to choose between linearizable, sequential, or eventual consistency per operation (e.g., etcdâ€™s consistency flags).
    
- **Benefit:**
    
    - Applications can balance performance and consistency as needed.
        
    - Stronger guarantees for critical operations, relaxed for others.
        

## 3.Â **Enhanced Data Models and Storage**

- **Modern Solution:**  
    Some modern coordination services (e.g., Consul KV, Redis for ephemeral coordination) support larger values, richer data types, and better integration with storage backends.
    
- **Benefit:**
    
    - More flexible storage for configuration and metadata.
        
    - Integration with secret management and service discovery.
        

## 4.Â **First-Class Support for Coordination Primitives**

- **Modern Solution:**  
    Libraries and frameworks provide robust, standardized implementations of leader election, distributed locks, and barriers (e.g., etcd concurrency API, Consul sessions).
    
- **Benefit:**
    
    - Reduces risk of bugs in client code.
        
    - Faster development and easier maintenance.
        

## 5.Â **Improved Session and Failure Detection**

- **Modern Solution:**  
    Use of advanced failure detectors (like SWIM), adaptive timeouts, and session fencing tokens to reduce false positives.
    
- **Benefit:**
    
    - More reliable membership and lock management.
        
    - Less risk of accidental session expiration.
        

## 6.Â **Horizontal Scalability and Global Distribution**

- **Modern Solution:**  
    Systems likeÂ **Spanner**,Â **CockroachDB**, andÂ **multi-region etcd**Â use advanced consensus protocols (Paxos, Raft, or custom protocols) to scale across many nodes and regions.
    
- **Benefit:**
    
    - Higher throughput, global availability, and fault tolerance.
        
    - Can support much larger clusters and workloads.
        

## 7.Â **Simplified Operations and Cloud-Native Deployments**

- **Modern Solution:**  
    Managed services (e.g.,Â **Amazon DynamoDB for coordination**,Â **Azure Cosmos DB**,Â **GKE/AKS/EKS managed etcd**) handle upgrades, backups, and monitoring automatically.
    
- **Benefit:**
    
    - Reduces operational burden.
        
    - Built-in high availability and disaster recovery.
        

## 8.Â **Namespacing, RBAC, and Multi-Tenancy**

- **Modern Solution:**  
    Modern systems offer fine-grained access control, namespaces, and multi-tenancy support (e.g., etcd namespaces, Consul ACLs).
    
- **Benefit:**
    
    - Secure isolation between applications.
        
    - Easier to run multiple workloads on the same cluster.
        

---

## **Summary Table**

|ZooKeeper Limitation|Modern Mitigation|Example Technologies|
|---|---|---|
|Centralized leader bottleneck|Sharding, decentralized consensus|etcd, Consul, Spanner|
|Stale reads|Configurable consistency per request|etcd v3, Consul|
|Limited data model|Enhanced KV stores, richer types|Consul KV, Redis|
|No built-in primitives|Standardized concurrency APIs|etcd concurrency, Consul sessions|
|Session/ephemeral issues|Adaptive failure detectors, fencing|SWIM, Consul, etcd leases|
|Small ensemble size|Horizontally scalable consensus|Spanner, CockroachDB|
|Operational complexity|Managed/cloud-native services|AWS, GCP, Azure managed services|
|No namespaces or RBAC|Namespacing, fine-grained ACLs|etcd namespaces, Consul ACLs|

[[Distributed Guide]]
[[999. Questions]]