## ğŸ“Œ 1. Introduction

- Facebook's scale demands high performance: serving **billions of requests per second** to **over a billion users**.
- Traditional databases alone canâ€™t handle this scale efficiently.
- **Memcached** is a lightweight, in-memory key-value store used to reduce database load.
- Facebook **customized and scaled** memcached to work as a **distributed caching system** (called **memcache** in the paper).
- Itâ€™s crucial for speeding up page loads â€” pages may need **thousands of key-value lookups**.
- Main takeaway: Performance, efficiency, fault-tolerance, and consistency are vital, but different deployment scales emphasize different trade-offs.

---

## ğŸ“Œ 2. Overview

- Facebook users **read more than they write** â†’ cache can reduce read load.
- Data comes from multiple sources (MySQL, HDFS, services) â†’ requires **flexible caching**.
- Facebook uses memcache in two ways:
    - **Query cache (look-aside cache)**: Web server asks memcache first. On a miss, it queries the DB, then sets the value in memcache.
        ![[Pasted image 20250516065452.png]]
    - **Generic cache**: For storing results of machine learning models, etc.
- Memcached itself is single-server. Facebook builds:
    - Configuration services
    - Aggregation logic
    - Routing infrastructure â†’ to form a **distributed system**.

---

## ğŸ“Œ 3. In a Cluster: Latency and Load

### ğŸ”¹ 3.1 Reducing Latency
- A single user request may need **hundreds of memcache lookups**.
    
- Items are sharded across servers using **consistent hashing**.
    
- Challenges:
    
    - **All-to-all communication** can cause congestion.
        
    - Replication can help, but uses more memory.
        

**Solutions:**

- **Smart client logic** (runs on web servers):
    
    - Handles serialization, routing, error handling, batching.
        
- **Parallel fetches**: Use a **DAG** to determine which keys to fetch in parallel.
    
- **UDP for get requests**: Faster and connectionless.
    
    - But has ~0.25% drop rate â†’ treat as cache miss.
        
- **TCP for sets/deletes**: Reliability is more important here.
    
- **Mcrouter**: A proxy that routes requests and helps coalesce TCP connections.
    

**Incast Congestion Control**:

- Use a **sliding window protocol** like TCP.
    
- Balances:
    
    - Small windows â†’ too many sequential requests.
        
    - Large windows â†’ network congestion.
        
- Graphs show how adjusting this improves scheduling latency.
    

---

### ğŸ”¹ 3.2 Reducing Load

#### ğŸ”¸ 3.2.1 Leases

- Fixes:
    
    - **Stale sets**: When outdated data gets written due to reordering.
        
    - **Thundering herds**: Many clients try to fetch & update the same key at once.
        

**Leases**:

- Cache gives a **token** (lease) on a miss.
    
- Client can only set if it still holds a valid lease.
    
- Limits who gets to populate cache â†’ avoids thundering herds.
    
- Reduced peak DB queries from 17K/s â†’ 1.3K/s in tests.
    

#### ğŸ”¸ 3.2.2 Memcache Pools

- Different data has different needs:
    
    - Some change rapidly (high-churn).
        
    - Some are accessed rarely but expensive to compute.
        
- **Pools**: Segregate workloads:
    
    - **Wildcard pool**: Default.
        
    - **High-churn pool**: Small and fast.
        
    - **Low-churn pool**: Bigger but accessed infrequently.
        

#### ğŸ”¸ 3.2.3 Replication Within Pools

- Useful when:
    
    - Data fits on one/two servers.
        
    - High request rates.
        
- Replicating keys reduces bottlenecks.
    
    - Clients pick a replica based on IP.
        
    - Invalidation sent to all replicas.
        

---

### ğŸ”¹ 3.3 Handling Failures

- **Two failure types**:
    
    1. Few hosts go down.
        
    2. Major outage.
        

**Gutter Pool**:

- 1% of memcached servers.
    
- Used to temporarily handle traffic for failed servers.
    
- Stores keys with short TTLs.
    
- Converts 10â€“25% of failed requests into hits.
    
- Helps prevent DB overload.
    

---

## ğŸ“Œ 4. In a Region: Replication

Clusters are grouped into **regions** (frontend + storage clusters).

### ğŸ”¹ 4.1 Regional Invalidations

- **Storage cluster** is authoritative â†’ sends invalidations.
    
- **mcsqueal** daemon:
    
    - Extracts which keys to delete from SQL commits.
        
    - Sends batched invalidations to **mcrouters** in all frontend clusters.
        
    - Improves efficiency (18Ã— more deletes per packet).
        

**Why not let web servers handle invalidations?**

- Too many packets.
    
- Harder to fix issues.
    

### ğŸ”¹ 4.2 Regional Pools

- **Problem**: Same data might be cached in all frontend clusters â†’ waste of memory.
    
- **Regional Pool**: Shared across clusters.
    
- Use regional pools for:
    
    - Rarely accessed data.
        
    - Large values.
        

**Tradeoff**:

- Saves memory.
    
- Increases inter-cluster latency.
    

### ğŸ”¹ 4.3 Cold Cluster Warmup

- When a new cluster starts up â†’ cache is empty â†’ massive DB load.
    
- **Solution**: Cold cluster fetches from warm clusters instead of DB.
    
- Avoids inconsistency via:
    
    - Two-second â€œhold-offâ€ deletes.
        
    - Re-fetch from DB if newer data is detected.
        

---

## ğŸ“Œ 5. Across Regions: Consistency

Facebook has multiple **geographic regions** (master + replicas).

**Challenges**:

- **Replication lag**: Replicas can be behind the master.
    
- Causes inconsistency if memcache and replica DBs donâ€™t match.
    

**Remote Marker Mechanism**:

- When a key is updated:
    
    1. Set remote marker `rk` in the region.
        
    2. Write to master DB (with `k` and `rk`).
        
    3. Delete key locally.
        
- If future fetch sees `rk`, it redirects the query to the **master**.
    

**Design trade-off**:

- Slight increase in latency.
    
- Greatly reduces risk of stale reads.
    

---

## ğŸ“Œ 6. Single Server Improvements

### ğŸ”¹ 6.1 Performance Optimizations

- Replaced global lock with **fine-grained locks**.
    
- Added multi-threading, per-thread UDP ports.
    
- **Results**:
    
    - Hits: 600k â†’ 1.8M ops/sec.
        
    - Misses: 2.7M â†’ 4.5M ops/sec.
        
    - UDP is 13% faster than TCP for gets.
        

### ğŸ”¹ 6.2 Adaptive Slab Allocator

- Memcached uses slab allocator (fixed-size chunks).
    
- Facebook added:
    
    - **Adaptive rebalancing** between slab classes based on **item age**, not just eviction rate.
        

### ğŸ”¹ 6.3 Transient Item Cache

- Memcached lazy-evicts expired keys â†’ wastes memory.
    
- Facebook adds **proactive eviction**:
    
    - Short-lived keys tracked via circular buffer.
        
    - Items evicted as soon as their TTL expires.
        

### ğŸ”¹ 6.4 Software Upgrades

- Upgrades used to flush cache â†’ bad for performance.
    
- Introduced **System V shared memory**:
    
    - Keeps data during upgrades.
        
    - Faster recovery.
        

---

## ğŸ“Œ 7. Memcache Workload

### ğŸ”¹ 7.1 Web Server Metrics

- 56% of requests touch <20 memcache servers.
    
- Some touch hundreds.
    
- Median response size: **135B**; Mean: **954B** â†’ high variance.
    
- Median latency: **333Î¼s**; 95th percentile: **1.135ms**.
    

### ğŸ”¹ 7.2 Pool Stats

- Different pools:
    
    - **Wildcard**: Default.
        
    - **App**: App-specific, high churn.
        
    - **Replicated**: High traffic.
        
    - **Regional**: Low traffic, large values.
        
- Replicated pool has highest get rate with smallest items.
    

### ğŸ”¹ 7.3 Invalidation Latency

- Delete reliability:
    
    - Master region: 99.99% within 1s.
        
    - Replica region: 99.9% within 10s.
        
- Most issues recover with retries.
    

---

## ğŸ“Œ 8. Related Work

- Mentions other systems like:
    
    - Dynamo (Amazon)
        
    - Voldemort (LinkedIn)
        
    - Redis, Twitterâ€™s Twemcache, Facebookâ€™s TAO
        
- Memcache is chosen for **simplicity and performance**.
    
- Builds on prior research in **leases**, **replication**, and **distributed caches**.
    

---

## ğŸ“Œ 9. Conclusion ğŸ¯

Key lessons:

1. Separate cache and persistent storage â†’ easier scaling.
    
2. Operational tools (monitoring, debugging) are as important as raw performance.
    
3. Stateless clients (mcrouter) simplify deployments.
    
4. Gradual rollouts are essential.
    
5. Simplicity is key at large scale


[[Distributed Guide]]
[[999. Questions]]