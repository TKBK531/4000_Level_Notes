## ğŸ“˜ Abstract

- Google built **GFS** to handle **large-scale, data-intensive applications**.
- It runs on **commodity hardware** (cheap machines) ğŸ–¥ï¸.
- Designed for **fault tolerance**, **scalability**, and **high throughput** âš¡.
- Serves hundreds of clients accessing petabytes of data in real time ğŸ’½.

---

## ğŸ”¹ 1. Introduction

Google faced big challenges:
- ğŸ’¥ Failures are normal (due to cheap hardware).
- ğŸ“‚ Files are **very large** (multi-GB or TB).
- ğŸ“ Most files are **append-only**, not overwritten.
- ğŸ› ï¸ Co-designed APIs with applications (like **atomic appends**).
- ğŸ“ˆ Prioritized **high bandwidth over low latency**.

â¡ï¸ Result: GFS is optimized for **big data** and **streaming workloads**, not general-purpose OS filesystems.

### *Q1: Why did Google design GFS instead of using existing distributed file systems?*

**A1:**Â Existing systems (e.g., AFS, NFS) were optimized for small files and low-latency access, while Google needed:
- **High throughput for large files**Â (multi-GB).
- **Fault tolerance on commodity hardware**.
- **Efficient appends**Â (for MapReduce/Bigtable workloads).

### *Q2: What are the three key workload observations that shaped GFS?*

**A2:**
1. **Component failures are common**Â â†’ Automatic recovery.
2. **Files are huge**Â â†’ 64 MB chunks.
3. **Most writes are appends**Â â†’ Optimized for sequential I/O.

---

## ğŸ”¹ 2. Design Overview

### ğŸ”¸ 2.1 Assumptions

GFS is built for:
- Frequent **hardware failures** ğŸ’£
- Fewer, **very large files** ğŸ“‚
- **Large streaming reads** and **appends**, not small updates ğŸš€
- **Many concurrent appends** to the same file ğŸ”„
- Focus on **bandwidth**, not latency â©

#### *Q1: Why does GFS assume "files are rarely overwritten"?*
**A1:**Â Overwrites require random writes, which are:
- **Slow on distributed systems**.
- **Hard to synchronize across replicas**.
- Append-only simplifies consistency (e.g., logs, batch jobs).

#### *Q2: How does GFS prioritize bandwidth over latency?*
**A2:**Â It batches writes and usesÂ **pipelined data transfer**Â between chunkservers, favoringÂ **throughput**Â (MB/s) overÂ **low latency**Â (ms).

---

### ğŸ”¸ 2.2 Interface
- Similar to normal file systems (create, read, write, delete) ğŸ—‚ï¸
- Plus two new operations:
    - **Snapshot** ğŸ“¸: Fast copy of a file or directory tree
    - **Record Append** ğŸ§¾: Atomic, parallel appends (ideal for producer-consumer patterns)
#### *Q1: Why does GFS supportÂ `snapshot`Â andÂ `record append`Â but not POSIX?*
**A1:**
- **Snapshot**: Fast copies for backups (critical for big data).
- **Record append**: Atomic concurrent writes (for producer-consumer queues).
- **No POSIX**: Too restrictive for Googleâ€™s scale (e.g., no strict consistency).
#### *Q2: How doesÂ `record append`Â differ from a traditional write?*
**A2:**
- **Traditional write**: Client specifies offset (risk of conflicts).
- **Record append**: GFS assigns offset (ensures atomicity).

---

### ğŸ”¸ 2.3 Architecture

ğŸ“Š GFS = **1 Master** + **many Chunkservers** + **many Clients**:
- **Files are split into 64MB chunks**.
- **Each chunk is replicated (default = 3 times)** for fault tolerance ğŸ”.
- **Master** stores metadata (names, chunk locations, permissions) ğŸ§ .
- Clients talk to the master _only_ for metadata, and then talk **directly** to chunkservers for data ğŸš›.

---

### ğŸ”¸ 2.4 Single Master

ğŸ“Œ Benefits:
- Centralized control makes intelligent decisions.
- Keeps metadata in memory = fast âš¡

ğŸ“‰ Avoids bottlenecks:
- Clients **cache** metadata from the master.
- Reads/writes go **directly to chunkservers**, not through the master ğŸš«ğŸ§‘â€ğŸ«.

![[Pasted image 20250518095609.png]]

---

### ğŸ”¸ 2.5 Chunk Size

âœ… Chosen size: **64MB**
- ğŸ§  Benefits:
    - Fewer requests to master
    - Efficient use of memory (less metadata)
    - Fewer TCP connections

- âš ï¸ Downside:    
    - **Hotspots** if many clients hit the same small file (solved by high replication)

---

### ğŸ”¸ 2.6 Metadata
- Stored **entirely in masterâ€™s memory** ğŸ’¾.
- 3 key types:
    1. **Namespace** (paths of files)
    2. **File-to-chunk mapping**
    3. **Chunk locations**
        
- Metadata updates are logged (operation log) and **replicated** for reliability ğŸ”.

---

### ğŸ”¸ 2.7 Consistency Model
ğŸ§© GFS provides **relaxed consistency**, suitable for append-heavy workloads.
- âœ… **Defined Region** = written correctly, same across replicas.
- â“ **Undefined Region** = concurrent writes â†’ result is consistent but **not predictable**.
- âŒ **Inconsistent Region** = failed write â†’ replicas disagree.

**Writes vs. Record Appends**:
- **Write**: user picks offset (risk of overlap)
- **Record Append**: GFS picks offset â†’ guarantees **atomic append** âœ…

---

## ğŸ”¹ 3. System Interactions

### ğŸ”¸ 3.1 Leases and Mutation Order
âœï¸ GFS uses **leases**:
- Master grants **one replica** the **lease (primary)**.
- Primary assigns **order of writes**.
- Others (**secondaries**) follow that order to ensure consistency âœ….

![[Pasted image 20250518095754.png]]
    

---

### ğŸ”¸ 3.2 Data Flow
ğŸ“¦ Data is **pushed to chunkservers** before the control signal.
- Uses **pipeline style** to maximize network throughput.
- Chooses â€œclosestâ€ chunkserver for speed ğŸŒ.

---
### ğŸ”¸ 3.3 Record Appends
- Client does not choose the offset.
- Primary finds offset and appends atomically ğŸ”’.
- **Duplicates or padding** may appear, but apps handle that using:
    - Checksums âœ…
    - Unique IDs ğŸ”

---

### ğŸ”¸ 3.4 Snapshots
- Master clones metadata only.
- **Copy-on-write** happens only when write follows.
- Efficient for backup and experiments ğŸ§ªğŸ“‚

---

## ğŸ”¹ 4. Master Operation
### ğŸ”¸ 4.1 Namespace Management and Locking
- GFS uses **fine-grained locks** (read/write per pathname) ğŸ”’.
- Allows **concurrent operations** on different files in the same directory âœ….

---

### ğŸ”¸ 4.2 Replica Placement
- Spread across:
    - Machines
    - Racks ğŸ§±

â¡ï¸ Improves fault tolerance (e.g., power loss to a rack wonâ€™t destroy all replicas) ğŸ”Œ

---

### ğŸ”¸ 4.3 Re-replication & Rebalancing
- Triggers:
    - Server failure
    - Corruption
    - More replicas needed
- Rebalancing = redistributing chunks to balance space and load âš–ï¸

---

### ğŸ”¸ 4.4 Garbage Collection

ğŸ—‘ï¸ Lazy deletion:
- File is hidden â†’ deleted later by background thread.
- Helps prevent accidental deletions.
- Allows "undelete" during grace period ğŸ§¹

---

### ğŸ”¸ 4.5 Stale Replica Detection

ğŸ“› Version numbers track replicas.
- When server misses a write, its version is outdated = **stale** â›”.
- Master ignores stale replicas in reads & garbage collects them ğŸ§¼.

---

## ğŸ”¹ 5. Fault Tolerance & Diagnosis
### ğŸ”¸ 5.1 High Availability

- **Fast restart** for servers âš¡
- **Replication** ensures data survives even if machines crash.
- **Master replication** with "shadow masters" allows **read-only access** during master outages ğŸ•µï¸

---

### ğŸ”¸ 5.2 Data Integrity
ğŸ§ª Checksums on 64KB blocks:
- Catch corruption before reading/sending ğŸ§±
- If failed, fallback to other replicas.
- Background checks during idle time to catch rare corruption.

---

### ğŸ”¸ 5.3 Diagnostic Tools
ğŸ“œ Extensive **logs** for:
- Every operation    
- All messages (RPCs)
- Used for debugging, profiling, performance tuning ğŸ› ï¸

---

## ğŸ”¹ 6. Measurements
ğŸ“Š Benchmarked on 16 chunkservers and 16 clients.
### ğŸ”¸ 6.1 Micro-benchmarks
- **Read throughput**: Up to 94 MB/s ğŸ“–
- **Write throughput**: Up to 35 MB/s âœï¸
- **Record append**: ~5 MB/s with 16 clients â•

---

### ğŸ”¸ 6.2 Real Clusters (A & B)

| Cluster        | Chunkservers | Used Disk | Ops/sec  |
| -------------- | ------------ | --------- | -------- |
| A (R&D)        | 342          | 55 TB     | ~200-300 |
| B (Production) | 227          | 155 TB    | ~500     |

ğŸ“ˆ Shows GFS handles **massive data and concurrent ops efficiently**.

---

### ğŸ”¸ 6.3 Workload Breakdown

- Most reads are:
    - **Small random reads** (e.g., lookups)
    - **Large sequential reads** (e.g., analysis)
- Most writes are:
    - **Large appends**

â¡ï¸ Matches Googleâ€™s use cases: web crawling, indexing, logging, ML data processing ğŸ’¼

---

## ğŸ Summary

**GFS** is a custom-built, highly optimized distributed file system for:
- Huge files (multi-GB/TB) ğŸ“‚
- Streaming reads and append-only writes ğŸ§¾
- Fault-tolerant, high-throughput environments âš™ï¸
- Simplified consistency model that works well for Googleâ€™s workloads ğŸ§ 

[[Distributed Guide]]
[[999. Questions]]