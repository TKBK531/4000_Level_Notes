## 📘 Abstract

- Google built **GFS** to handle **large-scale, data-intensive applications**.
- It runs on **commodity hardware** (cheap machines) 🖥️.
- Designed for **fault tolerance**, **scalability**, and **high throughput** ⚡.
- Serves hundreds of clients accessing petabytes of data in real time 💽.

---

## 🔹 1. Introduction

Google faced big challenges:
- 💥 Failures are normal (due to cheap hardware).
- 📂 Files are **very large** (multi-GB or TB).
- 📝 Most files are **append-only**, not overwritten.
- 🛠️ Co-designed APIs with applications (like **atomic appends**).
- 📈 Prioritized **high bandwidth over low latency**.

➡️ Result: GFS is optimized for **big data** and **streaming workloads**, not general-purpose OS filesystems.

### *Q1: Why did Google design GFS instead of using existing distributed file systems?*

**A1:** Existing systems (e.g., AFS, NFS) were optimized for small files and low-latency access, while Google needed:
- **High throughput for large files** (multi-GB).
- **Fault tolerance on commodity hardware**.
- **Efficient appends** (for MapReduce/Bigtable workloads).

### *Q2: What are the three key workload observations that shaped GFS?*

**A2:**
1. **Component failures are common** → Automatic recovery.
2. **Files are huge** → 64 MB chunks.
3. **Most writes are appends** → Optimized for sequential I/O.

---

## 🔹 2. Design Overview

### 🔸 2.1 Assumptions

GFS is built for:
- Frequent **hardware failures** 💣
- Fewer, **very large files** 📂
- **Large streaming reads** and **appends**, not small updates 🚀
- **Many concurrent appends** to the same file 🔄
- Focus on **bandwidth**, not latency ⏩

#### *Q1: Why does GFS assume "files are rarely overwritten"?*
**A1:** Overwrites require random writes, which are:
- **Slow on distributed systems**.
- **Hard to synchronize across replicas**.
- Append-only simplifies consistency (e.g., logs, batch jobs).

#### *Q2: How does GFS prioritize bandwidth over latency?*
**A2:** It batches writes and uses **pipelined data transfer** between chunkservers, favoring **throughput** (MB/s) over **low latency** (ms).

---

### 🔸 2.2 Interface
- Similar to normal file systems (create, read, write, delete) 🗂️
- Plus two new operations:
    - **Snapshot** 📸: Fast copy of a file or directory tree
    - **Record Append** 🧾: Atomic, parallel appends (ideal for producer-consumer patterns)
#### *Q1: Why does GFS support `snapshot` and `record append` but not POSIX?*
**A1:**
- **Snapshot**: Fast copies for backups (critical for big data).
- **Record append**: Atomic concurrent writes (for producer-consumer queues).
- **No POSIX**: Too restrictive for Google’s scale (e.g., no strict consistency).
#### *Q2: How does `record append` differ from a traditional write?*
**A2:**
- **Traditional write**: Client specifies offset (risk of conflicts).
- **Record append**: GFS assigns offset (ensures atomicity).

---

### 🔸 2.3 Architecture

📊 GFS = **1 Master** + **many Chunkservers** + **many Clients**:
- **Files are split into 64MB chunks**.
- **Each chunk is replicated (default = 3 times)** for fault tolerance 🔁.
- **Master** stores metadata (names, chunk locations, permissions) 🧠.
- Clients talk to the master _only_ for metadata, and then talk **directly** to chunkservers for data 🚛.

---

### 🔸 2.4 Single Master

📌 Benefits:
- Centralized control makes intelligent decisions.
- Keeps metadata in memory = fast ⚡

📉 Avoids bottlenecks:
- Clients **cache** metadata from the master.
- Reads/writes go **directly to chunkservers**, not through the master 🚫🧑‍🏫.

![[Pasted image 20250518095609.png]]

---

### 🔸 2.5 Chunk Size

✅ Chosen size: **64MB**
- 🧠 Benefits:
    - Fewer requests to master
    - Efficient use of memory (less metadata)
    - Fewer TCP connections

- ⚠️ Downside:    
    - **Hotspots** if many clients hit the same small file (solved by high replication)

---

### 🔸 2.6 Metadata
- Stored **entirely in master’s memory** 💾.
- 3 key types:
    1. **Namespace** (paths of files)
    2. **File-to-chunk mapping**
    3. **Chunk locations**
        
- Metadata updates are logged (operation log) and **replicated** for reliability 🔐.

---

### 🔸 2.7 Consistency Model
🧩 GFS provides **relaxed consistency**, suitable for append-heavy workloads.
- ✅ **Defined Region** = written correctly, same across replicas.
- ❓ **Undefined Region** = concurrent writes → result is consistent but **not predictable**.
- ❌ **Inconsistent Region** = failed write → replicas disagree.

**Writes vs. Record Appends**:
- **Write**: user picks offset (risk of overlap)
- **Record Append**: GFS picks offset → guarantees **atomic append** ✅

---

## 🔹 3. System Interactions

### 🔸 3.1 Leases and Mutation Order
✏️ GFS uses **leases**:
- Master grants **one replica** the **lease (primary)**.
- Primary assigns **order of writes**.
- Others (**secondaries**) follow that order to ensure consistency ✅.

![[Pasted image 20250518095754.png]]
    

---

### 🔸 3.2 Data Flow
📦 Data is **pushed to chunkservers** before the control signal.
- Uses **pipeline style** to maximize network throughput.
- Chooses “closest” chunkserver for speed 🌐.

---
### 🔸 3.3 Record Appends
- Client does not choose the offset.
- Primary finds offset and appends atomically 🔒.
- **Duplicates or padding** may appear, but apps handle that using:
    - Checksums ✅
    - Unique IDs 🔍

---

### 🔸 3.4 Snapshots
- Master clones metadata only.
- **Copy-on-write** happens only when write follows.
- Efficient for backup and experiments 🧪📂

---

## 🔹 4. Master Operation
### 🔸 4.1 Namespace Management and Locking
- GFS uses **fine-grained locks** (read/write per pathname) 🔒.
- Allows **concurrent operations** on different files in the same directory ✅.

---

### 🔸 4.2 Replica Placement
- Spread across:
    - Machines
    - Racks 🧱

➡️ Improves fault tolerance (e.g., power loss to a rack won’t destroy all replicas) 🔌

---

### 🔸 4.3 Re-replication & Rebalancing
- Triggers:
    - Server failure
    - Corruption
    - More replicas needed
- Rebalancing = redistributing chunks to balance space and load ⚖️

---

### 🔸 4.4 Garbage Collection

🗑️ Lazy deletion:
- File is hidden → deleted later by background thread.
- Helps prevent accidental deletions.
- Allows "undelete" during grace period 🧹

---

### 🔸 4.5 Stale Replica Detection

📛 Version numbers track replicas.
- When server misses a write, its version is outdated = **stale** ⛔.
- Master ignores stale replicas in reads & garbage collects them 🧼.

---

## 🔹 5. Fault Tolerance & Diagnosis
### 🔸 5.1 High Availability

- **Fast restart** for servers ⚡
- **Replication** ensures data survives even if machines crash.
- **Master replication** with "shadow masters" allows **read-only access** during master outages 🕵️

---

### 🔸 5.2 Data Integrity
🧪 Checksums on 64KB blocks:
- Catch corruption before reading/sending 🧱
- If failed, fallback to other replicas.
- Background checks during idle time to catch rare corruption.

---

### 🔸 5.3 Diagnostic Tools
📜 Extensive **logs** for:
- Every operation    
- All messages (RPCs)
- Used for debugging, profiling, performance tuning 🛠️

---

## 🔹 6. Measurements
📊 Benchmarked on 16 chunkservers and 16 clients.
### 🔸 6.1 Micro-benchmarks
- **Read throughput**: Up to 94 MB/s 📖
- **Write throughput**: Up to 35 MB/s ✍️
- **Record append**: ~5 MB/s with 16 clients ➕

---

### 🔸 6.2 Real Clusters (A & B)

| Cluster        | Chunkservers | Used Disk | Ops/sec  |
| -------------- | ------------ | --------- | -------- |
| A (R&D)        | 342          | 55 TB     | ~200-300 |
| B (Production) | 227          | 155 TB    | ~500     |

📈 Shows GFS handles **massive data and concurrent ops efficiently**.

---

### 🔸 6.3 Workload Breakdown

- Most reads are:
    - **Small random reads** (e.g., lookups)
    - **Large sequential reads** (e.g., analysis)
- Most writes are:
    - **Large appends**

➡️ Matches Google’s use cases: web crawling, indexing, logging, ML data processing 💼

---

## 🏁 Summary

**GFS** is a custom-built, highly optimized distributed file system for:
- Huge files (multi-GB/TB) 📂
- Streaming reads and append-only writes 🧾
- Fault-tolerant, high-throughput environments ⚙️
- Simplified consistency model that works well for Google’s workloads 🧠

[[Distributed Guide]]
[[999. Questions]]