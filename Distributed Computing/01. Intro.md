#### **1. Definition of Distributed Systems**

- **Definition**: A system with multiple independent nodes (computers/servers) that communicate to achieve a common goal.

- **Key Points**:
    - Geographically distributed nodes.
    - Autonomous but collaborative components.
    - Examples: Cloud systems (AWS), blockchain, CDNs (Cloudflare).


---

#### **2. Characteristics of Distributed Systems**

1. **Transparency** (Access, Location, Replication, Concurrency).
2. **Scalability**: Horizontal (add nodes) vs. Vertical (upgrade nodes).
3. **Fault Tolerance**: Replication, recovery mechanisms.
4. **Concurrency**: Parallel processing without interference.
5. **Openness**: Interoperability with other systems.
6. **Security**: Authentication, encryption, access control.
7. **Heterogeneity**: Diverse hardware/software components.


---

#### **3. Why Use Distributed Systems?**

- **Resource Sharing**: Efficient workload distribution.
- **Reliability**: Redundancy for failure recovery.
- **Performance**: Parallel processing improves speed.

---

#### **4. Types of Distributed Systems**

- **Client-Server**: Centralized server (e.g., web apps).
- **Peer-to-Peer**: Nodes act as clients/servers (e.g., BitTorrent).
- **Cloud-based**: On-demand resources (AWS, Google Cloud).
- **Grid Computing**: Harnessing power from diverse systems.

---

#### **5. Communication in Distributed Systems**

- **Message Passing**: Synchronous (wait for response) vs. Asynchronous (no wait).

- **Protocols**:
    - **TCP/IP**: Reliable but slower.
    - **UDP**: Fast but unreliable.

- **RPC**: Remote Procedure Call for task execution.

---

#### **6. Key Challenges**

- **Latency**: Communication delays.
- **Network Partitioning**: Disconnected nodes.
- **Consistency**: Ensuring uniform data across nodes (CAP Theorem).
- **Clock Drift**: Time synchronization issues (solved via NTP or Lamport’s Logical Clocks).

---

#### **7. Consistency & CAP Theorem**

- **CAP Theorem**: A system can only guarantee two of:
    - **Consistency** (all nodes see the same data).
    - **Availability** (every request gets a response).
    - **Partition Tolerance** (works despite network splits).

- **Types**: Strong vs. Eventual Consistency.

---

#### **8. Consensus Algorithms**

- **Purpose**: Achieve agreement among nodes despite failures.

- **Examples**:
    - **Paxos**: Complex but robust.
    - **Raft**: Simplified for ease of understanding.

---

#### **9. Fault Tolerance**

- **Replication**: Storing data copies across nodes.
- **Recovery**: Checkpointing, logging.
- **Fault Detection**: Identifying node failures.

---

#### **10. Case Studies**

- **Google File System (GFS)**:
    - Master-slave architecture.
    - Large block sizes, fault tolerance via replication.

- **Blockchain**:
    - Decentralized ledger with cryptographic security.
    - Consensus mechanisms (e.g., Proof of Work).

---

#### **11. Security**

- **Authentication**: Verify node identity.
- **Encryption**: Secure data transmission.
- **Threats**: MITM attacks, DoS.

---

#### **12. Scalability**

- **Horizontal Scaling**: Adding more nodes.
- **Vertical Scaling**: Upgrading node resources.
- **Load Balancing**: Distribute tasks evenly.

[[Distributed Guide]]
[[02. Charasteristics]]
[[Q & A]]