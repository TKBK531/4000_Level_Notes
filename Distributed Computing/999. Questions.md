
[[Distributed Guide]]

## [[101. MapReduce]]

#### ğŸ”¢ **1. Scenario: Counting Hashtags in Tweets**

**Scenario:** Youâ€™re working with a dataset containing millions of tweets. Your goal is to count how many times each hashtag appears.

**Question:** How would you design a MapReduce job for this task? Explain the role of Map, Shuffle, and Reduce in your solution.

**Answer:**

- **Map phase:** For each tweet, extract all hashtags and emit (hashtag, 1) ğŸ“¥.
    
- **Shuffle phase:** Group all values by hashtag (e.g., #AI â†’ [1, 1, 1, 1, â€¦]) ğŸ”€.
    
- **Reduce phase:** Sum the counts for each hashtag â†’ emit (#AI, 47329) â•.
    

ğŸ‘‰ **Optimization tip:** Use a combiner to partially sum up counts before sending them over the network.

---

#### ğŸ“š **2. Scenario: Building a Search Index (Inverted Index)**

**Scenario:** You have a corpus of web pages and want to build an inverted index that maps each word to the list of documents it appears in.

**Question:** Explain how MapReduce can be used to build the inverted index.

**Answer:**

- **Map phase:** For each document, emit (word, documentID) ğŸ“.
    
- **Shuffle phase:** Group values by word â†’ (word â†’ [doc1, doc2, doc5]) ğŸ”€.
    
- **Reduce phase:** Deduplicate and sort document IDs, then emit final (word, [docIDs]) ğŸ“š.
    

âœ… This allows fast lookup of documents containing a keyword!

---

#### ğŸ§¾ **3. Scenario: Log Analysis**

**Scenario:** Your company logs all website traffic. You want to find the number of visits per country per day.

**Question:** How would you apply MapReduce to solve this?

**Answer:**

- **Map phase:** For each log entry, extract (country, date) and emit ((country, date), 1) ğŸŒğŸ“….
    
- **Shuffle phase:** Group by (country, date).
    
- **Reduce phase:** Count entries per key â†’ emit ((country, date), total_visits) ğŸ§®.
    

ğŸ“¦ You could also use a partitioning function to ensure all logs for the same country go to the same reducer.

---

#### ğŸ” **4. Scenario: Duplicate Record Removal**

**Scenario:** Youâ€™re given a large dataset with duplicate records. You want to remove duplicates efficiently.

**Question:** Describe a MapReduce strategy to achieve this.

**Answer:**

- **Map phase:** Emit (record, null) for each input record ğŸ“¤.
    
- **Shuffle phase:** All duplicates are grouped together.
    
- **Reduce phase:** Emit only one instance of each record â–.
    

ğŸŒŸ Easy way to clean large datasets!

---

#### ğŸ“Š **5. Scenario: Top-K Frequent Products Sold**

**Scenario:** From a massive sales dataset, find the top 10 most frequently sold products.

**Question:** Explain a MapReduce approach to this.

**Answer:**

- **Map phase:** Emit (productID, 1) per sale.
    
- **Combiner:** Sum local counts ğŸ’¡.
    
- **Reduce phase:** Sum global counts per product â†’ emit (productID, total_sales) ğŸ§¾.
    
- **Post-process step (optional MapReduce):** Another job to sort and select top 10 ğŸ’¯.
    

ğŸ§  You might sort results using a custom partitioner and reducer.

---

#### ğŸ¯ **6. Scenario: Personalized Recommendations**

**Scenario:** You need to calculate how many times each user clicked on each product.

**Question:** How would you use MapReduce to generate user-product interaction counts?

**Answer:**

- **Map phase:** Emit ((userID, productID), 1) for each interaction ğŸ”.
    
- **Reduce phase:** Sum values for each (userID, productID) pair â†’ emit ((userID, productID), count) ğŸ‘¤ğŸ“¦.
    

ğŸ“ˆ Can be used as input to collaborative filtering models later.

---

#### âœ¨ Extra Tips for Answering Scenario Questions

When you see a MapReduce scenario:

1. **Break the data into key/value format** ğŸ¯
    
2. **Describe what the Map function will emit** ğŸ§ 
    
3. **Explain the grouping logic in the shuffle phase** ğŸ”ƒ
    
4. **Describe how the Reduce function aggregates the results** â•
    

ğŸ‘‰ Always consider:

- **Data skew** (uneven keys) ğŸ§®
    
- **Combiner usage** to reduce network load ğŸŒ
    
- **Partitioning strategy** if data locality or grouping matters ğŸ“


### **Question 1**

This question is based on MapReduce, Fundamentals of Distributed Systems, Linearizability, and Go Programming.

Assume that an ecommerce company is interested in finding the hit-counts of its products. Their web solution generates log files of customer hits in the format given below:
```
time_stamp customer_id product_id
```
It creates a new log file either after 10 million records or every three days, whichever comes first. The company is willing to investigate over ~600 files of the last 5 years.

**i.**Â Define theÂ **Map function**Â with details and explain its functionality by stating any assumptions you made. (You can provide a Pseudo code if you may)

**ii.**Â Define theÂ **Reduce function**Â and explain its functionality. (You can provide a Pseudo code if you may)

**iii.**Â Assume your Map function writes intermediate output toÂ `intermediate-<map_task_file_id>.csv`Â in the file system that is shared by all the workers using only the following code snippet. This is done without writing to a temporary file in its local storage. Is there any issue with this implementation? If yes, explain in detail why. If not, defend why not.

**Go programming code:**

```
outputfile := fmt.Sprintf("intermediate-%d", map_task_file_id)
file_ := os.Create(outputfile)
//write data to file while running.map reduce
file.Close()
```
**iv.**Â Suggest any modification to the above code that can mitigate the issues you have mentioned above. (Skip if there is no issue identified; marks will be allocated to part iii.)

**v.**Â When investigating the task files, it was noticed that:

- Most files had less than 10 million records.
    
- Files with 10 million records (~1 GB in size) constituted aboutÂ `1/3`Â of all files.
    
- `1/2`Â of the files were between `500 MB â€“ 750 MB`.
    
- The rest were less than `500 MB`.
    

Comment on theÂ **Master algorithm**Â that might best support load balancing and processing time, considering 10 homogeneous workers in a work pool.


### **ğŸ“Œ i. Map Function Definition & Explanation**

**ğŸ¯ Functionality**:  
TheÂ **Map**Â function processes each log entry to generate intermediate key-value pairs. For the e-commerce hit-count problem:

- **Input**:Â `(timestamp, customer_id, product_id)`
    
- **Output**:Â `(product_id, 1)`Â _(whereÂ `1`Â represents a single hit)_
    

**ğŸ” Assumptions**:  
âœ” Each log line is well-formatted.  
âœ”Â `product_id`Â is unique and suitable for aggregation.

**ğŸ“œ Pseudo-Code**:
```
def Map(line):
    timestamp, customer_id, product_id = line.split()
    EmitIntermediate(product_id, "1")  # Emit (product_id, 1)
```
**ğŸ“– Explanation**:

- Parses each log line â†’ extractsÂ `product_id`Â â†’ emitsÂ `(product_id, 1)`Â for counting.
    
- Follows theÂ **MapReduce**Â model (Section 2 of the paper).
    

**ğŸ”— Reference**:  
ğŸ“„Â _Paper Example_: Word counting (Section 2.1).

---

### **ğŸ“Œ ii. Reduce Function Definition & Explanation**

**ğŸ¯ Functionality**:  
Aggregates counts for eachÂ `product_id`Â to compute total hits.

**ğŸ“œ Pseudo-Code**:
```
def Reduce(product_id, counts):
    total_hits = 0
    for count in counts:
        total_hits += int(count)
    Emit(product_id, total_hits)
```
**ğŸ“– Explanation**:

- Sums allÂ `1`s for eachÂ `product_id`.
    
- Example: IfÂ `product_id=123`Â appearsÂ **5 times**, emitsÂ `(123, 5)`.
    

**ğŸ”— Reference**:  
ğŸ“„Â _Paper Example_: Summing word counts (Section 2.1).

---

### **ğŸ“Œ iii. Analysis of Go Code for Intermediate Output**

**ğŸš¨ Issue Identified**:  
The provided Go code hasÂ **concurrency & fault-tolerance problems**:

1. **Race Condition**Â ğŸï¸: Multiple workers may write to the same file simultaneously â†’Â **data corruption**.
    
2. **No Atomicity**Â âš¡: If a worker crashes mid-write, the file is leftÂ **incomplete**.
    

**âŒ Why Itâ€™s Bad**:

- The paper (Section 3.1) says workers shouldÂ **first write locally**, then share outputs.
    
- Direct shared writesÂ **bypass master coordination**Â (Section 3.2).
    

**ğŸ”— Reference**:  
ğŸ“„Â _Paper_: Fault tolerance relies on atomic commits (Section 3.4).

---

### **ğŸ“Œ iv. Suggested Fix for Go Code**

**âœ¨ Solution**:

1. **Write locally first**Â â†’ Avoid concurrency issues.
    
2. **Use atomic rename**Â â†’ Ensure consistency.
    

**ğŸ› ï¸ Improved Go Code**:
```
tempFile := fmt.Sprintf("/local/intermediate-%d.tmp", taskID)
file, err := os.Create(tempFile)
if err != nil {
    log.Fatal("Failed to create temp file:", err)
}
// Write data...
file.Close()
// Atomically rename to final location
finalFile := fmt.Sprintf("/shared/intermediate-%d.csv", taskID)
os.Rename(tempFile, finalFile)
```
**ğŸ“– Explanation**:  
âœ”Â **Local writes**Â â†’ No race conditions.  
âœ”Â **Atomic rename**Â â†’ File either fully exists or not at all.

**ğŸ”— Reference**:  
ğŸ“„Â _Paper_: Atomic renames for fault tolerance (Section 3.5).

---

### **ğŸ“Œ v. Master Algorithm for Load Balancing**

**âš–ï¸ Problem**:

- Files vary in size (**1GB**,Â **500-750MB**,Â **<500MB**).
    
- **10 workers**Â must process them efficiently.
    

**ğŸš€ Optimal Strategy**:

1. **Dynamic Scheduling**Â ğŸ”„:
    
    - AssignÂ **largest files first**Â (1GB).
        
    - UseÂ **backup tasks**Â (Section 3.8) for stragglers.
        
2. **Partitioning**Â âœ‚ï¸:
    
    - Split big files intoÂ **64MB chunks**Â (like in the paper).
        
    - Assign chunks to idle workers.
        

**ğŸ”— Reference**:  
ğŸ“„Â _Paper_:

- **Task granularity**Â (Section 3.6).
    
- **Backup tasks**Â reduce stragglers (Section 3.8).
    

---

### **ğŸ¯ Key Takeaways from the Paper**

1. **MapReduce Flow**Â â†’ Map â†’ Shuffle â†’ Reduce.
    
2. **Fault Tolerance**Â â†’ Re-execution + atomic commits.
    
3. **Locality Optimization**Â â†’ Prefer workers with local data.
    
4. **Load Balancing**Â â†’ Fine-grained tasks + backups.
    

These principles directly guide the answers above! ğŸš€

---

### **ğŸ’¡ Final Notes**

- **Concurrency**Â must be handled carefully (avoid shared writes).
    
- **Atomicity**Â is critical for fault tolerance.
    
- **Dynamic scheduling**Â ensures efficient processing.



## [[102. GFS]] and [[104. Raft]]

### **Question: Google File System, Consistency, Time Concepts, and Raft**

#### **i. Comment on the consistency promise of GFS in its original form.**Â [03 marks]

#### **ii. GFS with triple replica scenario:**

The system starts with the valueÂ **"5"**Â (one byte). Two clients perform operations concurrently:

**Client A:**

1. WritesÂ **"4"**Â at offset 0 (replaces "5").
    
2. Upon successful completion, writesÂ **"6"**Â at offset 0 (replaces "4").
    

**Client B:**

1. PerformsÂ **3 read operations**, displaying the file content each time.
    

**a.**Â Can Client B seeÂ **"5"**Â after seeingÂ **"4"**? Explain why this is possible or restricted. [04 marks]

**b.**Â Can Client B seeÂ **"5"**Â after seeingÂ **"6"**? Explain why this is possible or restricted. [04 marks]

#### **iii.**Â What modification to GFS would ensureÂ **read-after-write consistency**Â for each client in this case? [05 marks]

#### **iv. Introducing Raft to reduce write latency:**

**a.**Â How can Raft achieve this? [03 marks]  
**b.**Â Discuss the consequences of this design change. [03 marks]  
**c.**Â What additional steps can mitigate these consequences? [03 marks]

### ***Answer***

#### **i. Comment on the consistency promise of GFS in its original form.**Â [03 marks]

ğŸ”¹Â **GFS Relaxed Consistency Model**  
The Google File System (GFS) provides aÂ **relaxed consistency model**Â optimized for large-scale, append-heavy workloads. Unlike traditional file systems that guarantee strong consistency (all clients see the same data at the same time), GFS prioritizesÂ **high throughput and fault tolerance**Â over strict consistency.

ğŸ“ŒÂ **Key Points from the GFS Paper (Section 2.7):**

1. **Defined vs. Undefined Regions**
    
    - A file region isÂ **defined**Â if all clients see the same data written by a successful mutation.
        
    - A region isÂ **undefined but consistent**Â if concurrent mutations interleave, resulting in a mix of fragments.
        
    - Failed mutations leave regionsÂ **inconsistent**Â (different clients see different data).
        
2. **Append vs. Overwrite**
    
    - **Record appends**Â are atomic (at least once) but may include duplicates or padding.
        
    - **Writes**Â at explicit offsets are not atomic under concurrency.
        
3. **Replica Synchronization**
    
    - The primary replica serializes mutations, ensuring all replicas apply changes in the same order.
        
    - Stale replicas (missed updates) are garbage-collected.
        

ğŸ”¹Â **Why Relaxed Consistency?**

- GFS is designed forÂ **batch processing**Â (e.g., MapReduce) where sequential reads dominate.
    
- Clients use techniques likeÂ **checksums**Â andÂ **self-identifying records**Â to handle inconsistencies.
    

ğŸ‘‰Â **Conclusion:**Â GFS trades strict consistency for scalability and performance, relying on application-level checks to handle edge cases.

---

#### **ii. GFS Triple Replica Scenario**

**Scenario:**

- Initial value: "5" (1 byte).
    
- **Client A**:
    
    1. Writes "4" at offset 0 (replaces "5").
        
    2. Writes "6" at offset 0 (replaces "4").
        
- **Client B**: Reads the file 3 times during concurrent writes.
    

**a. Can Client B see "5" after seeing "4"?**Â [04 marks]  
âœ…Â **Yes, itâ€™s possible!**Â Hereâ€™s why:

1. **Replica Divergence**
    
    - GFS replicas may not sync instantly.
        
    - Client B might read from a replica that hasnâ€™t yet applied the write("4") due to network delays.
        
2. **Stale Reads**
    
    - If Client Bâ€™s first read fetches "4" from an up-to-date replica but the second read hits a stale replica still holding "5", it observesÂ **"4" â†’ "5"**.
        
3. **No Read-After-Write Guarantee**
    
    - GFS doesÂ **not**Â guarantee linearizability. Clients caching chunk locations may see outdated data until the master updates metadata.
        

ğŸ“ŒÂ **Reference (GFS Section 2.7.1):**

> "Clients cache chunk locations and may read from stale replicas before refreshing metadata."

---

**b. Can Client B see "5" after seeing "6"?**Â [04 marks]  
âŒÂ **No, this is restricted!**Â Hereâ€™s why:

1. **Order of Writes**
    
    - Client Aâ€™s operations are serialized: write("4") â†’ write("6").
        
    - Once "6" is written, all replicasÂ **must**Â have applied "4" first (per primaryâ€™s ordering).
        
2. **No Time Travel**
    
    - A replica showing "5" after "6" would imply data corruption or a severe bug (e.g., missed mutations).
        
    - GFS detects stale replicas viaÂ **version numbers**Â and excludes them from serving reads.
        

ğŸ“ŒÂ **Reference (GFS Section 4.5):**

> "Stale replicas are garbage-collected and never served to clients."

---

#### **iii. Achieving Read-After-Write Consistency**Â [05 marks]

To fix this, modify GFS to:

1. **Lease Extensions**
    
    - The master grantsÂ **longer leases**Â to primaries, reducing stale reads during write propagation.
        
2. **Client Cache Invalidation**
    
    - Force clients toÂ **invalidate cached chunk locations**Â after writes (e.g., shorter TTLs).
        
3. **Synchronous Replication**
    
    - Make the primary wait forÂ **all replicas**Â to acknowledge writes before replying to clients (at the cost of latency).
        
4. **Version Checks**
    
    - Clients include aÂ **version token**Â in reads, ensuring they only get data â‰¥ their last write.
        

ğŸ“ŒÂ **Reference (GFS Section 3.1):**

> "The primary sequences mutations and ensures all replicas apply them in order."

---

#### **iv. Raft for Reducing Write Latency**

**a. How Raft Helps**Â [03 marks]  
ğŸš€Â **Raftâ€™s Optimizations:**

1. **Strong Leader**
    
    - All writes go through the leader, avoiding Paxosâ€™s multi-round voting.
        
2. **Batched Appends**
    
    - The leader pipelines log entries to followers inÂ **single RPCs**Â (vs. Paxosâ€™s per-entry prepares).
        
3. **No Stale Reads**
    
    - Followers reject outdated writes, ensuring fast convergence.
        

ğŸ“ŒÂ **Reference (Raft Section 5.3):**

> "Leaders optimize log replication by batching entries and retrying inconsistently."

---

**b. Consequences**Â [03 marks]  
âš ï¸Â **Trade-offs:**

1. **Leader Bottleneck**
    
    - The leader handles all writes; if itâ€™s slow, the cluster slows.
        
2. **Network Sensitivity**
    
    - Heartbeat delays can trigger unnecessary elections.
        
3. **Scalability Limits**
    
    - Larger clusters increase leader load.
        

---

**c. Mitigations**Â [03 marks]  
ğŸ› Â **Solutions:**

1. **Leader Stickiness**
    
    - UseÂ **pre-voting**Â (Raft extension) to avoid flappy elections.
        
2. **Parallel Writes**
    
    - Shard data across multiple Raft groups (like Spanner).
        
3. **Hardware Tuning**
    
    - Deploy leaders on low-latency nodes.
        

ğŸ“ŒÂ **Reference (Raft Section 9.3):**

> "Randomized election timeouts reduce split votes and improve stability."

---

#### **Final Answer Summary**

- GFS favorsÂ **throughput over strong consistency**Â but can be enhanced with leases/caching.
    
- RaftÂ **reduces write latency**Â via leadership but requires careful tuning.
    
- Both systems exemplifyÂ **trade-offs in distributed design**! ğŸŒ


## [[103. VMWare-FT]]

Assume that in order to improve availability, the VMWare FT is extended toÂ **two backup state machine replicas**Â and implemented with anÂ **atomic clock-synchronized setup**, where the primary and both replicas are exactly in sync.

**i.**Â How would this new design impact the performance of the system in general while the primary is non-faulty?Â **[04 marks]**

**ii.**Â Assume after committing some entries, one secondary replica has crashed. How should the new design address this problem without compromising the original consistency guarantee of the VMWare design?Â **[05 marks]**

**iii.**Â When the primary is faulty, suggest the mechanism that will ensure only one of the secondaries is guaranteed to be promoted as the primary.Â **[03 marks]**

**iv.**Â To reduce the overhead of communication, it was suggested to simplify the append for time inquiries (such as current time inquiries) by treating them deterministically, as the clocks are in exact sync. Comment on this approach. What might be the result?Â **[06 marks]**

**v.**Â Consider the Clock Synchronization effort the implementers had made in this design.

- **a.**Â Justify or criticize this design choice.Â **[03 marks]**
    
- **b.**Â Assuming that precise time is a must in both secondary and primary clocks, discuss a cost-effective and feasible clock synchronization mechanism as an extension to the VMWare design.Â **[04 marks]**

### ***Answer***
#### **i. Impact on Performance with Non-Faulty Primary**

**Answer:**  
Extending VMware FT toÂ **two backup replicas**Â (instead of one) wouldÂ **increase performance overhead**Â while the primary is non-faulty due to:

1. **Additional Logging Bandwidth**: The primary must send log entries to two backups instead of one, doubling the network traffic (Section 5.1, Table 1).
    
2. **Synchronization Delay**: The Output Rule (Section 2.2) requires acknowledgments fromÂ **both backups**Â before the primary can send external outputs, increasing latency.
    
3. **CPU Overhead**: The primaryâ€™s hypervisor must manage logging channels to two replicas, adding computational load (Section 5.1).
    

**Reference**:

- The paper notes that even with one backup, logging bandwidth is typically <20 Mbit/s (Table 1), but adding a second backup would scale this linearly.
    
- The Output Rule (Section 2.2) delays outputs until backups acknowledge logs; waiting for two replicas would exacerbate this delay.
    

---

#### **ii. Handling a Secondary Replica Crash Without Compromising Consistency**

**Answer:**  
To maintain consistency if one secondary crashes:

1. **Continue Logging to the Remaining Backup**: The primary switches to sending logs only to the surviving backup (Section 2.3).
    
2. **Atomic Commit Protocol**: Ensure the surviving backup has acknowledged all logs up to the crash point (Section 2.2, Output Rule).
    
3. **Restart Redundancy**: UseÂ **FT VMotion**Â (Section 3.1) to spawn a new backup on another host, syncing its state with the primary.
    

**Reference**:

- The paper describes automatic redundancy restoration via FT VMotion (Section 3.1).
    
- The Output Rule guarantees consistency by ensuring backups replay all operations before outputs are sent (Section 2.2).
    

---

#### **iii. Promoting One Secondary When Primary Fails**

**Answer:**  
To ensureÂ **only one secondary**Â becomes primary:

1. **Shared Storage Arbitration**: Use an atomicÂ **test-and-set**Â operation on shared storage (Section 2.3). The first replica to succeed becomes primary; the other halts.
    
2. **Heartbeat Timeout**: If connectivity is lost, the backup with the lowest network latency/heartbeat response wins (Section 2.3).
    

**Reference**:

- The paperâ€™s split-brain resolution relies on shared storage (Section 2.3): "Only one VM can win the test-and-set and go live."
    

---

#### **iv. Treating Time Inquiries as Deterministic**

**Answer:**  
**Pros**:

- **Reduced Logging**: Eliminates logging for clock reads, cutting bandwidth (Section 4.2).
    
- **Lower Latency**: No need to wait for backup acknowledgments for time queries.
    

**Cons**:

- **Clock Drift Risk**: Even atomic clocks can desynchronize, causing divergence (Section 2.1).
    
- **Non-Determinism**: If clocks drift, backups may replay incorrectly (e.g., timer interrupts at wrong instructions).
    

**Reference**:

- The paper highlights non-determinism from clock reads (Section 2.1).
    
- Deterministic replay requires logging all non-deterministic events (Section 2.1).
    

---

#### **v. Clock Synchronization Design**

**a. Justification/Criticism**  
**Answer**:

- **Justification**: Atomic sync ensures replicas replay interrupts at the same instruction, critical for determinism (Section 2.1).
    
- **Criticism**: Overhead of sync may outweigh benefits for non-time-sensitive workloads.
    

**b. Cost-Effective Sync Mechanism**  
**Answer**:  
UseÂ **NTP with Hardware Assist**:

1. **NTP for Coarse Sync**: Low-cost, software-based synchronization.
    
2. **RDMA or PTP for Precision**: For sub-microsecond sync, use hardware-assisted protocols (e.g., Precision Time Protocol).
    

**Reference**:

- The paper assumes exact sync (Section 2.1) but doesnâ€™t prescribe a method.
    
- Hardware counters (Section 2.1) could be extended for sync.
    

---

### **Key Takeaways from the Paper**

1. **FT Overhead**: Primarily from logging non-deterministic events (Section 2.1).
    
2. **Consistency**: Output Rule and shared storage prevent split-brain (Section 2.2â€“2.3).
    
3. **Redundancy**: FT VMotion automates backup recovery (Section 3.1).


## [[104. Raft]] and [[107. Bitcoin]]

### **Question 4:**

**i.**Â Comment on the possibility of applying Rafi implementation as the consensus mechanism for Bitcoin.Â **[03 marks]**

**ii.**Â Consider a real-life problem where a Bitcoin spender commits a transaction with a mistaken vendor ID by inserting it manually.  
**a.**Â Assume that the vendor ID included does not exist. How would this transaction be handled by Bitcoin?Â **[04 marks]**  
**b.**Â In contrast to the above, if the mistakenly entered vendor ID exists, what processes should be in the Bitcoin architecture to avoid this transaction being committed?Â **[05 marks]**

**iii.**Â Blockchain makes use of proof-of-work to validate the mining process.  
**a.**Â Comment on its advantages and disadvantages.Â **[03 marks]**  
**b.**Â Explain how it is improved with its successors like proof-of-stake and proof-of-authority.Â **[04 marks]**

**iv.**Â In the Bitcoin paper by Satoshi Nakamoto, it is suggested to randomly vary the number of leading zeros required in the hashed block to secure the blockchain design, and it is defined by the hardness level. Explain how this will adjust to an extremely dynamic mining effort. For instance, how can blockchain design keep the 10-minute commitment guarantee if a drastic reduction in the number of miners takes place? Or vice versa?


### **Answer:**

ApplyingÂ **Raft**Â as a consensus mechanism forÂ **Bitcoin**Â isÂ **not feasible**Â due to fundamental differences in their design goals and operational models. Below are key reasons:

1. **Leader-Based vs. Leaderless Consensus**
    
    - **Raft**Â is aÂ **leader-based**Â consensus algorithm where a single leader coordinates log replication and decision-making. This introduces a centralization risk, which contradicts Bitcoinâ€™sÂ **decentralized**Â peer-to-peer nature (_Bitcoin paper, Section 1_).
        
    - **Bitcoin**Â usesÂ **Proof-of-Work (PoW)**, aÂ **leaderless**Â system where any node can propose blocks, ensuring no single point of control (_Bitcoin paper, Section 4_).
        
2. **Finality vs. Probabilistic Consensus**
    
    - **Raft**Â providesÂ **immediate finality**Â once a log entry is committed by a majority (_Raft paper, Section 5.3_).
        
    - **Bitcoin**Â usesÂ **probabilistic finality**, where transactions are considered confirmed only after multiple blocks are mined on top of them (_Bitcoin paper, Section 11_).
        
3. **Performance and Scalability**
    
    - **Raft**Â is optimized for low-latency, high-throughput systems (e.g., distributed databases) but requiresÂ **fixed membership**Â andÂ **strong synchronization**Â (_Raft paper, Section 5.2_).
        
    - **Bitcoin**Â prioritizesÂ **permissionless participation**Â andÂ **Byzantine fault tolerance**, allowing nodes to join/leave dynamically (_Bitcoin paper, Section 5_).
        
4. **Security Model**
    
    - **Raft**Â assumesÂ **non-Byzantine failures**Â (nodes may crash but not act maliciously) (_Raft paper, Section 2_).
        
    - **Bitcoin**Â defends againstÂ **Sybil attacks**Â andÂ **double-spending**Â via PoW and economic incentives (_Bitcoin paper, Section 6_).
        

**Conclusion:**  
Raftâ€™s design isÂ **unsuitable**Â for Bitcoin due to itsÂ **centralization risks, lack of Byzantine fault tolerance, and incompatibility with PoW**. Bitcoinâ€™sÂ **decentralized, adversarial model**Â requires a different approach (_Bitcoin paper, Section 12_).

---

### **(a) Assume that the vendor ID does not exist. How would this transaction be handled by Bitcoin? [04 marks]**

### **Answer:**

1. **Transaction Propagation**
    
    - The spender broadcasts the transaction to the Bitcoin network (_Bitcoin paper, Section 5_).
        
    - Nodes validate the transaction structure (signatures, inputs, outputs) butÂ **cannot verify the recipientâ€™s existence**Â since Bitcoin addresses are pseudonymous (_Bitcoin paper, Section 10_).
        
2. **Inclusion in Mempool**
    
    - Valid transactions are stored in theÂ **mempool**Â (pending transactions pool) of nodes (_Bitcoin paper, Section 5_).
        
3. **Mining and Block Inclusion**
    
    - Miners select transactions from the mempool to include in a block (_Bitcoin paper, Section 6_).
        
    - Since theÂ **vendor ID (address) does not exist**, the transaction is still processed because Bitcoin does not validate recipient activity.
        
4. **Final Outcome**
    
    - The transaction isÂ **irreversible once confirmed**Â (added to the blockchain).
        
    - The funds areÂ **lost forever**Â because no one controls the invalid address (_Bitcoin paper, Section 7_).
        

### **(b) If the mistakenly entered vendor ID exists, what processes should be in the Bitcoin architecture to avoid this transaction being committed? [05 marks]**

### **Answer:**

Bitcoinâ€™s architectureÂ **cannot prevent**Â accidental payments to valid addresses, but the following mechanisms help mitigate risks:

1. **Address Checksum Validation**
    
    - Bitcoin addresses include a checksum (Base58 encoding) to detect typos (_Bitcoin paper, Section 2_).
        
    - Wallets reject malformed addresses before broadcasting.
        
2. **Human-Readable Addresses (Bech32, ENS)**
    
    - Modern wallets supportÂ **Bech32 (SegWit)**Â andÂ **ENS (Ethereum Name Service)**Â for error-resistant addresses.
        
3. **Double-Confirmation Prompts**
    
    - Wallets canÂ **warn users**Â if a recipient address is new/unusual.
        
4. **Multi-Signature Escrow**
    
    - Businesses can useÂ **multi-sig wallets**Â requiring multiple approvals before funds are released (_Bitcoin paper, Section 1_).
        
5. **Transaction Replace-by-Fee (RBF)**
    
    - If the transaction is unconfirmed, the sender canÂ **replace it**Â with a corrected version (_Bitcoin paper, Section 5_).
        

**Conclusion:**  
Bitcoinâ€™sÂ **trustless model**Â means transactionsÂ **cannot be reversed**, but wallet-level safeguards reduce errors (_Bitcoin paper, Section 12_).

---
### **(a) Comment on its advantages and disadvantages. [03 marks]**

### **Answer:**

|**Advantages**|**Disadvantages**|
|---|---|
|**1. Security:**Â PoW ensures resistance to Sybil and 51% attacks (_Bitcoin paper, Section 4_).|**1. Energy Intensive:**Â High computational power required (_Bitcoin paper, Section 6_).|
|**2. Decentralization:**Â No single entity controls mining (_Bitcoin paper, Section 5_).|**2. Slow Transactions:**Â ~10 min block time (_Bitcoin paper, Section 5_).|
|**3. Fair Distribution:**Â Miners compete openly for rewards (_Bitcoin paper, Section 6_).|**3. Centralization Risk:**Â Mining pools dominate (_Bitcoin paper, Section 11_).|

### **(b) Explain how it is improved with its successors like proof-of-stake and proof-of-authority. [04 marks]**

### **Answer:**

1. **Proof-of-Stake (PoS)**
    
    - **Energy Efficiency:**Â Validators are chosen based onÂ **staked coins**, not computational work (_Ethereumâ€™s Casper FFG_).
        
    - **Faster Finality:**Â Shorter block times (~12 sec in Ethereum 2.0).
        
    - **Economic Security:**Â Attackers lose staked funds if malicious (_Bitcoin paper, Section 6_Â analogy).
        
2. **Proof-of-Authority (PoA)**
    
    - **Permissioned Model:**Â Only approved validators (e.g., enterprises) can produce blocks (_Raft-like leader selection_).
        
    - **High Throughput:**Â Suitable for private blockchains (e.g., Hyperledger).
        

**Conclusion:**  
PoS and PoA address PoWâ€™sÂ **energy waste**Â andÂ **scalability issues**Â but trade offÂ **decentralization**Â (_Bitcoin paper, Section 12_).

---
### **Answer:**

Bitcoinâ€™sÂ **difficulty adjustment algorithm**Â ensuresÂ **consistent block times**Â despite fluctuating hash power (_Bitcoin paper, Section 4_):

1. **Target Block Time:**Â 10 minutes (set in protocol).
    
2. **Difficulty Recalculation:**Â EveryÂ **2016 blocks**Â (~2 weeks), the network adjusts difficulty based on:
    
    NewÂ Difficulty=OldÂ DifficultyÃ—ActualÂ TimeExpectedÂ TimeÂ (2016Â blocksÂ Ã—Â 10Â min)NewÂ Difficulty=OldÂ DifficultyÃ—ExpectedÂ TimeÂ (2016Â blocksÂ Ã—Â 10Â min)ActualÂ Timeâ€‹
3. **Hash Rate Impact:**
    
    - IfÂ **more miners join**, blocks are found faster â†’ difficultyÂ **increases**.
        
    - IfÂ **miners leave**, blocks slow down â†’ difficultyÂ **decreases**.
        
4. **Self-Correcting Mechanism:**
    
    - EnsuresÂ **long-term stability**Â of the blockchain (_Bitcoin paper, Section 4_).
        

**Example:**

- If blocks are mined inÂ **8 min**Â on average, difficulty increases byÂ **25%**Â to restore 10 min.
    

**Conclusion:**  
This mechanism maintainsÂ **network security**Â andÂ **predictable coin issuance**Â (_Bitcoin paper, Section 6_).

---

### **Final Notes:**

- **Raft**Â is forÂ **permissioned, crash-fault-tolerant**Â systems (_Raft paper, Section 2_).
    
- **Bitcoin**Â is forÂ **permissionless, Byzantine-fault-tolerant**Â systems (_Bitcoin paper, Section 1_).
    
- **References:**Â Direct citations from both papers support each answer.