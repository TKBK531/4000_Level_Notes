### **ðŸ“Œ Abstract**

**Summary**:  
Raft is aÂ **consensus algorithm**Â designed to manage a replicated log. It achieves the same goals as Paxos but isÂ **more understandable**Â due to its decomposed structure (leader election, log replication, safety) and stronger coherence guarantees. It also introduces a novelÂ **membership change mechanism**Â using overlapping majorities for safety during cluster reconfiguration.

**Key Points**:
- ðŸŽ¯Â **Understandability**: Raft separates consensus into clear subproblems.
- ðŸ”„Â **Equivalence to Paxos**: Same safety/performance but simpler.
- ðŸ—ï¸Â **Practical foundation**: Used in real systems (e.g., RAMCloud, ZooKeeper).   

---

### **ðŸ“Œ 1. Introduction**
**Problem**:
- Paxos isÂ **hard to understand**Â and implement, despite being the standard for consensus.  
- Real-world Paxos implementations deviate significantly from theory, leading to complexity and errors.   

**Solution**:  
Raft prioritizesÂ **understandability**Â by:
- ðŸ§©Â **Decomposition**: Leader election, log replication, safety.
- ðŸ§ Â **State reduction**: Fewer edge cases vs. Paxos.
- ðŸ“ŠÂ **User study**: 33/43 students understood Raft better than Paxos.

**Novel Features**:
- ðŸ‘‘Â **Strong leader**: Log entries flow only from leader â†’ followers.
- â±ï¸Â **Randomized leader election**: Resolves conflicts quickly.
- ðŸ”„Â **Membership changes**: Safe transitions via joint consensus.

---

### **ðŸ“Œ 2. Replicated State Machines**
**Core Concept**:
- AÂ **replicated log**Â ensures all servers execute the same commands in the same order (deterministic state machines).
- Consensus algorithms manage this log.

**Properties**:
- ðŸ”’Â **Safety**: No incorrect results under network delays/partitions.
- âš¡Â **Availability**: Works if majority of servers are operational.
- â³Â **Timing independence**: Consistency doesnâ€™t rely on clocks.

**Example Systems**:
- Googleâ€™s Chubby, ZooKeeper, HDFS.


---

### **ðŸ“Œ 3. Whatâ€™s Wrong with Paxos?**
**Issues**:
1. **Complexity**:
    - Single-decree Paxos is hard to intuit; multi-Paxos adds more complexity.
    - "Even seasoned researchers struggle with Paxos."
2. **Practical gaps**:
    - No standard multi-Paxos implementation.
    - Real systems (e.g., Chubby) deviate from theory.

**Result**:
- Raft avoids Paxosâ€™s "dense" two-phase design by using aÂ **leader-centric approach**.

---

### **ðŸ“Œ 4. Designing for Understandability**
**Techniques**:
1. **Decomposition**:
    - Split into leader election, log replication, safety.
2. **State space reduction**:
    - No log holes; limited inconsistency cases.
3. **Randomization**:
    - Simplifies leader election (randomized timeouts).

**Goal**:
- Make it "obvious why it works" for implementers.

---

### **ðŸ“Œ 5. Raft Consensus Algorithm**

#### **5.1 Basics**
- **Server states**: Leader, follower, candidate.
- **Terms**: Logical time periods with elections (Figure 5).
- **RPCs**:Â `RequestVote`Â (elections),Â `AppendEntries`Â (log replication + heartbeats).

#### **5.2 Leader Election**

- **Process**:
    1. Followers â†’ candidates if leader heartbeat times out.
    2. Candidates request votes; majority wins â†’ leader.
    3. Randomized timeouts prevent split votes.

- **Safety**: Only one leader per term (Election Safety Property).

#### **5.3 Log Replication**
- **Leader appends entries**Â â†’ replicates to followers.
- **Committed entries**: Applied to state machines once replicated to majority.
- **Log Matching Property**:
    - Same index + term â†’ same command and preceding entries.

#### **5.4 Safety**
- **Leader completeness**: Leaders must have all committed entries (ensured by voting restrictions).
- **Election restriction**: Voters reject candidates with stale logs.
- **Commit rules**: Only current-term entries commit directly; older entries commit indirectly.
#### **5.5 Timing**
- **Requirement**:Â `broadcastTime â‰ª electionTimeout â‰ª MTBF`.
- Ensures stable leadership and progress.


---

### **ðŸ“Œ 6. Cluster Membership Changes**
**Challenge**: Switching configurations atomically without split-brain.  
**Solution**:Â **Joint consensus**Â (transitional phase):
1. Replicate to both old (`C_old`) and new (`C_new`) configurations.
2. CommitÂ `C_new`Â only after joint consensus is safe.  
    **Key**: Overlapping majorities prevent dual leadership.

---

### **ðŸ“Œ 7. Log Compaction**
**Problem**: Unbounded log growth â†’ storage/performance issues.  
**Solution**:Â **Snapshots**:
- Servers independently save state + metadata (last included index/term).
- Leaders send snapshots to lagging followers viaÂ `InstallSnapshot`Â RPC.

---

### **ðŸ“Œ 8. Client Interaction**
- Clients contact leaders (redirected if needed).
- **Linearizability**:
    - Unique client IDs + sequence numbers prevent duplicate execution.
    - Read-only queries use leader heartbeats to avoid stale reads.

---

### **ðŸ“Œ 9. Evaluation**
1. **Understandability**:
    - Raft scoredÂ **4.9 points higher**Â than Paxos in user studies.
2. **Correctness**:
    - Formal TLA+ proofs for safety properties.
3. **Performance**:
    - Elects leaders inÂ **~50ms**Â with randomized timeouts.

---

### **ðŸ“Œ 10. Related Work**
- **Paxos**: Complex, leader election orthogonal to consensus.
- **VR/ZooKeeper**: Leader-based but more complex message flows.
- **EPaxos**: Leaderless optimizations for commutativity (higher complexity).

---

### **ðŸ“Œ 11. Conclusion**
Raftâ€™sÂ **understandability**Â comes from
- Strong leadership.
- Clear decomposition.
- Reduced state space.  
    Itâ€™sÂ **safer for implementations**Â andÂ **easier to teach**Â than Paxos


# *How does Raft handle leader elections*

## *1. Election Trigger Mechanism*
- **Heartbeat Monitoring**: Followers monitor leader activity via periodic heartbeats (empty AppendEntries RPCs)
- **Randomized Election Timeout**: Each follower has a unique timeout (150â€“300 ms). If no heartbeat arrives before this timeout expires, the follower transitions toÂ **candidate**Â and starts an election

## *2. Election Process*
### *Step 1: Candidate Initiation*
- A candidate increments its term number, votes for itself, and broadcastsÂ **RequestVote RPCs**Â to all servers. These RPCs include:
    - Current term
    - Last log entry index and term (to prove log completeness)
### *Step 2: Voting Rules*
Servers grant votes if:
- They havenâ€™t voted in this term.
- The candidateâ€™s log is at least as up-to-date as theirs (prevents stale leaders)
### *Step 3: Election Outcomes*
- **Majority Win**: If a candidate secures votes from >50% of servers, it becomes leader and sends heartbeats to assert authority
- **Higher-Term Leader**: If another server claims leadership with a â‰¥ term, the candidate reverts to follower
- **Split Vote**: If no majority is achieved, the election times out, and a new election begins with aÂ **new randomized timeout**Â to reduce repeated splits

## *3. Safety Guarantees*
- **Leader Completeness**: Only servers with all committed log entries can win elections (enforced via log comparison in RequestVote)
- **Single Leader Per Term**: Each server votes once per term, ensuring at most one leader
- **Term Synchronization**: All servers update their term to the highest value observed, preventing stale leaders from disrupting the cluster

## *4. Example Workflow*
1. **Cluster Startup**: All nodes start as followers. Node E (with the shortest timeout) triggers an election firstÂ 
2. **Vote Collection**: Node E sends RequestVote RPCs. Nodes grant votes if Eâ€™s log is up-to-date.
3. **Leadership Assertion**: E becomes leader upon majority confirmation and begins sending heartbeats

## *5. Key Optimizations*
- **Randomized Timeouts**: Minimize split votes by staggering election starts
- **Log Consistency Checks**: Ensure new leaders have the most complete logs, preserving system integrity

Raftâ€™s leader election mechanism prioritizes safety and liveness, enabling distributed systems to recover quickly from failures while maintaining consistency.


# *Raft Leader Election : Example

### *Setup*
Imagine a Raft cluster with 5 servers: S1, S2, S3, S4, and S5. Initially, S1 is the leader, and the others are followers. The leader regularly sends heartbeat messages (AppendEntries RPCs) to followers to maintain authority.

### *Step 1: Leader Failure Detection*
- Suppose S1 crashes or becomes unreachable.
- Followers expect heartbeats from S1 at regular intervals.
- Each follower has a randomized election timeout (e.g., between 150ms and 300ms).
- If a follower does not receive a heartbeat before its timeout expires, it assumes the leader has failed.

For example, S3â€™s election timeout expires first because it had the shortest timeout.

### *Step 2: Becoming a Candidate and Starting an Election*
- S3 increments its current term number (say from term 2 to term 3).
- S3 votes for itself.
- S3 sendsÂ **RequestVote RPCs**Â to S1, S2, S4, and S5. This RPC includes:
    - S3â€™s current term (3).
    - The index and term of S3â€™s last log entry (to prove it has the most up-to-date log).

### *Step 3: Voting by Other Servers*

- Each follower receiving the RequestVote RPC checks:
    - If the candidateâ€™s term is at least as large as their current term.
    - If they have not already voted in this term.
    - If the candidateâ€™s log is at least as up-to-date as their own log (comparing last log term and index).
- If all conditions hold, they grant their vote to S3.
- For example:
    - S2 and S4 grant votes to S3.
    - S5 might deny vote if it has a more up-to-date log or has already voted for another candidate.

### *Step 4: Election Outcome*
- If S3 receives votes from a majority (3 out of 5), it becomes the new leader.
- S3 immediately sends heartbeat messages (empty AppendEntries RPCs) to all followers to assert leadership and prevent new elections.
- Followers reset their election timers upon receiving these heartbeats.

### *Step 5: Handling Complications*

- **Case: Another Candidate Emerges Simultaneously**  
    Suppose S2â€™s election timeout expires almost at the same time as S3â€™s, and S2 also becomes a candidate and requests votes.
    - Votes may split, e.g., S3 gets votes from S4 and S5, S2 gets votes from S1 and itself.
    - No candidate gets a majority â†’ split vote.
    - Both candidatesâ€™ election timers expire again after randomized timeouts.
    - A new election starts in a new term, and the process repeats until a candidate wins a majority.
        
- **Case: Candidate Receives AppendEntries from Another Leader**  
    If S3 is a candidate but receives an AppendEntries RPC from a server claiming to be leader with a higher or equal term, S3 steps down to follower and recognizes that leader.
    
- **Case: Candidate Receives RequestVote with Higher Term**  
    If S3 receives a RequestVote RPC with a higher term, it updates its term and reverts to follower.

### *Step 6: Term and Vote Updates*

- All servers update their current term to the highest term they observe in any RPC.
- Servers vote only once per term, preventing multiple leaders in the same term.
- This guarantees at most one leader per term.

| ***Step***                    | ***Description***                                                                                                     |
| ----------------------- | --------------------------------------------------------------------------------------------------------------- |
| 1. Leader failure       | Followers detect missing heartbeats and start election timers.                                                  |
| 2. Candidate initiation | A follower whose timeout expires becomes candidate, increments term, votes for self, sends RequestVote RPCs.    |
| 3. Vote granting        | Followers grant votes if candidateâ€™s term and log are up-to-date and they havenâ€™t voted yet.                    |
| 4. Election result      | Candidate with majority votes becomes leader and sends heartbeats.                                              |
| 5. Handling conflicts   | Split votes cause new elections with randomized timeouts; candidates step down if they see higher term leaders. |
| 6. Term synchronization | All servers update to highest term seen, ensuring election safety.                                              |

### *Why This Works*

- **Randomized Election Timeouts**Â reduce split votes by staggering election starts.
- **Log Up-to-date Checks**Â ensure only candidates with the most complete logs can become leaders, preserving consistency.
- **Majority Voting**Â ensures a single leader is chosen, preventing conflicting leaders.
- **Term Numbers**Â synchronize cluster state and prevent stale leaders.

| Limitation                   | Description                                                            | Modern Mitigations                                              |
| ---------------------------- | ---------------------------------------------------------------------- | --------------------------------------------------------------- |
| Scalability                  | High communication overhead in large clusters                          | Sharding, Multi-Raft, Witness nodes                             |
| High Resource/Energy Use     | Majority quorum requires many full nodes                               | Quorum tuning, Witness nodes                                    |
| Leader Bottleneck & Downtime | Single leader can be overloaded; brief unavailability during elections | Read-only queries on followers, fast failover, hybrid protocols |
| Log Divergence               | Temporary inconsistencies after partitions                             | Snapshotting, adaptive reconciliation                           |
| Limited Real-World Testing   | Insufficient evaluation under diverse conditions                       | Systematic evaluation frameworks, adaptive tuning               |
| Membership Change Overhead   | Noisy and complex with frequent node changes                           | Joint consensus optimization, dynamic management                |
# -
[[Distributed Guide]]
[[999. Questions]]