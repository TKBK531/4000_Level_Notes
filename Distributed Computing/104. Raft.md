### **📌 Abstract**

**Summary**:  
Raft is a **consensus algorithm** designed to manage a replicated log. It achieves the same goals as Paxos but is **more understandable** due to its decomposed structure (leader election, log replication, safety) and stronger coherence guarantees. It also introduces a novel **membership change mechanism** using overlapping majorities for safety during cluster reconfiguration.

**Key Points**:
- 🎯 **Understandability**: Raft separates consensus into clear subproblems.
- 🔄 **Equivalence to Paxos**: Same safety/performance but simpler.
- 🏗️ **Practical foundation**: Used in real systems (e.g., RAMCloud, ZooKeeper).   

---

### **📌 1. Introduction**
**Problem**:
- Paxos is **hard to understand** and implement, despite being the standard for consensus.  
- Real-world Paxos implementations deviate significantly from theory, leading to complexity and errors.   

**Solution**:  
Raft prioritizes **understandability** by:
- 🧩 **Decomposition**: Leader election, log replication, safety.
- 🧠 **State reduction**: Fewer edge cases vs. Paxos.
- 📊 **User study**: 33/43 students understood Raft better than Paxos.

**Novel Features**:
- 👑 **Strong leader**: Log entries flow only from leader → followers.
- ⏱️ **Randomized leader election**: Resolves conflicts quickly.
- 🔄 **Membership changes**: Safe transitions via joint consensus.

---

### **📌 2. Replicated State Machines**
**Core Concept**:
- A **replicated log** ensures all servers execute the same commands in the same order (deterministic state machines).
- Consensus algorithms manage this log.

**Properties**:
- 🔒 **Safety**: No incorrect results under network delays/partitions.
- ⚡ **Availability**: Works if majority of servers are operational.
- ⏳ **Timing independence**: Consistency doesn’t rely on clocks.

**Example Systems**:
- Google’s Chubby, ZooKeeper, HDFS.


---

### **📌 3. What’s Wrong with Paxos?**
**Issues**:
1. **Complexity**:
    - Single-decree Paxos is hard to intuit; multi-Paxos adds more complexity.
    - "Even seasoned researchers struggle with Paxos."
2. **Practical gaps**:
    - No standard multi-Paxos implementation.
    - Real systems (e.g., Chubby) deviate from theory.

**Result**:
- Raft avoids Paxos’s "dense" two-phase design by using a **leader-centric approach**.

---

### **📌 4. Designing for Understandability**
**Techniques**:
1. **Decomposition**:
    - Split into leader election, log replication, safety.
2. **State space reduction**:
    - No log holes; limited inconsistency cases.
3. **Randomization**:
    - Simplifies leader election (randomized timeouts).

**Goal**:
- Make it "obvious why it works" for implementers.

---

### **📌 5. Raft Consensus Algorithm**

#### **5.1 Basics**
- **Server states**: Leader, follower, candidate.
- **Terms**: Logical time periods with elections (Figure 5).
- **RPCs**: `RequestVote` (elections), `AppendEntries` (log replication + heartbeats).

#### **5.2 Leader Election**

- **Process**:
    1. Followers → candidates if leader heartbeat times out.
    2. Candidates request votes; majority wins → leader.
    3. Randomized timeouts prevent split votes.

- **Safety**: Only one leader per term (Election Safety Property).

#### **5.3 Log Replication**
- **Leader appends entries** → replicates to followers.
- **Committed entries**: Applied to state machines once replicated to majority.
- **Log Matching Property**:
    - Same index + term → same command and preceding entries.

#### **5.4 Safety**
- **Leader completeness**: Leaders must have all committed entries (ensured by voting restrictions).
- **Election restriction**: Voters reject candidates with stale logs.
- **Commit rules**: Only current-term entries commit directly; older entries commit indirectly.
#### **5.5 Timing**
- **Requirement**: `broadcastTime ≪ electionTimeout ≪ MTBF`.
- Ensures stable leadership and progress.


---

### **📌 6. Cluster Membership Changes**
**Challenge**: Switching configurations atomically without split-brain.  
**Solution**: **Joint consensus** (transitional phase):
1. Replicate to both old (`C_old`) and new (`C_new`) configurations.
2. Commit `C_new` only after joint consensus is safe.  
    **Key**: Overlapping majorities prevent dual leadership.

---

### **📌 7. Log Compaction**
**Problem**: Unbounded log growth → storage/performance issues.  
**Solution**: **Snapshots**:
- Servers independently save state + metadata (last included index/term).
- Leaders send snapshots to lagging followers via `InstallSnapshot` RPC.

---

### **📌 8. Client Interaction**
- Clients contact leaders (redirected if needed).
- **Linearizability**:
    - Unique client IDs + sequence numbers prevent duplicate execution.
    - Read-only queries use leader heartbeats to avoid stale reads.

---

### **📌 9. Evaluation**
1. **Understandability**:
    - Raft scored **4.9 points higher** than Paxos in user studies.
2. **Correctness**:
    - Formal TLA+ proofs for safety properties.
3. **Performance**:
    - Elects leaders in **~50ms** with randomized timeouts.

---

### **📌 10. Related Work**
- **Paxos**: Complex, leader election orthogonal to consensus.
- **VR/ZooKeeper**: Leader-based but more complex message flows.
- **EPaxos**: Leaderless optimizations for commutativity (higher complexity).

---

### **📌 11. Conclusion**
Raft’s **understandability** comes from
- Strong leadership.
- Clear decomposition.
- Reduced state space.  
    It’s **safer for implementations** and **easier to teach** than Paxos


# *How does Raft handle leader elections*

## *1. Election Trigger Mechanism*
- **Heartbeat Monitoring**: Followers monitor leader activity via periodic heartbeats (empty AppendEntries RPCs)
- **Randomized Election Timeout**: Each follower has a unique timeout (150–300 ms). If no heartbeat arrives before this timeout expires, the follower transitions to **candidate** and starts an election

## *2. Election Process*
### *Step 1: Candidate Initiation*
- A candidate increments its term number, votes for itself, and broadcasts **RequestVote RPCs** to all servers. These RPCs include:
    - Current term
    - Last log entry index and term (to prove log completeness)
### *Step 2: Voting Rules*
Servers grant votes if:
- They haven’t voted in this term.
- The candidate’s log is at least as up-to-date as theirs (prevents stale leaders)
### *Step 3: Election Outcomes*
- **Majority Win**: If a candidate secures votes from >50% of servers, it becomes leader and sends heartbeats to assert authority
- **Higher-Term Leader**: If another server claims leadership with a ≥ term, the candidate reverts to follower
- **Split Vote**: If no majority is achieved, the election times out, and a new election begins with a **new randomized timeout** to reduce repeated splits

## *3. Safety Guarantees*
- **Leader Completeness**: Only servers with all committed log entries can win elections (enforced via log comparison in RequestVote)
- **Single Leader Per Term**: Each server votes once per term, ensuring at most one leader
- **Term Synchronization**: All servers update their term to the highest value observed, preventing stale leaders from disrupting the cluster

## *4. Example Workflow*
1. **Cluster Startup**: All nodes start as followers. Node E (with the shortest timeout) triggers an election first 
2. **Vote Collection**: Node E sends RequestVote RPCs. Nodes grant votes if E’s log is up-to-date.
3. **Leadership Assertion**: E becomes leader upon majority confirmation and begins sending heartbeats

## *5. Key Optimizations*
- **Randomized Timeouts**: Minimize split votes by staggering election starts
- **Log Consistency Checks**: Ensure new leaders have the most complete logs, preserving system integrity

Raft’s leader election mechanism prioritizes safety and liveness, enabling distributed systems to recover quickly from failures while maintaining consistency.


# *Raft Leader Election : Example

### *Setup*
Imagine a Raft cluster with 5 servers: S1, S2, S3, S4, and S5. Initially, S1 is the leader, and the others are followers. The leader regularly sends heartbeat messages (AppendEntries RPCs) to followers to maintain authority.

### *Step 1: Leader Failure Detection*
- Suppose S1 crashes or becomes unreachable.
- Followers expect heartbeats from S1 at regular intervals.
- Each follower has a randomized election timeout (e.g., between 150ms and 300ms).
- If a follower does not receive a heartbeat before its timeout expires, it assumes the leader has failed.

For example, S3’s election timeout expires first because it had the shortest timeout.

### *Step 2: Becoming a Candidate and Starting an Election*
- S3 increments its current term number (say from term 2 to term 3).
- S3 votes for itself.
- S3 sends **RequestVote RPCs** to S1, S2, S4, and S5. This RPC includes:
    - S3’s current term (3).
    - The index and term of S3’s last log entry (to prove it has the most up-to-date log).

### *Step 3: Voting by Other Servers*

- Each follower receiving the RequestVote RPC checks:
    - If the candidate’s term is at least as large as their current term.
    - If they have not already voted in this term.
    - If the candidate’s log is at least as up-to-date as their own log (comparing last log term and index).
- If all conditions hold, they grant their vote to S3.
- For example:
    - S2 and S4 grant votes to S3.
    - S5 might deny vote if it has a more up-to-date log or has already voted for another candidate.

### *Step 4: Election Outcome*
- If S3 receives votes from a majority (3 out of 5), it becomes the new leader.
- S3 immediately sends heartbeat messages (empty AppendEntries RPCs) to all followers to assert leadership and prevent new elections.
- Followers reset their election timers upon receiving these heartbeats.

### *Step 5: Handling Complications*

- **Case: Another Candidate Emerges Simultaneously**  
    Suppose S2’s election timeout expires almost at the same time as S3’s, and S2 also becomes a candidate and requests votes.
    - Votes may split, e.g., S3 gets votes from S4 and S5, S2 gets votes from S1 and itself.
    - No candidate gets a majority → split vote.
    - Both candidates’ election timers expire again after randomized timeouts.
    - A new election starts in a new term, and the process repeats until a candidate wins a majority.
        
- **Case: Candidate Receives AppendEntries from Another Leader**  
    If S3 is a candidate but receives an AppendEntries RPC from a server claiming to be leader with a higher or equal term, S3 steps down to follower and recognizes that leader.
    
- **Case: Candidate Receives RequestVote with Higher Term**  
    If S3 receives a RequestVote RPC with a higher term, it updates its term and reverts to follower.

### *Step 6: Term and Vote Updates*

- All servers update their current term to the highest term they observe in any RPC.
- Servers vote only once per term, preventing multiple leaders in the same term.
- This guarantees at most one leader per term.

| ***Step***                    | ***Description***                                                                                                     |
| ----------------------- | --------------------------------------------------------------------------------------------------------------- |
| 1. Leader failure       | Followers detect missing heartbeats and start election timers.                                                  |
| 2. Candidate initiation | A follower whose timeout expires becomes candidate, increments term, votes for self, sends RequestVote RPCs.    |
| 3. Vote granting        | Followers grant votes if candidate’s term and log are up-to-date and they haven’t voted yet.                    |
| 4. Election result      | Candidate with majority votes becomes leader and sends heartbeats.                                              |
| 5. Handling conflicts   | Split votes cause new elections with randomized timeouts; candidates step down if they see higher term leaders. |
| 6. Term synchronization | All servers update to highest term seen, ensuring election safety.                                              |

### *Why This Works*

- **Randomized Election Timeouts** reduce split votes by staggering election starts.
- **Log Up-to-date Checks** ensure only candidates with the most complete logs can become leaders, preserving consistency.
- **Majority Voting** ensures a single leader is chosen, preventing conflicting leaders.
- **Term Numbers** synchronize cluster state and prevent stale leaders.

| Limitation                   | Description                                                            | Modern Mitigations                                              |
| ---------------------------- | ---------------------------------------------------------------------- | --------------------------------------------------------------- |
| Scalability                  | High communication overhead in large clusters                          | Sharding, Multi-Raft, Witness nodes                             |
| High Resource/Energy Use     | Majority quorum requires many full nodes                               | Quorum tuning, Witness nodes                                    |
| Leader Bottleneck & Downtime | Single leader can be overloaded; brief unavailability during elections | Read-only queries on followers, fast failover, hybrid protocols |
| Log Divergence               | Temporary inconsistencies after partitions                             | Snapshotting, adaptive reconciliation                           |
| Limited Real-World Testing   | Insufficient evaluation under diverse conditions                       | Systematic evaluation frameworks, adaptive tuning               |
| Membership Change Overhead   | Noisy and complex with frequent node changes                           | Joint consensus optimization, dynamic management                |
# -
[[Distributed Guide]]
[[999. Questions]]