### 🔍 Abstract

- **What it is:** - Google published the MapReduce paper, introducing it as a framework for large-scale distributed data processing. It powered many internal systems at *Google* and inspired the *open-source Hadoop MapReduce* (developed by Yahoo and the Apache community).
    
- **Key ideas:**
    
    - MapReduce is a programming model 🧠 and an implementation⚙️ used to process huge datasets in parallel across a distributed system.
        
    - Users write a `map()` function 🗺️ to generate *intermediate key-value pairs* and a `reduce()` function  to merge those values.
        
    - It's easy to use, hides complex distributed computing details, and scales to thousands of machines.
        
    - Used extensively at Google with thousands of daily jobs.


---

### 1️⃣ Introduction

- **Problem:** Simple data tasks become complex when data is huge and distributed across many machines 🧱. Google had to repeatedly perform large-scale data processing tasks (e.g., web indexing, log summarization, query stats).

- **Challenges**: Distributed computing introduced complexity (parallelization, fault-tolerance, data distribution, load balancing).
    
- **Solution:** MapReduce abstracts away the complexities (parallelism, fault-tolerance, load balancing) 💡.
    
- **Inspiration:** Comes from `map` and `reduce` functions in Lisp 🧾.

- **Goal**: Simplify programming by abstracting distributed system complexity into a *library.
    
- **Result:** Developers can focus on logic instead of infrastructure 🤖.

Google engineers had to frequently build programs to analyze *massive datasets* — for example:
- Creating *inverted indices* (used in search engines).
- Summarizing logs (e.g., top queries per day).
- Analyzing graph structures of web documents (e.g., for PageRank).
    
While the *logic was simple*, the *scale* made everything hard. Developers had to write extensive code for:
- Splitting data across machines.
- Handling machine failures.
- Synchronizing parallel tasks.
This *extra complexity* often overshadowed the actual computation.
#### Questions
>  - *"Why is MapReduce better than hand-written distributed code?"*
     > It abstracts complex tasks like fault-tolerance, distribution, and parallelization, letting the developer focus on the logic.
>     
> - *"What makes the MapReduce model suitable for large data sets?"*
     > Its functional model allows parallelism and automatic fault recovery, making it ideal for petabyte-scale data.
>     
> - *"How is fault tolerance handled in MapReduce?"*
     > Through re-execution of failed tasks using deterministic map/reduce functions.
---

### 2️⃣ Programming Model

#### 🧠 Core Concept:

- Input: A set of key/value pairs 🔑📄.
    
- **`Map` function:** Takes an input pair and outputs intermediate key/value pairs.
- **`Reduce` function:** Merges values associated with the same key.
    

#### 📖 2.1 Example: Word Count
```
map(String key, String value): 
// key: document name 
// value: document contents 
	for each word w in value:
		EmitIntermediate(w, "1"); 

reduce(String key, Iterator values): 
// key: a word
// values: a list of counts

	int result = 0;
	for each v in values: 
		result += ParseInt(v);
	Emit(AsString(result));
```
🧠 Simple logic → Powerful processing on large data!

#### 🧬 2.2 Types
```
map(k1, v1) → list(k2, v2) 
reduce(k2, list(v2)) → list(v2)
```
➡️ Inputs, intermediate data, and outputs can have different types.

#### 💡 2.3 More Examples

- **Distributed Grep:** Search for a pattern 📃🔍.
- **URL Access Frequency:** Count how many times each URL is accessed 🌐📈.
- **Reverse Web-Link Graph:** Track incoming links to a webpage 🔄.
- **Term Vector per Host:** Summarize word frequencies per website 🧠🌍.
- **Inverted Index:** Create a search index 📚.
- **Distributed Sort:** Sort large datasets efficiently 🔢.

---

### 3️⃣ Implementation

Targeted for clusters of low-cost machines 🧱 connected via Ethernet 🌐.
- No centralized or RAID storage — instead, each machine has its own IDE disk.
- Data is managed by Google’s proprietary Distributed File System (GFS)**.
- GFS handles:
    - Replication for reliability.
    - Scalability to petabyte-scale data.
    - Recovery from node or disk failures.
- Job Scheduling
	- Users don’t worry about machine details.
	- A job is composed of Map and Reduce tasks.
	- Jobs are submitted to a *cluster scheduler*:
	    - Scheduler assigns tasks to available machines.
	    - Supports load balancing and handles *re-execution on failure*.

#### ⚙️ 3.1 Execution Overview

🗂️ Process:

- **Input Splitting:** The input data is split into **M chunks** (16–64 MB each), distributed across machines.
- **Workers and Master:**
    - One **master** coordinates task assignment.
    - Multiple **workers** perform map or reduce tasks.
- **Map Phase:**
    1. Workers read input splits.
    2. Run the user-defined **Map** function.
    3. Buffer intermediate key/value pairs in memory.
    4. Write these to **local disk**, partitioned into **R parts** using a hash function.
- **Shuffle and Sort (handled by the framework):**  
	1. After map tasks finish (or partially finish), reduce workers are notified and **pull** (fetch) their corresponding partitions of intermediate data from the mappers’ local disks.  
	2. The reduce workers **sort the intermediate data by key** and group all values with the same key together. This sorting and grouping happens **before your reduce function is called**.
- **Reduce Phase:**  
    5. Reduce workers are notified of map outputs.  
    6. They fetch, sort by key, and pass grouped values to the **Reduce** function.  
    7. Output is written to R separate files.
    
🔁 Intermediate files = bridge between Map and Reduce.

#### 🧠 3.2 Master Data Structures

- - Tracks state (`idle`, `in-progress`, `completed`) and worker identity for every task.
- Maintains metadata on intermediate outputs from completed map tasks.
- Forwards this metadata to reduce workers for data fetching.

#### 💥 3.3 Fault Tolerance

- - **Worker Failure:**
    - Master **pings** workers periodically.
    - If unresponsive, their tasks are **reset and reassigned**.
    - Map task outputs on a failed worker’s disk are considered lost and must be **re-executed**
    - Reduce outputs are stored in a global file system, so they **don’t need re-execution**.
        
- **Master Failure:**
    - While master could checkpoint, the original implementation **aborts the job** upon master failure.
        
- **Semantics in Failures:**
    - Deterministic map/reduce functions → output is **as if sequential**.
    - Non-deterministic functions → output is **still valid**, though not necessarily reproducible.
#### 📍 3.4 Locality

- Attempts to run map tasks on machines that **already have the input data** locally (via GFS).
- Reduces network traffic and improves performance.
- GFS stores data in 64MB blocks with **3 replicas**.
    
#### 🧮 3.5 Task Granularity

- **M (map tasks)** and **R (reduce tasks)** should be **much larger than the number of workers**:
    - Better **load balancing**.
    - Faster **recovery** from failures.
- Limits: Master must handle O(M+R) scheduling and O(M×R) metadata (though lightweight).
- R is often constrained by the number of final output files desired.
- Example: M = 200,000, R = 5,000, run on 2,000 machines.
#### 🐢 3.6 Backup Tasks

- Some tasks (stragglers) take too long ⌛.
- Run duplicate (backup) tasks – fastest one wins 🏁.
- Improves overall job time by handling slow machines.

---

### 4️⃣ Refinements

#### 🔢 4.1 Partitioning Function

- **Purpose**: Determines how output of `Map()` is split among `R` reduce tasks. 
- **Default behavior**: Hashing the key, then using modulo operation (`hash(key) % R`).
- **Customization**: You can write your own partitioner (e.g., send "a-m" keys to one reducer, "n-z" to another).
- **Why it matters**: Helps in **load balancing** and **logical grouping** (e.g., time-based, range-based).    

#### 🔀 4.2 Ordering Guarantees

- Keys are processed in **sorted order** within each partition 📜.

#### ⚙️ 4.3 Combiner Function

- **Goal**: Reduce network traffic by partially aggregating data on the mapper node.
- **When to use**: When the reduce function is **commutative and associative** (like sum, max, etc.).
- **Behavior**: Combiner runs after map, before shuffle. Often reuses the same code as the reducer.
- **Example**: In word count, instead of sending 1,000 `<the, 1>` records, combiner sends `<the, 100>`.

#### 📥📤 4.4 Input/Output Types

- Flexible input/output (text, database, memory structures) 🔌.
- Users can define custom formats.

#### 📎 4.5 Side-effects

- Can write additional output files 📂.
- Must ensure **atomicity** and **idempotency**.

#### ⛔ 4.6 Skipping Bad Records

- If some records crash the program, they can be skipped ✅.
- Useful for noisy datasets or bugs in third-party code 🐞.

#### 🧪 4.7 Local Execution

- Debug or test locally without using a full cluster 🧑‍💻.

#### 📊 4.8 Status Information

- Web interface shows real-time progress 📈.
- Helps diagnose issues (e.g., failed tasks, slow tasks) 🔎.

#### 🔢 4.9 Counters

- Custom counters to track values like word count or error count 🔢.
- Auto-incremented and aggregated centrally 🧮.
    

---

### 5️⃣ Performance

#### 🖥️ 5.1 Cluster

- 1800 machines, each with:
    
    - 2 GHz CPUs ⚙️
        
    - 4GB RAM 💾
        
    - 160GB disks 💽
        
    - Gigabit Ethernet ⚡
        

#### 🧾 5.2 Grep Benchmark

- Searches for a pattern in 1TB of data 🔍.
- Peaks at 30GB/s 📈.
- Finishes in ~150 seconds ⏱️.

#### 📦 5.3 Sort Benchmark

- Sorts 1TB of 100-byte records 📊.
- Uses Map to extract keys, Reduce to output sorted results.
- With 1700 machines, finishes in 891s ⏱️.

#### 🔁 5.4 Effect of Backup Tasks

- Without backup: 1283s
- With backup: 891s  
    ✅ 44% improvement!

#### 💀 5.5 Machine Failures

- 200 workers killed mid-job 🔪.
- System recovered with only 5% increase in time 👏.
    

---

### 6️⃣ Experience

- Used across Google 🌍 for:
    - Machine learning 🤖
    - Data extraction 📤
    - Query trends (e.g., Google Zeitgeist) 📊
    - Indexing system rewrite 📚
        

✅ Reduced 3800 lines of C++ to 700 lines!
📈 Used for over **29,000 jobs in August 2004** alone!  
📊 Handled **3,288 TB** of input data in that month!

---

### 7️⃣ Related Work

- Compares MapReduce to:
    - Parallel prefix computations
    - Bulk Synchronous Processing
    - Active Disks, River, NOW-Sort
        
- MapReduce is **simpler and more fault-tolerant** 💪.

### Downsides of MapReduce

1. **High Latency / Batch-Oriented**
    - MapReduce jobs are **designed for batch processing**, which means each job can take minutes to hours to run. It’s not suited for **real-time or low-latency** processing needs.
    - The startup overhead (job scheduling, data shuffling, etc.) is significant.
        
2. **Rigid Two-Stage Model**
    - The strict **Map → Shuffle → Reduce** flow limits flexibility. Complex workflows often require chaining many jobs, which is inefficient.
        
3. **Inefficient Iterative Processing**
    - Many machine learning and graph algorithms require **iterative processing** (running multiple rounds of computation).
    - MapReduce writes intermediate results to disk after every stage, causing **slow disk I/O** and wasted resources.
        
4. **Lack of In-Memory Computation**
    - MapReduce relies heavily on disk for intermediate data storage. This leads to slower performance compared to systems that can keep data **in-memory** across multiple operations.
        
5. **Poor Support for Complex DAGs (Directed Acyclic Graphs)**
    - MapReduce can only express very simple DAGs (one Map followed by one Reduce), so complex workflows need manual chaining, which is error-prone and inefficient.
        
6. **Static Resource Allocation and Scheduling**
    - Resource management and scheduling in early MapReduce implementations are often static and not optimized for cluster utilization.
        

---

### Why Newer Technologies Replaced or Extended MapReduce

To address these limitations, new big data processing frameworks were developed. They cover the weaknesses of MapReduce by introducing:
### 1. **In-Memory Processing**
- Frameworks like **Apache Spark** use resilient distributed datasets (RDDs) and keep data in memory across operations, dramatically speeding up iterative and interactive workloads.
### 2. **Flexible DAG Execution**
- Systems support complex DAGs, allowing arbitrary directed acyclic graphs of operations instead of the fixed two-stage MapReduce pipeline. This enables more sophisticated workflows.
### 3. **Real-Time and Streaming Support**
- Technologies such as **Apache Flink**, **Apache Kafka Streams**, and **Spark Streaming** enable near real-time stream processing, low latency, and continuous data computation.
### 4. **Advanced Fault Tolerance and Scheduling**
- New frameworks incorporate dynamic resource allocation, better fault tolerance, and fine-grained task scheduling to improve cluster utilization and reduce job delays.    
### 5. **Higher-Level APIs and Libraries**
- Richer APIs for SQL queries (e.g., Spark SQL), machine learning (MLlib), graph processing (GraphX), and more, allow easier development compared to low-level MapReduce coding.
### 6. **reducers must wait until all mappers are done** 
- **Idle Resources**:   
    While mappers are running, reducers are mostly idle. This leads to under-utilization of cluster resources.
    
- **Stragglers Slow Everything Down**:  
    If even one mapper is slow (a "straggler"), all reducers must wait — delaying the entire computation.
    
- **No Pipelining**:  
    MapReduce doesn't allow reducers to start processing partial data as it becomes available — everything is **batched**, making the system **less real-time**.
    
- **High Latency**:  
    Since reducers wait until all map tasks complete, it introduces a **barrier** — which increases the **end-to-end latency** of jobs.

#### ✅ Apache Spark:

- Uses **Resilient Distributed Datasets (RDDs)** and a **DAG scheduler**.
- Allows **pipelining** of operations when possible.
- Reduces unnecessary shuffling and waiting.
- Can **cache** intermediate results in memory.
#### ✅ Apache Flink:

- True **streaming** engine, supports **continuous data flow**.
- Reduces latency drastically by enabling **event-at-a-time** processing.
- Can begin processing as soon as data starts arriving.


---

