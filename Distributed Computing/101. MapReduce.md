### ğŸ” Abstract

- **What it is:** - Google published the MapReduce paper, introducing it as aÂ framework for large-scale distributed data processing. It powered many internal systems at *Google* and inspired theÂ *open-source Hadoop MapReduce*Â (developed by Yahoo and the Apache community).
    
- **Key ideas:**
    
    - MapReduce is a programming model ğŸ§  and an implementationâš™ï¸ used to process huge datasets in parallel across a distributed system.
        
    - Users write a `map()` function ğŸ—ºï¸ to generate *intermediate key-value pairs* and a `reduce()` function  to merge those values.
        
    - It's easy to use, hides complex distributed computing details, and scales to thousands of machines.
        
    - Used extensively at Google with thousands of daily jobs.


---

### 1ï¸âƒ£ Introduction

- **Problem:** Simple data tasks become complex when data is huge and distributed across many machines ğŸ§±. Google had to repeatedly perform large-scale data processing tasks (e.g., web indexing, log summarization, query stats).

- **Challenges**: Distributed computing introduced complexity (parallelization, fault-tolerance, data distribution, load balancing).
    
- **Solution:** MapReduce abstracts away the complexities (parallelism, fault-tolerance, load balancing) ğŸ’¡.
    
- **Inspiration:** Comes from `map` and `reduce` functions in Lisp ğŸ§¾.

- **Goal**: Simplify programming byÂ abstracting distributed system complexityÂ into aÂ *library.
    
- **Result:** Developers can focus on logic instead of infrastructure ğŸ¤–.

Google engineers had to frequently build programs to analyzeÂ *massive datasets*Â â€” for example:
- CreatingÂ *inverted indices*Â (used in search engines).
- Summarizing logs (e.g., top queries per day).
- Analyzing graph structures of web documents (e.g., for PageRank).
    
While theÂ *logic was simple*, theÂ *scale*Â made everything hard. Developers had to write extensive code for:
- Splitting data across machines.
- Handling machine failures.
- Synchronizing parallel tasks.
ThisÂ *extra complexity*Â often overshadowed the actual computation.
#### Questions
>  - *"Why is MapReduce better than hand-written distributed code?"*
     > It abstracts complex tasks like fault-tolerance, distribution, and parallelization, letting the developer focus on the logic.
>     
> - *"What makes the MapReduce model suitable for large data sets?"*
     > Its functional model allows parallelism and automatic fault recovery, making it ideal for petabyte-scale data.
>     
> - *"How is fault tolerance handled in MapReduce?"*
     > ThroughÂ re-executionÂ of failed tasks using deterministic map/reduce functions.
---

### 2ï¸âƒ£ Programming Model

#### ğŸ§  Core Concept:

- Input: A set of key/value pairs ğŸ”‘ğŸ“„.
    
- **`Map` function:** Takes an input pair and outputs intermediate key/value pairs.
- **`Reduce` function:** Merges values associated with the same key.
    

#### ğŸ“– 2.1 Example: Word Count
```
map(String key, String value): 
// key: document name 
// value: document contents 
	for each word w in value:
		EmitIntermediate(w, "1"); 

reduce(String key, Iterator values): 
// key: a word
// values: a list of counts

	int result = 0;
	for each v in values: 
		result += ParseInt(v);
	Emit(AsString(result));
```
ğŸ§  Simple logic â†’ Powerful processing on large data!

#### ğŸ§¬ 2.2 Types
```
map(k1, v1) â†’ list(k2, v2) 
reduce(k2, list(v2)) â†’ list(v2)
```
â¡ï¸ Inputs, intermediate data, and outputs can have different types.

#### ğŸ’¡ 2.3 More Examples

- **Distributed Grep:** Search for a pattern ğŸ“ƒğŸ”.
- **URL Access Frequency:** Count how many times each URL is accessed ğŸŒğŸ“ˆ.
- **Reverse Web-Link Graph:** Track incoming links to a webpage ğŸ”„.
- **Term Vector per Host:** Summarize word frequencies per website ğŸ§ ğŸŒ.
- **Inverted Index:** Create a search index ğŸ“š.
- **Distributed Sort:** Sort large datasets efficiently ğŸ”¢.

---

### 3ï¸âƒ£ Implementation

Targeted for clusters of low-cost machines ğŸ§± connected via Ethernet ğŸŒ.
- No centralized or RAID storage â€” instead,Â each machine has its own IDE disk.
- Data is managed by Googleâ€™sÂ proprietary Distributed File System (GFS)**.
- GFS handles:
    - ReplicationÂ for reliability.
    - ScalabilityÂ to petabyte-scale data.
    - Recovery fromÂ node or disk failures.
- Job Scheduling
	- Users donâ€™t worry about machine details.
	- AÂ jobÂ is composed ofÂ Map and Reduce tasks.
	- Jobs areÂ submitted to a *cluster scheduler*:
	    - Scheduler assigns tasks to available machines.
	    - Supports load balancing and handles *re-execution on failure*.

#### âš™ï¸ 3.1 Execution Overview

ğŸ—‚ï¸ Process:

- **Input Splitting:**Â The input data is split intoÂ **M chunks**Â (16â€“64 MB each), distributed across machines.
- **Workers and Master:**
    - OneÂ **master**Â coordinates task assignment.
    - MultipleÂ **workers**Â perform map or reduce tasks.
- **Map Phase:**
    1. Workers read input splits.
    2. Run the user-definedÂ **Map**Â function.
    3. Buffer intermediate key/value pairs in memory.
    4. Write these toÂ **local disk**, partitioned intoÂ **R parts**Â using a hash function.
- **Shuffle and Sort (handled by the framework):**  
	1. After map tasks finish (or partially finish), reduce workers are notified andÂ **pull**Â (fetch) their corresponding partitions of intermediate data from the mappersâ€™ local disks.  
	2. The reduce workersÂ **sort the intermediate data by key**Â and group all values with the same key together. This sorting and grouping happensÂ **before your reduce function is called**.
- **Reduce Phase:**  
    5. Reduce workers are notified of map outputs.  
    6. They fetch, sort by key, and pass grouped values to theÂ **Reduce**Â function.  
    7. Output is written to R separate files.
    
ğŸ” Intermediate files = bridge between Map and Reduce.

#### ğŸ§  3.2 Master Data Structures

- - Tracks state (`idle`,Â `in-progress`,Â `completed`) and worker identity for every task.
- Maintains metadata on intermediate outputs from completed map tasks.
- Forwards this metadata to reduce workers for data fetching.

#### ğŸ’¥ 3.3 Fault Tolerance

- - **Worker Failure:**
    - MasterÂ **pings**Â workers periodically.
    - If unresponsive, their tasks areÂ **reset and reassigned**.
    - Map task outputs on a failed workerâ€™s disk are considered lost and must beÂ **re-executed**
    - Reduce outputs are stored in a global file system, so theyÂ **donâ€™t need re-execution**.
        
- **Master Failure:**
    - While master could checkpoint, the original implementationÂ **aborts the job**Â upon master failure.
        
- **Semantics in Failures:**
    - Deterministic map/reduce functions â†’ output isÂ **as if sequential**.
    - Non-deterministic functions â†’ output isÂ **still valid**, though not necessarily reproducible.
#### ğŸ“ 3.4 Locality

- Attempts to run map tasks on machines thatÂ **already have the input data**Â locally (via GFS).
- Reduces network traffic and improves performance.
- GFS stores data in 64MB blocks withÂ **3 replicas**.
    
#### ğŸ§® 3.5 Task Granularity

- **M (map tasks)**Â andÂ **R (reduce tasks)**Â should beÂ **much larger than the number of workers**:
    - BetterÂ **load balancing**.
    - FasterÂ **recovery**Â from failures.
- Limits: Master must handle O(M+R) scheduling and O(MÃ—R) metadata (though lightweight).
- R is often constrained by the number of final output files desired.
- Example: M = 200,000, R = 5,000, run on 2,000 machines.
#### ğŸ¢ 3.6 Backup Tasks

- Some tasks (stragglers) take too long âŒ›.
- Run duplicate (backup) tasks â€“ fastest one wins ğŸ.
- Improves overall job time by handling slow machines.

---

### 4ï¸âƒ£ Refinements

#### ğŸ”¢ 4.1 Partitioning Function

- **Purpose**: Determines how output ofÂ `Map()`Â is split amongÂ `R`Â reduce tasks. 
- **Default behavior**: Hashing the key, then using modulo operation (`hash(key) % R`).
- **Customization**: You can write your own partitioner (e.g., send "a-m" keys to one reducer, "n-z" to another).
- **Why it matters**: Helps inÂ **load balancing**Â andÂ **logical grouping**Â (e.g., time-based, range-based).    

#### ğŸ”€ 4.2 Ordering Guarantees

- Keys are processed in **sorted order** within each partition ğŸ“œ.

#### âš™ï¸ 4.3 Combiner Function

- **Goal**: Reduce network traffic by partially aggregating data on the mapper node.
- **When to use**: When the reduce function isÂ **commutative and associative**Â (like sum, max, etc.).
- **Behavior**: Combiner runs after map, before shuffle. Often reuses the same code as the reducer.
- **Example**: In word count, instead of sending 1,000Â `<the, 1>`Â records, combiner sendsÂ `<the, 100>`.

#### ğŸ“¥ğŸ“¤ 4.4 Input/Output Types

- Flexible input/output (text, database, memory structures) ğŸ”Œ.
- Users can define custom formats.

#### ğŸ“ 4.5 Side-effects

- Can write additional output files ğŸ“‚.
- Must ensure **atomicity** and **idempotency**.

#### â›” 4.6 Skipping Bad Records

- If some records crash the program, they can be skipped âœ….
- Useful for noisy datasets or bugs in third-party code ğŸ.

#### ğŸ§ª 4.7 Local Execution

- Debug or test locally without using a full cluster ğŸ§‘â€ğŸ’».

#### ğŸ“Š 4.8 Status Information

- Web interface shows real-time progress ğŸ“ˆ.
- Helps diagnose issues (e.g., failed tasks, slow tasks) ğŸ”.

#### ğŸ”¢ 4.9 Counters

- Custom counters to track values like word count or error count ğŸ”¢.
- Auto-incremented and aggregated centrally ğŸ§®.
    

---

### 5ï¸âƒ£ Performance

#### ğŸ–¥ï¸ 5.1 Cluster

- 1800 machines, each with:
    
    - 2 GHz CPUs âš™ï¸
        
    - 4GB RAM ğŸ’¾
        
    - 160GB disks ğŸ’½
        
    - Gigabit Ethernet âš¡
        

#### ğŸ§¾ 5.2 Grep Benchmark

- Searches for a pattern in 1TB of data ğŸ”.
- Peaks at 30GB/s ğŸ“ˆ.
- Finishes in ~150 seconds â±ï¸.

#### ğŸ“¦ 5.3 Sort Benchmark

- Sorts 1TB of 100-byte records ğŸ“Š.
- Uses Map to extract keys, Reduce to output sorted results.
- With 1700 machines, finishes in 891s â±ï¸.

#### ğŸ” 5.4 Effect of Backup Tasks

- Without backup: 1283s
- With backup: 891s  
    âœ… 44% improvement!

#### ğŸ’€ 5.5 Machine Failures

- 200 workers killed mid-job ğŸ”ª.
- System recovered with only 5% increase in time ğŸ‘.
    

---

### 6ï¸âƒ£ Experience

- Used across Google ğŸŒ for:
    - Machine learning ğŸ¤–
    - Data extraction ğŸ“¤
    - Query trends (e.g., Google Zeitgeist) ğŸ“Š
    - Indexing system rewrite ğŸ“š
        

âœ… Reduced 3800 lines of C++ to 700 lines!
ğŸ“ˆ Used for over **29,000 jobs in August 2004** alone!  
ğŸ“Š Handled **3,288 TB** of input data in that month!

---

### 7ï¸âƒ£ Related Work

- Compares MapReduce to:
    - Parallel prefix computations
    - Bulk Synchronous Processing
    - Active Disks, River, NOW-Sort
        
- MapReduce is **simpler and more fault-tolerant** ğŸ’ª.

### Downsides of MapReduce

1. **High Latency / Batch-Oriented**
    - MapReduce jobs areÂ **designed for batch processing**, which means each job can take minutes to hours to run. Itâ€™s not suited forÂ **real-time or low-latency**Â processing needs.
    - The startup overhead (job scheduling, data shuffling, etc.) is significant.
        
2. **Rigid Two-Stage Model**
    - The strictÂ **Map â†’ Shuffle â†’ Reduce**Â flow limits flexibility. Complex workflows often require chaining many jobs, which is inefficient.
        
3. **Inefficient Iterative Processing**
    - Many machine learning and graph algorithms requireÂ **iterative processing**Â (running multiple rounds of computation).
    - MapReduce writes intermediate results to disk after every stage, causingÂ **slow disk I/O**Â and wasted resources.
        
4. **Lack of In-Memory Computation**
    - MapReduce relies heavily on disk for intermediate data storage. This leads to slower performance compared to systems that can keep dataÂ **in-memory**Â across multiple operations.
        
5. **Poor Support for Complex DAGs (Directed Acyclic Graphs)**
    - MapReduce can only express very simple DAGs (one Map followed by one Reduce), so complex workflows need manual chaining, which is error-prone and inefficient.
        
6. **Static Resource Allocation and Scheduling**
    - Resource management and scheduling in early MapReduce implementations are often static and not optimized for cluster utilization.
        

---

### Why Newer Technologies Replaced or Extended MapReduce

To address these limitations, new big data processing frameworks were developed. They cover the weaknesses of MapReduce by introducing:
### 1.Â **In-Memory Processing**
- Frameworks likeÂ **Apache Spark**Â use resilient distributed datasets (RDDs) and keep data in memory across operations, dramatically speeding up iterative and interactive workloads.
### 2.Â **Flexible DAG Execution**
- Systems support complex DAGs, allowing arbitrary directed acyclic graphs of operations instead of the fixed two-stage MapReduce pipeline. This enables more sophisticated workflows.
### 3.Â **Real-Time and Streaming Support**
- Technologies such asÂ **Apache Flink**,Â **Apache Kafka Streams**, andÂ **Spark Streaming**Â enable near real-time stream processing, low latency, and continuous data computation.
### 4.Â **Advanced Fault Tolerance and Scheduling**
- New frameworks incorporate dynamic resource allocation, better fault tolerance, and fine-grained task scheduling to improve cluster utilization and reduce job delays.    
### 5.Â **Higher-Level APIs and Libraries**
- Richer APIs for SQL queries (e.g., Spark SQL), machine learning (MLlib), graph processing (GraphX), and more, allow easier development compared to low-level MapReduce coding.
### 6. **reducers must wait until all mappers are done** 
- **Idle Resources**:   
    While mappers are running, reducers are mostly idle. This leads to under-utilization of cluster resources.
    
- **Stragglers Slow Everything Down**:  
    If even one mapper is slow (a "straggler"), all reducers must wait â€” delaying the entire computation.
    
- **No Pipelining**:  
    MapReduce doesn't allow reducers to start processing partial data as it becomes available â€” everything isÂ **batched**, making the systemÂ **less real-time**.
    
- **High Latency**:  
    Since reducers wait until all map tasks complete, it introduces aÂ **barrier**Â â€” which increases theÂ **end-to-end latency**Â of jobs.

#### âœ… Apache Spark:

- UsesÂ **Resilient Distributed Datasets (RDDs)**Â and aÂ **DAG scheduler**.
- AllowsÂ **pipelining**Â of operations when possible.
- Reduces unnecessary shuffling and waiting.
- CanÂ **cache**Â intermediate results in memory.
#### âœ… Apache Flink:

- TrueÂ **streaming**Â engine, supportsÂ **continuous data flow**.
- Reduces latency drastically by enablingÂ **event-at-a-time**Â processing.
- Can begin processing as soon as data starts arriving.


---

