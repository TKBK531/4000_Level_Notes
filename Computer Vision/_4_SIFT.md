# *âœ…Step By Step Guide*

## ðŸ”§ **Step 1: Extract and Match Interest Points Using SIFT**

### **1.a. Load Images**

- Use grayscale images for simplicity.
    
- Start with the sample images provided, then later create your own.
    

### **1.b. Detect Interest Points with SIFT**

- Use OpenCVâ€™s SIFT detector.
    
- This finds **keypoints** (location, scale, and orientation) in the image that are stable under scale and rotation.
    

### **1.c. Implement a Simple 5Ã—5 Intensity Descriptor**

- For each keypoint, extract a **5Ã—5 patch of raw intensity values** centered at the keypoint location.
    
- Flatten this into a 25-dimensional vector.
    
- Use only this descriptor for simple matching later.
    

### **1.d. Use SIFT Descriptor from OpenCV**

- After detection, use SIFTâ€™s own **128-dimensional descriptor**.
    
- This descriptor is robust to rotation, scale, and some affine transformations.
    

### **1.e. Match Descriptors**

#### Using Your Own Matching:

- Use **Sum of Squared Differences (SSD)**:
    
    - For each feature in image 1, compute SSD to all features in image 2.
        
    - Pick the one with **minimum SSD** as the match.
        

#### Using OpenCV's SIFT Matcher:

- Use the **BFMatcher** (Brute Force Matcher) or **FLANN-based matcher**.
    
- Optionally apply **ratio test** (Loweâ€™s ratio test):
    
    - Compare distances of the best and second-best matches.
        
    - Accept match only if:  
        `distance(best) < ratio * distance(second_best)`
        

### **1.f. Visualize Matches**

- Use OpenCVâ€™s drawing functions to plot lines between matching keypoints.
    

---

## ðŸ“¸ **Step 2: Use Harris Corner Detector**

### **2.a. Detect Corners with Harris**

- Use OpenCVâ€™s Harris detector or implement your own.
    
- Harris gives **corner strength values**, not keypoints directly.
    

### **2.b. Extract Interest Points**

- Threshold the corner response map to select strong corners.
    
- Optionally, perform **non-maximum suppression** to avoid dense clusters of points.
    

### **2.c. Use Local Intensity Descriptors**

- Around each detected corner, extract a **window of intensities** (e.g., 5Ã—5, 7Ã—7, 9Ã—9).
    
- Flatten this window into a feature vector.
    
- Try multiple window sizes and observe the results.
    

### **2.d. Match Harris Descriptors**

- Use the same SSD-based matching method.
    
- Optionally, try OpenCV matchers to compare performance.
    

---

## ðŸ“Š **Step 3: Compare SIFT and Harris Results**

### **3.a. Visual Comparison**

- Show matched images for SIFT and Harris.
    
- Visually compare the number of matches, accuracy, and robustness to transformations.
    

### **3.b. Quantitative Comparison**

- Count number of matches for both methods.
    
- Calculate percentage of correct matches if ground truth is available.
    
- Compare:
    
    - **Window sizes** for Harris descriptors.
        
    - **Ratio threshold** in SIFT matching (e.g., 0.7, 0.8, 0.9).
        

### **3.c. Evaluate Under Transformations**

- Apply small transformations:
    
    - Translation
        
    - Rotation
        
    - Scale
        
    - Illumination changes
        
- Repeat detection and matching, then compare robustness of SIFT vs. Harris.



# *âœ… Summary of Key Experiments to Run*

| Task                                | Description                                                    |
| ----------------------------------- | -------------------------------------------------------------- |
| SIFT Detection + Your Descriptor    | Use 5Ã—5 patches and SSD for matching                           |
| SIFT Detection + SIFT Descriptor    | Use OpenCVâ€™s descriptor and matcher                            |
| Harris Detection + Patch Descriptor | Use raw patches and SSD                                        |
| Matching Comparison                 | Compare SSD and OpenCV matching                                |
| Parameter Tuning                    | Try different patch sizes (Harris) and ratio thresholds (SIFT) |
| Custom Images                       | Create synthetic or real test images (rotated, scaled, etc.)   |

# *âœ… Main challenges associated with selecting a clustering algorithm*

## ðŸ”¸ **1. How to define a cluster?**

Defining a **cluster** depends on the **context** and the **clustering objective**. Broadly, a cluster is:

- A group of data points that are **similar to each other** and **dissimilar from points in other groups**.
    
- In practice, this could mean:
    
    - **High density** regions (e.g., in DBSCAN),
        
    - Points close to a **common centroid** (e.g., in K-Means),
        
    - Data generated from a **common distribution** (e.g., in GMM).
        

The definition should match the **nature of the data** and **problem goal**.

---

## ðŸ”¸ **2. What features should be used?**

Choosing the right features is **critical**:

- Use features that are **relevant** to the clustering goal.
    
- Avoid features that are:
    
    - Redundant (highly correlated),
        
    - Noisy or irrelevant (they add confusion).
        
- Techniques to help:
    
    - **Feature selection** based on domain knowledge or statistical tests.
        
    - **Feature extraction** using PCA or autoencoders for dimensionality reduction.
        

ðŸ§  _Tip_: Good features produce meaningful distances between data points.

---

## ðŸ”¸ **3. Should the data be normalized?**

Yes, **usually**:

- Many clustering algorithms (like K-Means, DBSCAN) rely on **distance measures**.
    
- If features are on different scales (e.g., height in cm vs income in dollars), one will dominate.
    
- **Standardization** (mean=0, std=1) or **min-max normalization** (scale to [0, 1]) are common.
    

ðŸ“Œ Exception: Some algorithms (like tree-based models or algorithms with categorical data) may not need it.

---

## ðŸ”¸ **4. Does the data contain any outliers?**

Outliers can **distort** clustering results:

- **K-Means** is highly sensitive (outliers skew centroids).
    
- **DBSCAN** can handle outliers as noise.
    
- Consider:
    
    - Visualizing the data (e.g., box plots, scatter plots).
        
    - Using outlier detection methods (e.g., z-score, IQR, isolation forest).
        

You may:

- Remove them,
    
- Treat them separately,
    
- Use robust algorithms.
    

---

## ðŸ”¸ **5. How do we define the pair-wise similarity?**

This depends on the data type and algorithm:

- **Numerical data**: Euclidean, Manhattan, Cosine similarity.
    
- **Categorical data**: Hamming distance, Jaccard similarity.
    
- **Mixed data**: Gower distance.
    

The similarity measure must:

- Reflect the **true relationship** between points,
    
- Align with the **clustering methodâ€™s assumptions**.
    

ðŸ§  _Example_: Cosine similarity is better than Euclidean when **magnitude doesn't matter** (e.g., in text data).

---

## ðŸ”¸ **6. How many clusters are present in the data?**

This is a classic problem:

- **K-Means** requires you to predefine K.
    
- You can estimate it using:
    
    - **Elbow Method**
        
    - **Silhouette Score**
        
    - **Gap Statistic**
        
    - **Hierarchical Dendrograms**
        

ðŸ§  _Note_: There may not be a single correct number â€” the â€œbestâ€ K may depend on your use case.

---

## ðŸ”¸ **7. Which clustering method should be used?**

It depends on:

|Factor|Recommendation|
|---|---|
|Cluster shape|Use **K-Means** for spherical, **DBSCAN** for arbitrary shapes|
|Outliers present|Avoid K-Means, use **DBSCAN** or **HDBSCAN**|
|High-dimensional data|Consider **spectral clustering** or **PCA + clustering**|
|Hierarchical structure|Use **Agglomerative Hierarchical Clustering**|
|Mixed data types|Use **Gower distance** + clustering like **k-prototypes**|

There is **no one-size-fits-all**â€”algorithm choice depends on data structure and problem goal.

---

## ðŸ”¸ **8. Does the data have any clustering tendency?**

Before clustering, ask: **Does the data even have natural clusters?**

- Use **clustering tendency tests**:
    
    - **Hopkins statistic** (value close to 1 = good clustering tendency)
        
    - **Visualizations**: t-SNE, PCA scatter plots
        
- Random data may still form â€œclustersâ€ by accident.
    

Clustering tendency tells you whether **clustering is meaningful** at all.

---

## ðŸ”¸ **9. Are the discovered clusters and partition valid?**

Once you cluster, you must **evaluate** the results:

- **Internal Validation** (no labels needed):
    
    - Silhouette Score
        
    - Daviesâ€“Bouldin Index
        
    - Calinskiâ€“Harabasz Index
        
- **External Validation** (if labels available):
    
    - Adjusted Rand Index (ARI)
        
    - Normalized Mutual Information (NMI)
        

Also, assess:

- Cluster **cohesiveness** (within-cluster similarity),
    
- Cluster **separation** (between-cluster dissimilarity),
    
- **Interpretability**: Do the clusters make sense in the real world?
    

---

# âœ… *Summary*

| Question                   | Importance                                       |
| -------------------------- | ------------------------------------------------ |
| How to define a cluster?   | Sets the foundation for algorithm and evaluation |
| What features to use?      | Affects how similarity is perceived              |
| Should data be normalized? | Prevents scale bias in distance                  |
| Any outliers?              | May distort clusters                             |
| Pairwise similarity?       | Must suit data type and domain                   |
| How many clusters?         | Determines meaningful partitions                 |
| Which method?              | Depends on data shape, size, and goal            |
| Clustering tendency?       | Validates whether clustering is appropriate      |
| Are results valid?         | Confirms if output is trustworthy and useful     |
