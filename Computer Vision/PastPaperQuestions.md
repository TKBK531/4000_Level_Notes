# SamplePprQ1
## a) Assume you are assigned the task of devising a simple skin color detector for a variety of photographs of people.

#### i) List the steps you would follow with a justification for each step.

##### Step 1: Preprocessing the Image

**Justification:**  
Standardizes the input data to reduce variability due to lighting, image resolution, and color balance.

- **Convert image to a suitable color space** (e.g., from RGB to HSV, YCrCb, or CIE Lab).
    
    - **Why?** Skin color clusters more consistently in certain color spaces (like YCrCb or HSV), making it easier to detect than in RGB where lighting affects all channels.
        
- **Apply image smoothing or blurring** (e.g., Gaussian blur).
    
    - **Why?** Reduces noise and minor color variations that may affect detection.
        

---

##### Step 2: Define Skin Color Range

**Justification:**  
Skin tones fall within certain ranges in specific color spaces regardless of ethnicity.

- **Use empirical thresholds in chosen color space** to define skin color.
    
    - Example in **HSV**:
        - H: 0–50
        - S: 0.2–0.68
        - V: 0.35–1.0
            
    - Example in **YCrCb**:
        - Cr: 135–180
        - Cb: 85–135

---

##### Step 3: Create a Skin Mask

**Justification:**  
Binary mask isolates potential skin regions.

- **Apply thresholding** based on the defined skin color range.
    
- **Result:** A binary mask where pixels classified as skin are white (1), others are black (0).
    

---

##### Step 4: Post-processing

**Justification:**  
Cleans up the mask to remove noise and refine the detected areas.

- **Morphological operations** such as erosion and dilation (opening/closing) to remove small artifacts.
    
- **Connected component analysis** to identify large, contiguous skin regions.
    

---

##### Step 5: Apply the Mask

**Justification:**  
To extract or highlight detected skin regions in the original image.

- Use the binary mask to isolate and visualize skin areas from the original image for validation or further processing.
    

---

##### Step 6: Evaluation and Testing

**Justification:**  
To ensure the detector performs well across varied lighting conditions and skin tones.

- Test on a **diverse dataset** with varied ethnicities, lighting, and image qualities.
    
- Compute metrics like **precision, recall, F1-score**, and adjust thresholds accordingly.

#### ii) How sensitive is your algorithm to color balance (scene lighting)?

##### 1. Lighting Affects Pixel Values

- In **RGB**, lighting changes directly affect all three channels, making skin tones appear different under varying conditions (e.g., sunlight vs. indoor fluorescent lighting).
    
- Even in color spaces like **HSV** or **YCrCb**, lighting can influence the **brightness (V or Y)** and **saturation/chroma** components, shifting skin tones **outside the defined thresholds**.
    

---

##### 2. Shadow and Highlight Issues

- **Shadows** can make skin appear much darker, potentially pushing it out of the skin color range.
    
- **Overexposed areas** (e.g., from strong flash or backlighting) may wash out skin color, leading to false negatives.
    

---

##### 3. Camera White Balance

- If the camera applies automatic or incorrect white balance, skin tones may shift towards blue, green, or red hues, reducing accuracy.
    

---

##### 4. Ethnic Diversity and Subtle Tones

- Lighter or darker skin tones might be **more sensitive to lighting shifts**, making fixed thresholds too rigid.
    

---

##### Mitigation Strategies:

- **Use adaptive color ranges** (e.g., machine learning models that learn skin color under various lighting).
    
- **Normalize illumination** using techniques like histogram equalization or Retinex.
    
- **Combine color detection with texture, shape, or facial feature detection** to improve robustness.
    

---

##### Conclusion:

A simple threshold-based skin detector is **fairly sensitive to lighting and color balance**, and while using more stable color spaces helps, achieving robustness typically requires **additional techniques** such as adaptive thresholds, machine learning, or preprocessing to normalize lighting effects.


## b) Hybrid images are generated by superimposing two images at two different spatial scales

#### i) Given a set of sample images, explain the steps to obtain hybrid images

**Hybrid images** are created by **combining the low-frequency (blurred) content of one image** with the **high-frequency (detail) content of another image**. This causes the image to appear differently depending on viewing distance:

- **Close up** → You see the high-frequency image.
    
- **From far away** → You see the low-frequency image.

---
##### 1. Choose Two Compatible Images

- Select two images with similar structure, alignment, and perspective (e.g., two faces).
    
- **Justification:** Visual alignment is crucial to blending the images meaningfully.
    

---

##### 2. Align the Images

- Use image processing tools to **resize, rotate, and translate** the images to match key features (e.g., eyes, nose, mouth in portraits).
    
- Manual alignment or automatic methods (e.g., feature matching) can be used.
    

---

##### 3. Convert Images to Grayscale or Color (Optional)

- Choose whether to process in grayscale or retain color channels.
    
- If working in color, process each channel (R, G, B) separately.
    

---

##### 4. Extract Low-Frequency Components

- Apply a **Gaussian blur** to one of the images to keep only low frequencies (smooth structures).
    
    - Example:
$$
        \text{LowFreqImage} = \text{GaussianBlur}(\text{Image1}, \sigma)
$$
- **Justification:** Low-pass filtering removes details, retaining broad shape and luminance.
    

---

##### 5. Extract High-Frequency Components

- Subtract a **blurred version** of the second image from itself to get high-frequency details:
    
$$
\text{HighFreqImage} = \text{Image2} - \text{GaussianBlur}(\text{Image2}, \sigma)
$$
- **Justification:** High-pass filtering isolates edges and fine textures.
    

---

##### 6. Combine Low and High Frequency Images

- Add the two components together:
   
$$
\text{HybridImage} = \text{LowFreqImage} + \text{HighFreqImage}
$$

- Ensure the pixel values are within a valid range (e.g., [0, 255] for 8-bit images).
    
- **Optional:** Normalize or clip the resulting image.
    

---

##### 7. Visualize the Hybrid Image at Different Scales

- Show the image at different sizes (zoomed in and out).
    
- **Justification:** Demonstrates the dual perception effect.
    

---


>**Summary of Tools Used**
>
 **Gaussian Blur** (for both low and high frequency extraction)
 **Image subtraction**
 **Image alignment techniques**
 **Image addition and normalization**


#### ii) Suggest three (03) applications where hybrid images can be used

##### 1. Visual Illusions and Art

- **Purpose:** Create compelling optical illusions that change with viewing distance.
    
- **Use Case:** Interactive art installations, posters, or book covers that appear to shift images as you move.
    

---

##### 2. Data Hiding or Steganography

- **Purpose:** Embed information that is only visible under certain conditions (e.g., distance or resolution).
    
- **Use Case:** Hidden messages, watermarks, or dual-layer security visuals.
    

---

##### 3. Advertising and Marketing

- **Purpose:** Attract attention by using dynamic visuals that convey different messages or products depending on viewer proximity.
    
- **Use Case:** Billboards or ads that show one brand up close and another from afar.
    

---

##### 4. Education and Demonstration

- **Purpose:** Teach concepts of human visual perception, frequency domain processing, and cognitive science.
    
- **Use Case:** Visual perception courses, computer vision classes, or image processing tutorials.
    

---

##### 5. Dual-Message Signage

- **Purpose:** Convey two sets of information in a single image.
    
- **Use Case:** Public signs that show a clear message up close but reveal additional information (like branding or instructions) from a distance.
    

---

##### 6. Privacy and Camouflage

- **Purpose:** Show meaningful content only when viewed from certain distances.
    
- **Use Case:** Public-facing displays or content that changes based on viewer distance, reducing visibility of sensitive information from afar.
    

---

##### 7. Gaming and Entertainment

- **Purpose:** Include hidden content or puzzles in games that require changes in perspective to uncover.
    
- **Use Case:** Easter eggs, puzzle games, or interactive storytelling.


# SamplePprQ2
#### a) Discuss the main challenges associated with selecting a clustering algorithm

##### 1. No Universal Best Algorithm

- **Challenge:** No single clustering algorithm performs best across all datasets.
    
- **Reason:** Clustering is **unsupervised**, so there's no ground truth to validate against in most cases.
    

---

##### 2. Type and Shape of Clusters

- **Challenge:** Different algorithms assume different cluster shapes (e.g., spherical, arbitrary, convex).
    
    - **K-Means**: Assumes clusters are spherical and equally sized.
        
    - **DBSCAN**: Can find clusters of arbitrary shape.
        
- **Consequence:** Misalignment between algorithm assumptions and actual data can lead to poor results.
    

---

##### 3. Scalability and Performance

- **Challenge:** Some algorithms don't scale well with large datasets.
    
    - **Hierarchical clustering**: Often computationally expensive (O(n²)).
        
    - **K-Means and MiniBatch K-Means**: More scalable for large data.
        
- **Consequence:** Algorithm choice must consider dataset size and computational resources.
    

---

##### 4. Choice of Distance Metric

- **Challenge:** Many clustering algorithms depend on a distance metric (e.g., Euclidean, Manhattan, cosine).
    
- **Consequence:** An inappropriate distance measure can lead to incorrect cluster assignments, especially for high-dimensional or categorical data.
    

---

##### 5. Determining the Number of Clusters (k)

- **Challenge:** Algorithms like **K-Means** require the number of clusters to be specified beforehand.
    
- **Consequence:** Estimating the right number of clusters (e.g., using the Elbow method or Silhouette score) adds complexity.
    

---

##### 6. Sensitivity to Noise and Outliers

- **Challenge:** Some algorithms are highly sensitive to outliers.
    
    - **K-Means**: Strongly affected by outliers.
        
    - **DBSCAN**: More robust due to density-based approach.
        
- **Consequence:** Choice must consider data cleanliness and robustness needs.
    

---

##### 7. Interpretability of Results

- **Challenge:** Some clustering methods produce results that are harder to interpret.
    
    - **Hierarchical clustering**: Offers dendrograms but may be hard to interpret in high dimensions.
        
    - **Spectral clustering**: Relies on graph theory and may be abstract.
        
- **Consequence:** Interpretability is crucial in applications like customer segmentation or bioinformatics.
    

---

##### 8. Data Type Compatibility

- **Challenge:** Algorithms differ in their ability to handle different data types:
    
    - **K-Means**: Only suitable for numerical data.
        
    - **K-Modes or Gower distance**: Suitable for categorical or mixed data.
        
- **Consequence:** Must match algorithm to data type.
    

---

##### Summary Table of Challenges and Considerations

| **Challenge**                 | **Consideration**                                         |
| ----------------------------- | --------------------------------------------------------- |
| No universal best algorithm   | Choose based on data and goal                             |
| Cluster shape assumptions     | Match to expected distribution                            |
| Scalability                   | Consider dataset size and algorithm complexity            |
| Distance metric sensitivity   | Choose appropriate metric for data type                   |
| Estimating number of clusters | Use heuristics or model selection methods                 |
| Outlier sensitivity           | Use robust methods if data is noisy                       |
| Interpretability              | Favor simpler models if results must be explained         |
| Data type compatibility       | Ensure algorithm supports data format (e.g., categorical) |

#### b) Consider the data set given below and find 3 clusters on them using k-means clustering algorithm. Assume that the initial cluster centroids are defined by A, B and C. Provide a graphical representation of the clusters.
| ID  | Dimension 1 | Dimension 2 |
| :-: | :---------: | :---------: |
|  A  |      1      |      1      |
|  B  |      8      |      6      |
|  C  |     20      |      3      |
|  D  |     21      |      2      |
|  E  |     11      |      7      |
|  F  |      7      |      7      |
|  G  |      1      |      2      |
|  H  |      6      |      8      |

Initial centroids:
- Cluster 1: *A (1, 1)*
- Cluster 2: *B (8, 6)*
- Cluster 3: *C (20, 3)*

---

##### Step-by-step K-means Clustering

We will:

1. Assign points to the nearest centroid using Euclidean distance.
    
2. Compute new centroids.
    
3. Repeat until assignments don’t change (only 2–3 iterations are generally needed).
    

---

##### Step 1: Initial Assignment

Calculate distance from each point to A, B, and C:

Let’s denote clusters as:

- **Cluster 1 (C1)**: initialized by A
- **Cluster 2 (C2)**: initialized by B
- **Cluster 3 (C3)**: initialized by C

| ID  | Coordinates | dist to A (C1) | dist to B (C2) | dist to C (C3) | Assigned Cluster |
| :-: | :---------: | :------------: | :------------: | :------------: | :--------------: |
|  A  |    (1,1)    |       0        |      8.60      |     19.10      |        C1        |
|  B  |    (8,6)    |      8.60      |       0        |     12.73      |        C2        |
|  C  |   (20,3)    |     19.10      |     12.73      |       0        |        C3        |
|  D  |   (21,2)    |     20.62      |     14.87      |      1.41      |        C3        |
|  E  |   (11,7)    |     12.04      |      3.16      |      9.22      |        C2        |
|  F  |    (7,7)    |      8.49      |      1.41      |     13.04      |        C2        |
|  G  |    (1,2)    |      1.00      |      7.21      |     19.03      |        C1        |
|  H  |    (6,8)    |      8.49      |      2.83      |     14.76      |        C2        |

---

##### Cluster Assignments after Iteration 1

- **C1**: A, G
- **C2**: B, E, F, H
- **C3**: C, D

---

##### Step 2: Compute New Centroids

**C1 (A, G):**
- Mean: ((1+1)/2, (1+2)/2) = (1, 1.5)

**C2 (B, E, F, H):**
- Mean: ((8+11+7+6)/4, (6+7+7+8)/4) = (8, 7)

**C3 (C, D):**
- Mean: ((20+21)/2, (3+2)/2) = (20.5, 2.5)

---

##### Step 3: Reassign Points (Iteration 2)

Recalculate distances to new centroids and reassign.

|ID|Coordinates|dist to C1 (1,1.5)|dist to C2 (8,7)|dist to C3 (20.5,2.5)|Cluster|
|---|---|---|---|---|---|
|A|(1,1)|0.5|8.60|19.52|C1|
|B|(8,6)|7.93|1.00|13.45|C2|
|C|(20,3)|19.09|13.60|0.53|C3|
|D|(21,2)|20.12|14.42|0.53|C3|
|E|(11,7)|11.33|3.00|9.60|C2|
|F|(7,7)|8.05|1.00|13.13|C2|
|G|(1,2)|0.5|7.62|19.01|C1|
|H|(6,8)|8.20|2.24|14.87|C2|

**No change in cluster assignments** — convergence achieved.

---

##### Final Clusters

- **Cluster 1**: A, G
- **Cluster 2**: B, E, F, H
- **Cluster 3**: C, D

#### c) When applying the k-means clustering algorithm for a given set of data points, if different starting positions were used for the centers in separate instances, will the algorithm always converge to the same solution? Explain your answer.

**No, the k-means clustering algorithm will not always converge to the same solution if different starting positions for the cluster centers are used.**

##### Explanation:

K-means is an **iterative and heuristic** algorithm that:

1. **Starts with initial centroids** (chosen randomly or manually).
2. **Assigns points** to the nearest centroid.
3. **Recomputes centroids** of the resulting clusters.
4. Repeats steps 2 and 3 until assignments no longer change (convergence).

However, **k-means can converge to a local minimum**, not necessarily the **global minimum** of the total within-cluster variance (also called the "inertia").

##### Why Different Results Can Occur:

- The **initial centroids** heavily influence the outcome.
- Different initializations can lead the algorithm down **different optimization paths**, resulting in different final clusters.
- This is especially true in datasets with **overlapping** or **non-spherical** cluster structures.
    
##### Example:

Consider a dataset where two clusters are close together. If initial centroids are placed poorly (e.g. both near the same cluster), the algorithm may group distant points incorrectly and settle on a suboptimal clustering.

---

##### Solution to Mitigate This:

- Use **k-means++** initialization: It chooses starting centroids in a smarter way to spread them out and improve consistency.
    
- Run k-means multiple times with different random seeds and select the best result based on the lowest total variance.
    

---

##### Conclusion:

> **K-means is not guaranteed to produce the same results with different starting positions**, and multiple runs are often necessary to achieve a robust clustering solution.


#### d) If clusters are to be meaningful, they should be invariant to transformations natural to the problem. Suggest a method to obtain invariance to displacement and scale changes. Will this work for all the cases?

If clusters are to be **meaningful**, they should be **invariant to natural transformations** of the data — such as **displacement (translation)** and **scale (resizing)** — so that clustering reflects the _structure_ of the data, not its _position_ or _units_.

---

##### ✅ Suggested Method: Standardization (Z-score Normalization)

To achieve **invariance to displacement and scale**, you can apply the following preprocessing steps:

##### 1. Displacement Invariance: Centering

Subtract the **mean** of each feature so the data is centered at the origin:

$$
x'=x−μ
$$

##### 2. Scale Invariance: Scaling

Divide each feature by its **standard deviation**:

$$
x'' = \frac{x - \mu}{\sigma}
$$

This transformation ensures:

- All features have **zero mean** and **unit variance**.
    
- The clustering process is unaffected by differences in measurement units or absolute positions.
    

This method is commonly used before applying **distance-based algorithms** like **k-means**.

---

##### ❌ Will This Work for All Cases?

**Not always.** Here’s why:

|Case|Effectiveness|
|---|---|
|✔ Displacement (translation)|✅ Works well|
|✔ Scale (resizing)|✅ Works well|
|❌ Rotation|❌ Not addressed by normalization|
|❌ Non-linear structures (e.g., concentric rings)|❌ K-means won’t capture these well|
|❌ Features with different semantic importance|❌ Standardization may distort true weights|

---

##### 🧠 Conclusion:

> **Standardization** (centering and scaling) is a widely used and effective method to make clustering invariant to **displacement and scale**.  
> However, it **does not work for all cases**, particularly when the data requires **rotation invariance**, **non-linear separation**, or when **feature importance** must be preserved.



# SamplePprQ3
#### a) Explain how an approximation to the first derivative of an image can be obtained by convolving the image with the kernel [1 -1] for the image specified below [56 64 79 98 115 126 132 133]

To approximate the **first derivative** of a 1D image (or signal), we use a **convolution** operation with a kernel that highlights **changes between neighboring pixels**.

---
##### 🔧 Kernel Used:

$$
[1 −1]
$$

This kernel approximates the **forward difference**, i.e.,

$$
f'(x) \approx f(x) - f(x+1)
$$

It captures the **rate of change** (intensity difference) between adjacent pixels.

---

##### 📷 Given 1D Image:
$$
[56,64,79,98,115,126,132,133]
$$

---

##### 🧮 Convolution with [1 −1] Kernel:

We'll apply the kernel from **left to right** (ignoring padding), computing:
$$

\text{Result}[i] = \text{Image}[i] - \text{Image}[i+1]
$$

|i|Pixel Pair|Derivative (f[i] − f[i+1])|
|---|---|---|
|0|56 − 64|-8|
|1|64 − 79|-15|
|2|79 − 98|-19|
|3|98 − 115|-17|
|4|115 − 126|-11|
|5|126 − 132|-6|
|6|132 − 133|-1|

---

##### ✅ Output (First Derivative Approximation):
$$
[−8,−15,−19,−17,−11,−6,−1]
$$

(Note: output length is one less than the input.)

---

##### 🧠 Interpretation:

- Large negative values indicate **sharp increases** in intensity.
- Small values (near 0) indicate **slow changes** or flat regions.

This method helps in **edge detection**, **gradient computation**, and **feature extraction** in image processing.


#### b) Indicate where edges would be detected in the result of a) and state why.

##### 📍 Edge Locations:

- **Strong edges** are detected at:
    - **Index 2** (between 79 and 98) → |−19|
    - **Index 3** (between 98 and 115) → |−17|
    - **Index 1** (between 64 and 79) → |−15|

These correspond to **locations with the greatest change in pixel values**, hence likely **edges**.

#### c) When would detecting corners be more appropriate than detecting edges as an initial step in an application using computer vision?

##### ✅ 1. Identify Distinctive and Repeatable Features

- **Corners** are points where two edges meet, forming a high-variation region in **both directions (X and Y)**.
- Unlike edges (which only vary in one direction), corners are more **unique** and **locally stable**, making them ideal for matching across images.
    
**Use case:**
- **Feature matching**, **image stitching**, **object recognition**, and **SLAM (Simultaneous Localization and Mapping)**.
    

---

##### ✅ 2. Perform Image Matching or Tracking

- In **tracking algorithms** (e.g., Lucas-Kanade optical flow), corners are better because they are easier to **track over time** and under motion.

**Use case:**
- **Motion tracking**, **augmented reality**, **video stabilization**.

---

##### ✅ 3. Build Descriptors for Object Recognition

- Corner detectors (e.g., Harris, FAST, or used in SIFT/SURF) provide **keypoints** for computing robust descriptors, which can be used to recognize objects from different angles or under different lighting.

**Use case:**
- **3D reconstruction**, **pattern recognition**, **pose estimation**.

---

##### ✅ 4. Ensure Rotation and Scale Invariance

- Corner-based detectors often support better **invariance** to transformations like **rotation**, **scaling**, and **viewpoint change** compared to edge-based methods.

---

##### ❌ Edges Are Better When:

- You need the **shape** or **boundary** of objects (e.g., for segmentation or object contour detection).
- The application relies on **outlines**, not specific matching points.
---

##### 🧠 Conclusion:

> **Corner detection is more appropriate** when you need **robust, repeatable, and distinctive points** for tasks like **matching, tracking, localization, and recognition**, whereas **edge detection** is better for **object boundaries and segmentation**.


#### d) The Harris corner detection algorithm computes a 2 x 2 matrix at each pixel based on the first derivatives at that point and then computes the two eigenvalues of the matrix. Explain how you can use the two eigenvalues to label each pixel as a locally smoothed region, an edge point, or a corner point.

The **Harris corner detection** algorithm identifies **corners** by analyzing changes in image intensity in a **local neighborhood** using the **first derivatives**.

---

##### 🧮 Core Concept: Structure Tensor (Second-Moment Matrix)

At each pixel, Harris computes a **2×2 matrix**:
$$
M = \begin{bmatrix} I_x^2 & I_x I_y \\ I_x I_y & I_y^2 \\ \end{bmatrix}
$$
Where:
- $I_x​$ and $I_y$​ are the **image gradients** in the $x$ and $y$ directions.
- These values are **smoothed** over a window (e.g., using a Gaussian kernel).

---

##### 🔢 Eigenvalues of M:
Let:
- $\lambda_1$​, $\lambda_2$ be the **eigenvalues** of matrix $M$

They represent the **variation of intensity** in the two orthogonal directions around the pixel.

---

##### 🧠 Interpretation of Eigenvalues:

| $\lambda_1$, $\lambda_2$ | Meaning                               | Classification           |
| ------------------------ | ------------------------------------- | ------------------------ |
| Both small               | Little change in all directions       | **Flat region (smooth)** |
| One large, one small     | Change in one direction only          | **Edge**                 |
| Both large               | Significant change in both directions | **Corner**               |

---

##### 📌 Summary:

| Condition                                  | Pixel Type            |
| ------------------------------------------ | --------------------- |
| $\lambda_1 \approx 0, \lambda_2 \approx 0$ | Flat / uniform region |
| $\lambda_1 \gg \lambda_2$ or vice versa    | Edge                  |
| $\lambda_1 \approx \lambda_2 \gg 0$        | Corner                |

---

##### ✅ Why It Works:

- **Flat regions** have **no gradient** → both eigenvalues are near zero.
- **Edges** have **strong gradient in one direction** → one large, one small eigenvalue.
- **Corners** have **strong gradients in two directions** → both eigenvalues are large.

This makes Harris a robust method for detecting **corner-like features** that are stable and distinctive across images.

#### e) Explain the changes SIFT descriptor is robust to. Is it invariant to major view point changes as well?
