# SamplePprQ1
### a) Assume you are assigned the task of devising a simple skin color detector for a variety of photographs of people.

#### i) List the steps you would follow with a justification for each step.

##### Step 1: Preprocessing the Image

**Justification:**  
Standardizes the input data to reduce variability due to lighting, image resolution, and color balance.

- **Convert image to a suitable color space** (e.g., from RGB to HSV, YCrCb, or CIE Lab).
    
    - **Why?** Skin color clusters more consistently in certain color spaces (like YCrCb or HSV), making it easier to detect than in RGB where lighting affects all channels.
        
- **Apply image smoothing or blurring** (e.g., Gaussian blur).
    
    - **Why?** Reduces noise and minor color variations that may affect detection.
        

---

##### Step 2: Define Skin Color Range

**Justification:**  
Skin tones fall within certain ranges in specific color spaces regardless of ethnicity.

- **Use empirical thresholds in chosen color space** to define skin color.
    
    - Example in **HSV**:
        - H: 0–50
        - S: 0.2–0.68
        - V: 0.35–1.0
            
    - Example in **YCrCb**:
        - Cr: 135–180
        - Cb: 85–135

---

##### Step 3: Create a Skin Mask

**Justification:**  
Binary mask isolates potential skin regions.

- **Apply thresholding** based on the defined skin color range.
    
- **Result:** A binary mask where pixels classified as skin are white (1), others are black (0).
    

---

##### Step 4: Post-processing

**Justification:**  
Cleans up the mask to remove noise and refine the detected areas.

- **Morphological operations** such as erosion and dilation (opening/closing) to remove small artifacts.
    
- **Connected component analysis** to identify large, contiguous skin regions.
    

---

##### Step 5: Apply the Mask

**Justification:**  
To extract or highlight detected skin regions in the original image.

- Use the binary mask to isolate and visualize skin areas from the original image for validation or further processing.
    

---

##### Step 6: Evaluation and Testing

**Justification:**  
To ensure the detector performs well across varied lighting conditions and skin tones.

- Test on a **diverse dataset** with varied ethnicities, lighting, and image qualities.
    
- Compute metrics like **precision, recall, F1-score**, and adjust thresholds accordingly.

#### ii) How sensitive is your algorithm to color balance (scene lighting)?

##### 1. Lighting Affects Pixel Values

- In **RGB**, lighting changes directly affect all three channels, making skin tones appear different under varying conditions (e.g., sunlight vs. indoor fluorescent lighting).
    
- Even in color spaces like **HSV** or **YCrCb**, lighting can influence the **brightness (V or Y)** and **saturation/chroma** components, shifting skin tones **outside the defined thresholds**.
    

---

##### 2. Shadow and Highlight Issues

- **Shadows** can make skin appear much darker, potentially pushing it out of the skin color range.
    
- **Overexposed areas** (e.g., from strong flash or backlighting) may wash out skin color, leading to false negatives.
    

---

##### 3. Camera White Balance

- If the camera applies automatic or incorrect white balance, skin tones may shift towards blue, green, or red hues, reducing accuracy.
    

---

##### 4. Ethnic Diversity and Subtle Tones

- Lighter or darker skin tones might be **more sensitive to lighting shifts**, making fixed thresholds too rigid.
    

---

##### Mitigation Strategies:

- **Use adaptive color ranges** (e.g., machine learning models that learn skin color under various lighting).
    
- **Normalize illumination** using techniques like histogram equalization or Retinex.
    
- **Combine color detection with texture, shape, or facial feature detection** to improve robustness.
    

---

##### Conclusion:

A simple threshold-based skin detector is **fairly sensitive to lighting and color balance**, and while using more stable color spaces helps, achieving robustness typically requires **additional techniques** such as adaptive thresholds, machine learning, or preprocessing to normalize lighting effects.


### b) Hybrid images are generated by superimposing two images at two different spatial scales

#### i) Given a set of sample images, explain the steps to obtain hybrid images

**Hybrid images** are created by **combining the low-frequency (blurred) content of one image** with the **high-frequency (detail) content of another image**. This causes the image to appear differently depending on viewing distance:

- **Close up** → You see the high-frequency image.
    
- **From far away** → You see the low-frequency image.

---
##### 1. Choose Two Compatible Images

- Select two images with similar structure, alignment, and perspective (e.g., two faces).
    
- **Justification:** Visual alignment is crucial to blending the images meaningfully.
    

---

##### 2. Align the Images

- Use image processing tools to **resize, rotate, and translate** the images to match key features (e.g., eyes, nose, mouth in portraits).
    
- Manual alignment or automatic methods (e.g., feature matching) can be used.
    

---

##### 3. Convert Images to Grayscale or Color (Optional)

- Choose whether to process in grayscale or retain color channels.
    
- If working in color, process each channel (R, G, B) separately.
    

---

##### 4. Extract Low-Frequency Components

- Apply a **Gaussian blur** to one of the images to keep only low frequencies (smooth structures).
    
    - Example:
$$
        \text{LowFreqImage} = \text{GaussianBlur}(\text{Image1}, \sigma)
$$
- **Justification:** Low-pass filtering removes details, retaining broad shape and luminance.
    

---

##### 5. Extract High-Frequency Components

- Subtract a **blurred version** of the second image from itself to get high-frequency details:
    
$$
\text{HighFreqImage} = \text{Image2} - \text{GaussianBlur}(\text{Image2}, \sigma)
$$
- **Justification:** High-pass filtering isolates edges and fine textures.
    

---

##### 6. Combine Low and High Frequency Images

- Add the two components together:
   
$$
\text{HybridImage} = \text{LowFreqImage} + \text{HighFreqImage}
$$

- Ensure the pixel values are within a valid range (e.g., [0, 255] for 8-bit images).
    
- **Optional:** Normalize or clip the resulting image.
    

---

##### 7. Visualize the Hybrid Image at Different Scales

- Show the image at different sizes (zoomed in and out).
    
- **Justification:** Demonstrates the dual perception effect.
    

---


>**Summary of Tools Used**
>
 **Gaussian Blur** (for both low and high frequency extraction)
 **Image subtraction**
 **Image alignment techniques**
 **Image addition and normalization**


#### ii) Suggest three (03) applications where hybrid images can be used

##### 1. Visual Illusions and Art

- **Purpose:** Create compelling optical illusions that change with viewing distance.
    
- **Use Case:** Interactive art installations, posters, or book covers that appear to shift images as you move.
    

---

##### 2. Data Hiding or Steganography

- **Purpose:** Embed information that is only visible under certain conditions (e.g., distance or resolution).
    
- **Use Case:** Hidden messages, watermarks, or dual-layer security visuals.
    

---

##### 3. Advertising and Marketing

- **Purpose:** Attract attention by using dynamic visuals that convey different messages or products depending on viewer proximity.
    
- **Use Case:** Billboards or ads that show one brand up close and another from afar.
    

---

##### 4. Education and Demonstration

- **Purpose:** Teach concepts of human visual perception, frequency domain processing, and cognitive science.
    
- **Use Case:** Visual perception courses, computer vision classes, or image processing tutorials.
    

---

##### 5. Dual-Message Signage

- **Purpose:** Convey two sets of information in a single image.
    
- **Use Case:** Public signs that show a clear message up close but reveal additional information (like branding or instructions) from a distance.
    

---

##### 6. Privacy and Camouflage

- **Purpose:** Show meaningful content only when viewed from certain distances.
    
- **Use Case:** Public-facing displays or content that changes based on viewer distance, reducing visibility of sensitive information from afar.
    

---

##### 7. Gaming and Entertainment

- **Purpose:** Include hidden content or puzzles in games that require changes in perspective to uncover.
    
- **Use Case:** Easter eggs, puzzle games, or interactive storytelling.


# SamplePprQ2
#### a) Discuss the main challenges associated with selecting a clustering algorithm

##### 1. No Universal Best Algorithm

- **Challenge:** No single clustering algorithm performs best across all datasets.
    
- **Reason:** Clustering is **unsupervised**, so there's no ground truth to validate against in most cases.
    

---

##### 2. Type and Shape of Clusters

- **Challenge:** Different algorithms assume different cluster shapes (e.g., spherical, arbitrary, convex).
    
    - **K-Means**: Assumes clusters are spherical and equally sized.
        
    - **DBSCAN**: Can find clusters of arbitrary shape.
        
- **Consequence:** Misalignment between algorithm assumptions and actual data can lead to poor results.
    

---

##### 3. Scalability and Performance

- **Challenge:** Some algorithms don't scale well with large datasets.
    
    - **Hierarchical clustering**: Often computationally expensive (O(n²)).
        
    - **K-Means and MiniBatch K-Means**: More scalable for large data.
        
- **Consequence:** Algorithm choice must consider dataset size and computational resources.
    

---

##### 4. Choice of Distance Metric

- **Challenge:** Many clustering algorithms depend on a distance metric (e.g., Euclidean, Manhattan, cosine).
    
- **Consequence:** An inappropriate distance measure can lead to incorrect cluster assignments, especially for high-dimensional or categorical data.
    

---

##### 5. Determining the Number of Clusters (k)

- **Challenge:** Algorithms like **K-Means** require the number of clusters to be specified beforehand.
    
- **Consequence:** Estimating the right number of clusters (e.g., using the Elbow method or Silhouette score) adds complexity.
    

---

##### 6. Sensitivity to Noise and Outliers

- **Challenge:** Some algorithms are highly sensitive to outliers.
    
    - **K-Means**: Strongly affected by outliers.
        
    - **DBSCAN**: More robust due to density-based approach.
        
- **Consequence:** Choice must consider data cleanliness and robustness needs.
    

---

##### 7. Interpretability of Results

- **Challenge:** Some clustering methods produce results that are harder to interpret.
    
    - **Hierarchical clustering**: Offers dendrograms but may be hard to interpret in high dimensions.
        
    - **Spectral clustering**: Relies on graph theory and may be abstract.
        
- **Consequence:** Interpretability is crucial in applications like customer segmentation or bioinformatics.
    

---

##### 8. Data Type Compatibility

- **Challenge:** Algorithms differ in their ability to handle different data types:
    
    - **K-Means**: Only suitable for numerical data.
        
    - **K-Modes or Gower distance**: Suitable for categorical or mixed data.
        
- **Consequence:** Must match algorithm to data type.
    

---

##### Summary Table of Challenges and Considerations

| **Challenge**                 | **Consideration**                                         |
| ----------------------------- | --------------------------------------------------------- |
| No universal best algorithm   | Choose based on data and goal                             |
| Cluster shape assumptions     | Match to expected distribution                            |
| Scalability                   | Consider dataset size and algorithm complexity            |
| Distance metric sensitivity   | Choose appropriate metric for data type                   |
| Estimating number of clusters | Use heuristics or model selection methods                 |
| Outlier sensitivity           | Use robust methods if data is noisy                       |
| Interpretability              | Favor simpler models if results must be explained         |
| Data type compatibility       | Ensure algorithm supports data format (e.g., categorical) |

#### b) Consider the data set given below and find 3 clusters on them using k-means clustering algorithm. Assume that the initial cluster centroids are defined by A, B and C. Provide a graphical representation of the clusters.
| ID  | Dimension 1 | Dimension 2 |
| :-: | :---------: | :---------: |
|  A  |      1      |      1      |
|  B  |      8      |      6      |
|  C  |     20      |      3      |
|  D  |     21      |      2      |
|  E  |     11      |      7      |
|  F  |      7      |      7      |
|  G  |      1      |      2      |
|  H  |      6      |      8      |

Initial centroids:
- Cluster 1: *A (1, 1)*
- Cluster 2: *B (8, 6)*
- Cluster 3: *C (20, 3)*

---

##### Step-by-step K-means Clustering

We will:

1. Assign points to the nearest centroid using Euclidean distance.
    
2. Compute new centroids.
    
3. Repeat until assignments don’t change (only 2–3 iterations are generally needed).
    

---

##### Step 1: Initial Assignment

Calculate distance from each point to A, B, and C:

Let’s denote clusters as:

- **Cluster 1 (C1)**: initialized by A
- **Cluster 2 (C2)**: initialized by B
- **Cluster 3 (C3)**: initialized by C

| ID  | Coordinates | dist to A (C1) | dist to B (C2) | dist to C (C3) | Assigned Cluster |
| :-: | :---------: | :------------: | :------------: | :------------: | :--------------: |
|  A  |    (1,1)    |       0        |      8.60      |     19.10      |        C1        |
|  B  |    (8,6)    |      8.60      |       0        |     12.73      |        C2        |
|  C  |   (20,3)    |     19.10      |     12.73      |       0        |        C3        |
|  D  |   (21,2)    |     20.62      |     14.87      |      1.41      |        C3        |
|  E  |   (11,7)    |     12.04      |      3.16      |      9.22      |        C2        |
|  F  |    (7,7)    |      8.49      |      1.41      |     13.04      |        C2        |
|  G  |    (1,2)    |      1.00      |      7.21      |     19.03      |        C1        |
|  H  |    (6,8)    |      8.49      |      2.83      |     14.76      |        C2        |

---

##### Cluster Assignments after Iteration 1

- **C1**: A, G
- **C2**: B, E, F, H
- **C3**: C, D

---

##### Step 2: Compute New Centroids

**C1 (A, G):**
- Mean: ((1+1)/2, (1+2)/2) = (1, 1.5)

**C2 (B, E, F, H):**
- Mean: ((8+11+7+6)/4, (6+7+7+8)/4) = (8, 7)

**C3 (C, D):**
- Mean: ((20+21)/2, (3+2)/2) = (20.5, 2.5)

---

##### Step 3: Reassign Points (Iteration 2)

Recalculate distances to new centroids and reassign.

|ID|Coordinates|dist to C1 (1,1.5)|dist to C2 (8,7)|dist to C3 (20.5,2.5)|Cluster|
|---|---|---|---|---|---|
|A|(1,1)|0.5|8.60|19.52|C1|
|B|(8,6)|7.93|1.00|13.45|C2|
|C|(20,3)|19.09|13.60|0.53|C3|
|D|(21,2)|20.12|14.42|0.53|C3|
|E|(11,7)|11.33|3.00|9.60|C2|
|F|(7,7)|8.05|1.00|13.13|C2|
|G|(1,2)|0.5|7.62|19.01|C1|
|H|(6,8)|8.20|2.24|14.87|C2|

**No change in cluster assignments** — convergence achieved.

---

##### Final Clusters

- **Cluster 1**: A, G
- **Cluster 2**: B, E, F, H
- **Cluster 3**: C, D

#### c) When applying the k-means clustering algorithm for a given set of data points, if different starting positions were used for the centers in separate instances, will the algorithm always converge to the same solution? Explain your answer.

**No, the k-means clustering algorithm will not always converge to the same solution if different starting positions for the cluster centers are used.**

##### Explanation:

K-means is an **iterative and heuristic** algorithm that:

1. **Starts with initial centroids** (chosen randomly or manually).
2. **Assigns points** to the nearest centroid.
3. **Recomputes centroids** of the resulting clusters.
4. Repeats steps 2 and 3 until assignments no longer change (convergence).

However, **k-means can converge to a local minimum**, not necessarily the **global minimum** of the total within-cluster variance (also called the "inertia").

##### Why Different Results Can Occur:

- The **initial centroids** heavily influence the outcome.
- Different initializations can lead the algorithm down **different optimization paths**, resulting in different final clusters.
- This is especially true in datasets with **overlapping** or **non-spherical** cluster structures.
    
##### Example:

Consider a dataset where two clusters are close together. If initial centroids are placed poorly (e.g. both near the same cluster), the algorithm may group distant points incorrectly and settle on a suboptimal clustering.

---

##### Solution to Mitigate This:

- Use **k-means++** initialization: It chooses starting centroids in a smarter way to spread them out and improve consistency.
    
- Run k-means multiple times with different random seeds and select the best result based on the lowest total variance.
    

---

##### Conclusion:

> **K-means is not guaranteed to produce the same results with different starting positions**, and multiple runs are often necessary to achieve a robust clustering solution.


#### d) If clusters are to be meaningful, they should be invariant to transformations natural to the problem. Suggest a method to obtain invariance to displacement and scale changes. Will this work for all the cases?

If clusters are to be **meaningful**, they should be **invariant to natural transformations** of the data — such as **displacement (translation)** and **scale (resizing)** — so that clustering reflects the _structure_ of the data, not its _position_ or _units_.

---

##### ✅ Suggested Method: Standardization (Z-score Normalization)

To achieve **invariance to displacement and scale**, you can apply the following preprocessing steps:

##### 1. Displacement Invariance: Centering

Subtract the **mean** of each feature so the data is centered at the origin:

$$
x'=x−μ
$$

##### 2. Scale Invariance: Scaling

Divide each feature by its **standard deviation**:

$$
x'' = \frac{x - \mu}{\sigma}
$$

This transformation ensures:

- All features have **zero mean** and **unit variance**.
    
- The clustering process is unaffected by differences in measurement units or absolute positions.
    

This method is commonly used before applying **distance-based algorithms** like **k-means**.

---

##### ❌ Will This Work for All Cases?

**Not always.** Here’s why:

|Case|Effectiveness|
|---|---|
|✔ Displacement (translation)|✅ Works well|
|✔ Scale (resizing)|✅ Works well|
|❌ Rotation|❌ Not addressed by normalization|
|❌ Non-linear structures (e.g., concentric rings)|❌ K-means won’t capture these well|
|❌ Features with different semantic importance|❌ Standardization may distort true weights|

---

##### 🧠 Conclusion:

> **Standardization** (centering and scaling) is a widely used and effective method to make clustering invariant to **displacement and scale**.  
> However, it **does not work for all cases**, particularly when the data requires **rotation invariance**, **non-linear separation**, or when **feature importance** must be preserved.



# SamplePprQ3 / 2019_2020Q3
#### a) Explain how an approximation to the first derivative of an image can be obtained by convolving the image with the kernel [1 -1] for the image specified below [56 64 79 98 115 126 132 133]

To approximate the **first derivative** of a 1D image (or signal), we use a **convolution** operation with a kernel that highlights **changes between neighboring pixels**.

---
##### 🔧 Kernel Used:

$$
[1 −1]
$$

This kernel approximates the **forward difference**, i.e.,

$$
f'(x) \approx f(x) - f(x+1)
$$

It captures the **rate of change** (intensity difference) between adjacent pixels.

---

##### 📷 Given 1D Image:
$$
[56,64,79,98,115,126,132,133]
$$

---

##### 🧮 Convolution with [1 −1] Kernel:

We'll apply the kernel from **left to right** (ignoring padding), computing:
$$

\text{Result}[i] = \text{Image}[i] - \text{Image}[i+1]
$$

|i|Pixel Pair|Derivative (f[i] − f[i+1])|
|---|---|---|
|0|56 − 64|-8|
|1|64 − 79|-15|
|2|79 − 98|-19|
|3|98 − 115|-17|
|4|115 − 126|-11|
|5|126 − 132|-6|
|6|132 − 133|-1|

---

##### ✅ Output (First Derivative Approximation):
$$
[−8,−15,−19,−17,−11,−6,−1]
$$

(Note: output length is one less than the input.)

---

##### 🧠 Interpretation:

- Large negative values indicate **sharp increases** in intensity.
- Small values (near 0) indicate **slow changes** or flat regions.

This method helps in **edge detection**, **gradient computation**, and **feature extraction** in image processing.


#### b) Indicate where edges would be detected in the result of a) and state why.

##### 📍 Edge Locations:

- **Strong edges** are detected at:
    - **Index 2** (between 79 and 98) → |−19|
    - **Index 3** (between 98 and 115) → |−17|
    - **Index 1** (between 64 and 79) → |−15|

These correspond to **locations with the greatest change in pixel values**, hence likely **edges**.

#### c) When would detecting corners be more appropriate than detecting edges as an initial step in an application using computer vision?

##### ✅ 1. Identify Distinctive and Repeatable Features

- **Corners** are points where two edges meet, forming a high-variation region in **both directions (X and Y)**.
- Unlike edges (which only vary in one direction), corners are more **unique** and **locally stable**, making them ideal for matching across images.
    
**Use case:**
- **Feature matching**, **image stitching**, **object recognition**, and **SLAM (Simultaneous Localization and Mapping)**.
    

---

##### ✅ 2. Perform Image Matching or Tracking

- In **tracking algorithms** (e.g., Lucas-Kanade optical flow), corners are better because they are easier to **track over time** and under motion.

**Use case:**
- **Motion tracking**, **augmented reality**, **video stabilization**.

---

##### ✅ 3. Build Descriptors for Object Recognition

- Corner detectors (e.g., Harris, FAST, or used in SIFT/SURF) provide **keypoints** for computing robust descriptors, which can be used to recognize objects from different angles or under different lighting.

**Use case:**
- **3D reconstruction**, **pattern recognition**, **pose estimation**.

---

##### ✅ 4. Ensure Rotation and Scale Invariance

- Corner-based detectors often support better **invariance** to transformations like **rotation**, **scaling**, and **viewpoint change** compared to edge-based methods.

---

##### ❌ Edges Are Better When:

- You need the **shape** or **boundary** of objects (e.g., for segmentation or object contour detection).
- The application relies on **outlines**, not specific matching points.
---

##### 🧠 Conclusion:

> **Corner detection is more appropriate** when you need **robust, repeatable, and distinctive points** for tasks like **matching, tracking, localization, and recognition**, whereas **edge detection** is better for **object boundaries and segmentation**.


#### d) The Harris corner detection algorithm computes a 2 x 2 matrix at each pixel based on the first derivatives at that point and then computes the two eigenvalues of the matrix. Explain how you can use the two eigenvalues to label each pixel as a locally smoothed region, an edge point, or a corner point.

The **Harris corner detection** algorithm identifies **corners** by analyzing changes in image intensity in a **local neighborhood** using the **first derivatives**.

---

##### 🧮 Core Concept: Structure Tensor (Second-Moment Matrix)

At each pixel, Harris computes a **2×2 matrix**:
$$
M = \begin{bmatrix} I_x^2 & I_x I_y \\ I_x I_y & I_y^2 \\ \end{bmatrix}
$$
Where:
- $I_x​$ and $I_y$​ are the **image gradients** in the $x$ and $y$ directions.
- These values are **smoothed** over a window (e.g., using a Gaussian kernel).

---

##### 🔢 Eigenvalues of M:
Let:
- $\lambda_1$​, $\lambda_2$ be the **eigenvalues** of matrix $M$

They represent the **variation of intensity** in the two orthogonal directions around the pixel.

---

##### 🧠 Interpretation of Eigenvalues:

| $\lambda_1$, $\lambda_2$ | Meaning                               | Classification           |
| ------------------------ | ------------------------------------- | ------------------------ |
| Both small               | Little change in all directions       | **Flat region (smooth)** |
| One large, one small     | Change in one direction only          | **Edge**                 |
| Both large               | Significant change in both directions | **Corner**               |

---

##### 📌 Summary:

| Condition                                  | Pixel Type            |
| ------------------------------------------ | --------------------- |
| $\lambda_1 \approx 0, \lambda_2 \approx 0$ | Flat / uniform region |
| $\lambda_1 \gg \lambda_2$ or vice versa    | Edge                  |
| $\lambda_1 \approx \lambda_2 \gg 0$        | Corner                |

---

##### ✅ Why It Works:

- **Flat regions** have **no gradient** → both eigenvalues are near zero.
- **Edges** have **strong gradient in one direction** → one large, one small eigenvalue.
- **Corners** have **strong gradients in two directions** → both eigenvalues are large.

This makes Harris a robust method for detecting **corner-like features** that are stable and distinctive across images.

#### e) Explain the changes SIFT descriptor is robust to. Is it invariant to major view point changes as well?

##### 1. Scale Changes ✅
- **Invariant** due to its **scale-space extrema detection** using Difference-of-Gaussians (DoG).
- Keypoints are detected at multiple scales and normalized to a common scale.

##### 2. Rotation Changes ✅
- **Invariant** because it assigns a **dominant orientation** to each keypoint.
- The descriptor is then rotated relative to this orientation.

##### 3. Illumination Changes ✅
- **Robust** (not fully invariant) due to:
    - Gradient-based descriptors (magnitude and orientation of edges are used).
    - Gradients are more stable under lighting changes than raw intensities.

##### 4. Affine Distortion (Small Viewpoint Changes) ⚠️
- **Partially invariant** to **small affine transformations**, such as those caused by **minor changes in viewpoint**.
- This works because local image patches remain approximately planar, and gradient patterns are somewhat preserved.

##### 5. Translation and Noise ✅
- **Robust** to small shifts and image noise due to:
    - Gaussian smoothing
    - Localized descriptor computation


---

##### 🔴 Major Viewpoint Changes (Large Affine or Perspective Transforms)
- SIFT is **not fully invariant** to large changes in viewpoint.
- When the appearance of a keypoint changes significantly due to **3D rotation** or **perspective warping**, SIFT may fail.

> For example, rotating an object from the front to a side view might distort local gradients beyond recognition.

---

##### 🔁 Alternatives for Better Viewpoint Invariance:
If significant viewpoint changes are expected, consider:
- **Affine-SIFT (ASIFT)** – extends SIFT by simulating multiple viewpoints.
- **MSER**, **Harris-Affine**, or **Hessian-Affine** – designed for affine invariance.
- **Deep learning-based descriptors** (e.g., SuperPoint, D2-Net) – can learn invariance from data.

---
##### 🧠 Summary:

|Transformation|SIFT Robustness/Invariance|
|---|---|
|Scale|✅ Invariant|
|Rotation|✅ Invariant|
|Illumination|✅ Robust|
|Minor Viewpoint Changes|⚠️ Partially invariant|
|Major Viewpoint Changes|❌ Not invariant|
|Noise, Translation|✅ Robust|

> **Conclusion:** SIFT is highly effective for many scenarios but **not fully invariant to large viewpoint changes**. Use advanced methods or multiple views for better 3D or perspective handling.

# SamplePprQ4

#### a) What happens when there is no clear Hyperplane in SVM?

##### ✅ 1. Data Is Not Linearly Separable
- The most common case where there is **no clear hyperplane** is when the data is **not linearly separable** in its original feature space.
    **Example:** You cannot draw a straight line to separate classes shaped like two concentric circles.
    
---
##### a) Use a Soft Margin SVM (C-SVM)
- Allows some misclassifications by introducing **slack variables**.
- The **cost parameter CCC** controls the trade-off between maximizing the margin and minimizing classification error.
    ➤ This allows SVM to handle **overlapping** data by **relaxing** the requirement for perfect separation.
    
##### b) Use the Kernel Trick
- Maps data into a **higher-dimensional space** where a hyperplane **can** separate them.
- Common kernels:
    - **RBF (Gaussian) kernel**
    - **Polynomial kernel**
    - **Sigmoid kernel**

> This enables SVM to find **nonlinear decision boundaries**.

---

##### ❌ What Happens If You Don’t Use Soft Margin or Kernel?
- The optimization problem becomes **infeasible**.
- The algorithm **fails to converge** to a solution, because it can't satisfy the constraint of separating all data points correctly.
    
---

##### 🧠 Summary:

> **When there is no clear hyperplane** in SVM (i.e., the data is not linearly separable), the SVM must:
> - **Relax the separation constraints** using a **soft margin**, or
> - **Transform the data** using a **kernel function** to enable separation in a higher-dimensional space.

Without these strategies, **SVM cannot find a valid solution**.

#### The picture below shows some original data points. In 1-dimensional, this data is not linearly separable. Explain how, after applying the transformation ϕ(x) = x² and adding this second dimension to our feature space, the classes become linearly separable.

![[Pasted image 20250522085825.png]]

##### Left Plot (Original 1D Space):
- We have two classes:
    - **Black squares** (class 1)
    - **Gray circles** (class 2)

- All points lie on the **x-axis (1D)**, i.e., in the form of (x1,0)(x_1, 0)(x1​,0).
- Visibly, **no straight line** (in 1D) can separate the two classes – the gray points are **sandwiched** between black points.

> Hence, the data is **not linearly separable** in 1D.

---

##### Transformation:
We apply the **nonlinear mapping**:
$$
\phi(x_1) = x_1^2
$$
This means we add a second dimension:

$$
x_2 = \phi(x_1) = x_1^2
$$
So, every data point becomes:

$$
(x_1, x_2) = (x_1, x_1^2)
$$

---

##### Right Plot (Transformed 2D Space):

- Now, data is in **2D**: $(x_1, x_1^2)$
- The gray circles (which were near the origin) now cluster **low on the parabola**.
- The black squares (which were farther left/right) are mapped **higher up on the parabola**.

Now, you can draw a **horizontal line** (e.g., $x_2 = 6$) that **clearly separates** the two classes.

---

##### 🧠 Why This Works:

- The nonlinear transformation **"lifts"** the outer (black) points up more than the inner (gray) ones.
- In $2D$, this transforms a **nonlinear decision boundary** in $1D$ into a **linear one**.
- This is the key idea behind **kernel methods**: transform data into a higher-dimensional space where it becomes linearly separable.
    

---

##### ✅ Final Answer:

In the original 1D space (left plot), the two classes overlap and cannot be linearly separated. By applying the transformation $\phi(x) = x^2$, we project the data into a 2D space where each point becomes $(x_1, x_1^2)$. In this new feature space (right plot), the gray class is mapped close to the origin while the black class is mapped further away due to larger $x_1^2$​ values. This allows us to draw a linear decision boundary (horizontal line) to separate the two classes, making the data **linearly separable in the transformed space**.

#### b) What is this approach called in SVM?

Lifting’ of the data points represents the mapping of data into a higher dimension. This is known as kernelling.

#### c) Can a similar approach be used for the XOR Problem? Explain

##### 🔍 Why XOR Is a Problem for Linear Models:

The **XOR problem** is **not linearly separable** in its original 2D input space. The four input points for XOR are:

| $x_1$ | $x_2$​ | $Output$ |
| :---: | :----: | :------: |
|   0   |   0    |    0     |
|   0   |   1    |    1     |
|   1   |   0    |    1     |
|   1   |   1    |    0     |
![[Pasted image 20250522090842.png]]
**No straight line** can separate the outputs 1 from 0 — this makes XOR a classic example of a problem **linear classifiers cannot solve**.

---

##### 🔁 Using a Feature Transformation:

To solve XOR, we can **transform** the input space into a higher dimension where it becomes **linearly separable**. One common transformation is:

$\phi(x_1, x_2) = (x_1, x_2, x_1 \cdot x_2)$

Now our transformed dataset becomes:

| $x_1$​ | $x_2$​ | $x_1 \cdot x_2​$ | $Output$ |
| ------ | ------ | ---------------- | -------- |
| 0      | 0      | 0                | 0        |
| 0      | 1      | 0                | 1        |
| 1      | 0      | 0                | 1        |
| 1      | 1      | 1                | 0        |

In this **3D space**, a **linear decision boundary** can separate the classes. This is essentially the same idea as in the SVM kernel trick.

---

##### 📌 Final answer 

> **Yes**, a similar approach can be used for the XOR problem. By applying a nonlinear transformation (e.g., $\phi(x_1, x_2) = x_1 \cdot x_2$, we can map the XOR data into a higher-dimensional space where it becomes **linearly separable**. This is exactly what the **kernel trick** in SVMs achieves—solving nonlinear problems like XOR using **linear methods in transformed spaces**.


#### d) When your data is imbalanced, is SVM a good approach?

No. an SVM model developed with an imbalanced dataset can be skewed toward the minority class and this skewness can degrade the performance of that model with respect to the minority class.

##### ⚠️ Why SVM Struggles with Imbalanced Data:

- SVM tries to **maximize the margin** between the two classes.
- It selects **support vectors** that are closest to the decision boundary.
- In **imbalanced data**, the **minority class** may have **too few support vectors**, leading the SVM to:
    - Bias the decision boundary toward the **majority class**.
    - Misclassify many instances of the **minority class**.

---

##### 🔧 How to Make SVM Work with Imbalanced Data:

If you still want to use SVM on imbalanced data, here are some techniques to improve its performance:
1. **Use class weighting**:
    - Give more penalty to misclassifying the **minority class**.

2. **Resampling techniques**:    
    - **Oversample** the minority class (e.g., SMOTE).
    - **Undersample** the majority class.
    
3. **Change the decision threshold**:
    - Adjust the threshold for decision-making to favor the minority class.

4. **Use one-class SVM (for anomaly detection)**:    
    - If the minority class is very rare, model it as an anomaly.

---

##### ✅ Conclusion:

> **Standard SVM is not ideal for imbalanced data**, as it tends to favor the majority class. However, with techniques like **class weighting, resampling, or threshold adjustment**, SVM can be adapted to handle imbalanced datasets more effectively.


# SamplePprQ5

#### For each part, design a system that would achieve the expected outcomes using computer vision algorithms and briefly explain the components of your system. Use flow diagrams to indicate the system components. Briefly outline an algorithm for each system. State any assumptions you make.

#### a) A driver assistant system equipped with a method to monitor how alert a car driver is and sets an alarm when he/she is about to fall asleep

##### Objective:
Detect if a driver is drowsy or about to fall asleep, and trigger an alarm.
##### Assumptions:
- A camera is installed facing the driver's face.
- System operates in real-time, under varying light conditions.
- The vehicle has audio output for alarms.

---

##### System Components:

**Flow Diagram (Text Representation):**

```
[Camera Feed]
	↓ 
[Face & Eye Detection]
	↓ 
[Eye State Classification (Open/Closed)]
	↓ 
[Drowsiness Detection Logic]
	↓ 
[Alarm Trigger (if necessary)]
```

---

##### Components Explained:

- **Camera Feed**: Captures live video of the driver’s face.   
- **Face & Eye Detection**: Detects facial landmarks and locates eyes (using Haar cascades or Dlib).
- **Eye State Classification**: Classifies eyes as open or closed using a CNN or Eye Aspect Ratio (EAR).
- **Drowsiness Detection Logic**: Determines if the driver is likely drowsy based on metrics (e.g., eyes closed > 2 seconds).
- **Alarm Trigger**: Activates an alarm to alert the driver if drowsiness is detected.

---

##### Algorithm Outline:

1. Start video capture.
2. Detect face and eyes using facial landmark detection.
3. Calculate Eye Aspect Ratio (EAR).
4. If EAR < threshold for N consecutive frames:
    - Assume eyes are closed for too long.
    - Trigger drowsiness warning.
5. Else:
    - Continue monitoring.

---

#### b) A vision system to automatically monitor the speed limit for a vehicle by “reading” any speed limit signs which are passed by the car.

##### Objective:
Detect and recognize speed limit signs from the environment and update the vehicle's known speed limit.

##### Assumptions:
- A front-facing camera captures the road.
- Speed limit signs follow a standard appearance (e.g., circular signs with numbers).
- The vehicle has a GPS or existing speed database to compare with.

---

##### System Components:
**Flow Diagram (Text Representation):**
```

[Camera Feed]       
	↓ 
[Sign Detection (Shape & Color)]       
	↓ 
[Sign Classification (CNN/OCR)]       
	↓ 
[Speed Limit Extraction]       
	↓ 
[Display / Update Speed Limit]

```

---

##### Components Explained:

- **Camera Feed**: Continuously captures the front view of the road.
- **Sign Detection**: Detects circular signs using shape detection and color filtering (e.g., red borders).
- **Sign Classification**: Uses OCR or a trained CNN to read the digits inside the sign.
- **Speed Limit Extraction**: Extracts numerical speed value.
- **Display/Update Speed Limit**: Updates vehicle's known speed limit for assistance or enforcement.

---

##### Algorithm Outline:

1. Capture frames from the camera.
2. Use Hough Circle Transform or contour detection to locate circular signs.
3. Filter based on color (red border, white background).
4. Crop and preprocess the sign region.
5. Use OCR or CNN to extract the speed limit digits.
6. Update vehicle’s speed limit knowledge base.
7. Repeat process for each new frame.






# 2019_2020_Q1
#### a) "Clustering is a more difficult and challenging problem than classification". Comment. (05 Marks)

Clustering is generally considered more difficult than classification due to the following reasons:

- **Lack of Ground Truth**: In classification, labeled data is available, whereas clustering operates on unlabeled data, making evaluation and guidance harder.
    
- **No Objective Function**: Classification uses known labels to minimize error, while clustering lacks a standard objective—resulting in multiple possible "correct" partitions.
    
- **Choosing k**: Determining the number of clusters (k) is non-trivial and often requires heuristics or domain knowledge.
    
- **Interpretability**: Clusters might not have clear semantic meaning, while class labels are predefined and interpretable.
    
- **Sensitive to Noise**: Clustering algorithms are typically more sensitive to noise and outliers than classification algorithms.

#### b) How can the clustering quality of any partition of the data be measured?

Clustering quality can be evaluated using **internal** and **external** measures:

- **Internal Measures** (no ground truth):
    - **Cohesion (Intra-cluster distance)**: Measures how close the points in a cluster are. Lower is better.
    - **Separation (Inter-cluster distance)**: Measures how distinct or far apart clusters are. Higher is better.
    - **Silhouette Coefficient**: Combines cohesion and separation into a single score ranging from -1 to 1.

- **External Measures** (with ground truth):    
    - **Rand Index / Adjusted Rand Index**
    - **Normalized Mutual Information (NMI)**
    - **F-measure / Purity**

#### c) How does k-means algorithm work? What are its key strengths and weaknesses? 

**K-Means Algorithm Steps**:
1. Choose `k` initial centroids (randomly or via heuristics).
2. Assign each point to the nearest centroid (based on a distance metric).
3. Recompute centroids as the mean of points in each cluster.
4. Repeat steps 2–3 until convergence (no change in assignments or centroids).

**Strengths**:
- Simple and easy to implement.
- Efficient for large datasets (time complexity is O(nkt)).
- Works well when clusters are spherical and equally sized.

**Weaknesses**:
- Sensitive to initial centroids.
- Assumes spherical clusters (not good for arbitrary shapes).
- Requires pre-specification of `k`.
- Sensitive to outliers and noise.


#### d) K-Means (1st iteration using city block distance) (10 Marks)
Given Points:
$$
 A1(2,10), A2(2,5), A3(8,4), A4(5,8), A5(7,5), A6(6,4), A7(1,2), A8(4,9)
$$

**Initial centroids**:

$C1 = A1(2,10),$
$C2 = A4(5,8),$
$C3 = A7(1,2)$  

**Distance Metric**: City Block (Manhattan Distance):  
For two points
$(x1, y1), (x2, y2): D = |x1 - x2| + |y1 - y2|$

**Compute distances and assign points to nearest cluster**:

| Point | Dist to C1 (2,10) | Dist to C2 (5,8) | Dist to C3 (1,2) | Assigned Cluster |
| :---: | :---------------: | :--------------: | :--------------: | :--------------: |
|  A1   |         0         |        5         |        9         |        C1        |
|  A2   |         5         |        6         |        4         |        C3        |
|  A3   |        12         |        7         |        9         |        C2        |
|  A4   |         5         |        0         |        10        |        C2        |
|  A5   |        10         |        5         |        9         |        C2        |
|  A6   |        10         |        4         |        7         |        C2        |
|  A7   |         9         |        10        |        0         |        C3        |
|  A8   |         3         |        2         |        10        |        C2        |

**Clusters after 1st iteration**:
- C1: {A1}
- C2: {A3, A4, A5, A6, A8}
- C3: {A2, A7}

**New centroids (mean of x and y coordinates in each cluster)**:
- C1: A1 = (2, 10)
- C2:
    - x̄ = (8+5+7+6+4)/5 = 30/5 = 6  
    - ȳ = (4+8+5+4+9)/5 = 30/5 = 6
    - New C2 = (6,6)
- C3:
    - x̄ = (2+1)/2 = 1.5  
    - ȳ = (5+2)/2 = 3.5
    - New C3 = (1.5, 3.5)

#### **e) Clustering points on two concentric circles using k-means (05 Marks)**

**K-means fails on this type of data** due to its assumptions:

- K-means clusters by **minimizing Euclidean distances**, which leads to **convex-shaped** clusters.
    
- Concentric circles form **non-convex shapes** (donut/ring structure), which K-means cannot represent.
    

**Expected Result**:

- K-means may split the data into **wedges (like slices of a pie)** crossing both circles, rather than separating the two rings.
    
- This is because the algorithm tries to divide space into **Voronoi cells**, not based on the shape of the data.
    

**Better alternatives**:

- **DBSCAN** or **Spectral Clustering** can handle such shapes more effectively since they do not assume convex clusters.